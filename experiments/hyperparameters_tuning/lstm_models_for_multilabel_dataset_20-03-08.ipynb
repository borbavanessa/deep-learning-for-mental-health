{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include project path to available custom class at jupyter\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath('/home/vanessa/PycharmProjects/RecurrentNetworks/'))\n",
    "\n",
    "# Disable warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Load libs standard python and custom\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "from network_model.custom_ensemble import CustomEnsemble\n",
    "from network_model.model_class import ModelClass\n",
    "from utils.experiment_processes import ExperimentProcesses\n",
    "from utils.log import Log\n",
    "import utils.definition_network as dn\n",
    "\n",
    "import datetime\n",
    "\n",
    "# LAYERS\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Running the main LSTM topologies for multilabel dataset </h2>\n",
    "<p>\n",
    "    <ul> When reviewing the experiments, there were some gaps that motivated the tests below:\n",
    "    <li> <b> 1.0 Train, validate and test experiment lstm_exp14_L3_N64_B20_E96 </b>: experiment with the best result multilabel dataset.\n",
    "        <ol><li> <b> Test lstm_exp14_L3_N64_B20_E96 + SMHD _ * _ 2640 </b> </li>\n",
    "            <li> <b> Test lstm_exp14_L3_N64_B20_E96 + SMHD _ * _ 1760 </b> </li>\n",
    "        </ol>\n",
    "    </li>\n",
    "    <li> <b> 2.0 Train, validate and test experiment lstm_exp9_var_L3_N16_B40_E32_D0.2 </b>: it has already been tested with multi-label dataset for the best configuration using Glove6B static embeddings. However, this same model and dataset and personalized word embeddings were tested, generating the set of tests below:\n",
    "        <ol><li> <b> Test lstm_exp9_var_L3_N16_B40_E32_D0.2 + word embeddings SMHD all users (SMHD _ * _ 1760) </b>, generated with algorithms (skipgram, cbow, glove) </li>\n",
    "            <li> <b> Test lstm_exp9_var_L3_N16_B40_E32_D0.2 + word embeddings SMHD all users (SMHD _ * _ 2640) </b>, generated with algorithms (skipgram, cbow, glove) </li>\n",
    "            <li> <b> Test lstm_exp9_var_L3_N16_B40_E32_D0.2 + word embeddings SMHD 3 pathologies (best result dataset between tests 2.1 and 2.2) </b>, generated with algorithms (skipgram, cbow, glove) </li>\n",
    "            <li> <b> Test lstm_exp9_var_L3_N16_B40_E32_D0.2 + Glove6B, glorot x lecun kernel initializer </b> </li>\n",
    "        </ol>\n",
    "    </li>\n",
    "    </ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "anxiety               440\n",
       "anxiety,depression    440\n",
       "control               880\n",
       "depression            440\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Checks composition of the used dataset train\n",
    "file_path = \"/home/vanessa/PycharmProjects/RecurrentNetworks/dataset/anx_dep_multilabel/\"\n",
    "dataset_train = pd.read_pickle(file_path + str(\"SMHD_train_2640.df\"))\n",
    "dataset_train.groupby('label').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "anxiety               440\n",
       "anxiety,depression    440\n",
       "control               880\n",
       "depression            440\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Checks composition of the used dataset valid\n",
    "dataset_valid = pd.read_pickle(file_path + str(\"SMHD_validation_2640.df\"))\n",
    "dataset_valid.groupby('label').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "anxiety               440\n",
       "anxiety,depression    440\n",
       "control               880\n",
       "depression            440\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Checks composition of the used dataset test\n",
    "dataset_test = pd.read_pickle(file_path + str(\"SMHD_test_2640.df\"))\n",
    "dataset_test.groupby('label').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>texts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>6600</td>\n",
       "      <td>6600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>unique</td>\n",
       "      <td>4</td>\n",
       "      <td>6600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>top</td>\n",
       "      <td>control</td>\n",
       "      <td>['We get new celebrities. ', 'Wolf Children. V...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>freq</td>\n",
       "      <td>2640</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          label                                              texts\n",
       "count      6600                                               6600\n",
       "unique        4                                               6600\n",
       "top     control  ['We get new celebrities. ', 'Wolf Children. V...\n",
       "freq       2640                                                  1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.concat([dataset_train, dataset_valid, dataset_test])\n",
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.0 Train, validate and test experiment lstm_exp14_L3_N64_B20_E96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_lstm_exp14(exp, name_model):\n",
    "    exp.pp_data.vocabulary_size = 5000\n",
    "    exp.pp_data.embedding_size = 300\n",
    "    exp.pp_data.max_posts = 1750\n",
    "    exp.pp_data.max_terms_by_post = 300\n",
    "    exp.pp_data.binary_classifier = True\n",
    "    exp.pp_data.format_input_data = dn.InputData.POSTS_LIST\n",
    "    exp.pp_data.remove_stopwords = False\n",
    "    exp.pp_data.delete_low_tfid = False\n",
    "    exp.pp_data.min_df = 0\n",
    "    exp.pp_data.min_tf = 0\n",
    "    exp.pp_data.random_posts = False\n",
    "    exp.pp_data.random_users = False\n",
    "    exp.pp_data.tokenizing_type = 'WE'\n",
    "    exp.pp_data.word_embedding_custom_file = ''    \n",
    "    exp.pp_data.use_embedding = dn.UseEmbedding.NONE\n",
    "    exp.pp_data.embedding_type = dn.EmbeddingType.NONE\n",
    "    exp.pp_data.load_dataset_type = dn.LoadDataset.TRAIN_DATA_MODEL\n",
    "    exp.pp_data.type_prediction_label= dn.TypePredictionLabel.MULTI_LABEL_CATEGORICAL\n",
    "\n",
    "    exp.use_custom_metrics = False\n",
    "    exp.use_valid_set_for_train = True\n",
    "    exp.valid_split_from_train_set = 0.0\n",
    "    exp.imbalanced_classes = False\n",
    "\n",
    "    we_file_name = 'ET_' + str(exp.pp_data.embedding_type.value) + '_UE_' + str(exp.pp_data.use_embedding.value) +\\\n",
    "                                 '_EF_' + 'glove6B300d_glorot'\n",
    "    \n",
    "    lstm = ModelClass(1)\n",
    "    lstm.loss_function = 'binary_crossentropy'\n",
    "    lstm.optmizer_function = 'adam'\n",
    "    lstm.use_embedding_pre_train = exp.pp_data.use_embedding\n",
    "    lstm.embed_trainable = False\n",
    "\n",
    "    # Train\n",
    "    neuronios_by_layer = [64]\n",
    "    epochs = [96]\n",
    "    batch_sizes = [20]\n",
    "    dropouts = [0.2]\n",
    "\n",
    "    np.random.seed(dn.SEED)\n",
    "    time_ini_rep = datetime.datetime.now()\n",
    "\n",
    "    x_train, y_train, x_valid, y_valid, num_words, embedding_matrix = exp.pp_data.load_data()\n",
    "    exp.set_period_time_end(time_ini_rep, 'Load data')\n",
    "\n",
    "    for neuronios in neuronios_by_layer:\n",
    "        for batch_size in batch_sizes:\n",
    "            for epoch in epochs:\n",
    "                for dropout in dropouts:\n",
    "                    lstm.epochs = epoch\n",
    "                    lstm.batch_size = batch_size\n",
    "                    lstm.patience_train = epoch/2\n",
    "                    data_dim = exp.pp_data.max_terms_by_post\n",
    "                    timesteps = exp.pp_data.max_posts\n",
    "\n",
    "                    exp.experiment_name = name_model + 'lstm_exp14_L3' + '_N' + str(neuronios) + '_B' +\\\n",
    "                                          str(batch_size) + '_E' + str(epoch) + '_D' + str(dropout) + '_' +\\\n",
    "                                          we_file_name\n",
    "\n",
    "                    lstm.model = Sequential()\n",
    "                    lstm.model.add(\n",
    "                            LSTM(neuronios, activation='tanh', dropout=dropout, recurrent_dropout=dropout, \n",
    "                                 return_sequences=True, stateful=True, \n",
    "                                 batch_input_shape=(batch_size, timesteps, data_dim),\n",
    "                                 name='dense_1_' + name_model))\n",
    "                    lstm.model.add(\n",
    "                            LSTM(neuronios, activation='tanh', dropout=dropout, recurrent_dropout=dropout,\n",
    "                                 return_sequences=True, stateful=True,\n",
    "                                 name='dense_2_' + name_model))\n",
    "                    lstm.model.add(LSTM(neuronios, activation='tanh', dropout=dropout, recurrent_dropout=dropout,\n",
    "                                        stateful=True,\n",
    "                                        name='dense_3_' + name_model))\n",
    "                    lstm.model.add(Dense(3, activation='sigmoid',\n",
    "                                         name='dense_4_' + name_model))\n",
    "\n",
    "                    time_ini_exp = datetime.datetime.now()\n",
    "                    exp.generate_model_hypeparams(lstm, x_train, y_train, x_valid, y_valid, embedding_matrix)\n",
    "                    exp.set_period_time_end(time_ini_exp, 'Total experiment')\n",
    "\n",
    "    del x_train, y_train, x_valid, y_valid, num_words, embedding_matrix\n",
    "\n",
    "    # Test\n",
    "    exp.pp_data.load_dataset_type = dn.LoadDataset.TEST_DATA_MODEL\n",
    "    np.random.seed(dn.SEED)\n",
    "    time_ini_rep = datetime.datetime.now()\n",
    "    x_test, y_test = exp.pp_data.load_data()\n",
    "    exp.set_period_time_end(time_ini_rep, 'Load data')\n",
    "\n",
    "    for neuronios in neuronios_by_layer:\n",
    "        for batch_size in batch_sizes:\n",
    "            for epoch in epochs:\n",
    "                for dropout in dropouts:\n",
    "                    exp.experiment_name = name_model + 'lstm_exp14_L3' + '_N' + str(neuronios) + '_B' +\\\n",
    "                                          str(batch_size) + '_E' + str(epoch) + '_D' + str(dropout) + '_' +\\\n",
    "                                          we_file_name\n",
    "                    lstm.epochs = epoch\n",
    "                    lstm.batch_size = batch_size\n",
    "                    lstm.patience_train = epoch/2\n",
    "\n",
    "                    lstm.model = exp.load_model(dn.PATH_PROJECT + exp.experiment_name + '.h5')\n",
    "                    exp.save_geral_configs()\n",
    "                    exp.save_summary_model(lstm.model)\n",
    "                    exp.predict_samples(lstm, x_test, y_test)\n",
    "    \n",
    "    del x_test, y_test, lstm, exp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Test lstm_exp14_L3_N64_B20_E96 + SMHD_*_2640"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializer experiment 1 (model SMHD_ml_gl_2640)\n",
      "Set: kernel_initializer=glorot_uniform=xavier_uniform, dataset=SMHD_2640 multi-label\n",
      "Loading data... /home/vanessa/PycharmProjects/RecurrentNetworks/tokenizers/anx_dep_multilabel/SMHD_TR_2640_VS_5000_TF_0_DF_0_RSW_F_IT_2_RP_F\n",
      "Preprocess data... /home/vanessa/PycharmProjects/RecurrentNetworks/dataset/anx_dep_multilabel/SMHD_train_2640.df\n",
      "Load data - Ini: 2020-03-08 13:49:08\tEnd: 2020-03-08 13:50:19\tTotal: 0:01:11.455724\n",
      "WARNING:tensorflow:From /home/vanessa/anaconda3/envs/tensorflow_cpu/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/vanessa/anaconda3/envs/tensorflow_cpu/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/vanessa/anaconda3/envs/tensorflow_cpu/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/vanessa/anaconda3/envs/tensorflow_cpu/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/vanessa/anaconda3/envs/tensorflow_cpu/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /home/vanessa/anaconda3/envs/tensorflow_cpu/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "Training using single GPU or CPU..\n",
      "WARNING:tensorflow:From /home/vanessa/anaconda3/envs/tensorflow_cpu/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/vanessa/anaconda3/envs/tensorflow_cpu/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Train on 2200 samples, validate on 2200 samples\n",
      "Epoch 1/96\n",
      "2200/2200 [==============================] - 266s 121ms/step - loss: 0.6764 - acc: 0.5995 - val_loss: 0.6736 - val_acc: 0.6000\n",
      "Epoch 2/96\n",
      "2200/2200 [==============================] - 258s 117ms/step - loss: 0.6741 - acc: 0.6000 - val_loss: 0.6730 - val_acc: 0.6000\n",
      "Epoch 3/96\n",
      "2200/2200 [==============================] - 260s 118ms/step - loss: 0.6740 - acc: 0.6000 - val_loss: 0.6731 - val_acc: 0.6000\n",
      "Epoch 4/96\n",
      "2200/2200 [==============================] - 262s 119ms/step - loss: 0.6741 - acc: 0.6000 - val_loss: 0.6732 - val_acc: 0.6000\n",
      "Epoch 5/96\n",
      "2200/2200 [==============================] - 261s 119ms/step - loss: 0.6738 - acc: 0.6000 - val_loss: 0.6731 - val_acc: 0.6000\n",
      "Epoch 6/96\n",
      "2200/2200 [==============================] - 261s 119ms/step - loss: 0.6738 - acc: 0.6000 - val_loss: 0.6731 - val_acc: 0.6000\n",
      "Epoch 7/96\n",
      "2200/2200 [==============================] - 262s 119ms/step - loss: 0.6737 - acc: 0.6000 - val_loss: 0.6732 - val_acc: 0.6000\n",
      "Epoch 8/96\n",
      "2200/2200 [==============================] - 263s 119ms/step - loss: 0.6733 - acc: 0.6000 - val_loss: 0.6730 - val_acc: 0.6000\n",
      "Epoch 9/96\n",
      "2200/2200 [==============================] - 262s 119ms/step - loss: 0.6734 - acc: 0.6000 - val_loss: 0.6732 - val_acc: 0.6000\n",
      "Epoch 10/96\n",
      "2200/2200 [==============================] - 261s 119ms/step - loss: 0.6736 - acc: 0.6000 - val_loss: 0.6732 - val_acc: 0.6000\n",
      "Epoch 11/96\n",
      "2200/2200 [==============================] - 262s 119ms/step - loss: 0.6735 - acc: 0.6000 - val_loss: 0.6733 - val_acc: 0.6000\n",
      "Epoch 12/96\n",
      "2200/2200 [==============================] - 262s 119ms/step - loss: 0.6741 - acc: 0.6000 - val_loss: 0.6731 - val_acc: 0.6000\n",
      "Epoch 13/96\n",
      "2200/2200 [==============================] - 261s 119ms/step - loss: 0.6733 - acc: 0.6000 - val_loss: 0.6733 - val_acc: 0.6000\n",
      "Epoch 14/96\n",
      "2200/2200 [==============================] - 261s 119ms/step - loss: 0.6736 - acc: 0.6000 - val_loss: 0.6731 - val_acc: 0.6000\n",
      "Epoch 15/96\n",
      "2200/2200 [==============================] - 258s 117ms/step - loss: 0.6736 - acc: 0.6000 - val_loss: 0.6731 - val_acc: 0.6000\n",
      "Epoch 16/96\n",
      "2200/2200 [==============================] - 261s 119ms/step - loss: 0.6733 - acc: 0.6000 - val_loss: 0.6732 - val_acc: 0.6000\n",
      "Epoch 17/96\n",
      "2200/2200 [==============================] - 261s 119ms/step - loss: 0.6735 - acc: 0.6000 - val_loss: 0.6730 - val_acc: 0.6000\n",
      "Epoch 18/96\n",
      "2200/2200 [==============================] - 262s 119ms/step - loss: 0.6733 - acc: 0.6000 - val_loss: 0.6730 - val_acc: 0.6000\n",
      "Epoch 19/96\n",
      "2200/2200 [==============================] - 262s 119ms/step - loss: 0.6736 - acc: 0.6000 - val_loss: 0.6730 - val_acc: 0.6000\n",
      "Epoch 20/96\n",
      "2200/2200 [==============================] - 260s 118ms/step - loss: 0.6732 - acc: 0.6000 - val_loss: 0.6731 - val_acc: 0.6000\n",
      "Epoch 21/96\n",
      "2200/2200 [==============================] - 260s 118ms/step - loss: 0.6732 - acc: 0.6000 - val_loss: 0.6731 - val_acc: 0.6000\n",
      "Epoch 22/96\n",
      "2200/2200 [==============================] - 260s 118ms/step - loss: 0.6734 - acc: 0.6000 - val_loss: 0.6731 - val_acc: 0.6000\n",
      "Epoch 23/96\n",
      "2200/2200 [==============================] - 262s 119ms/step - loss: 0.6735 - acc: 0.6000 - val_loss: 0.6731 - val_acc: 0.6000\n",
      "Epoch 24/96\n",
      "2200/2200 [==============================] - 262s 119ms/step - loss: 0.6733 - acc: 0.6000 - val_loss: 0.6733 - val_acc: 0.6000\n",
      "Epoch 25/96\n",
      "2200/2200 [==============================] - 262s 119ms/step - loss: 0.6734 - acc: 0.6000 - val_loss: 0.6731 - val_acc: 0.6000\n",
      "Epoch 26/96\n",
      "2200/2200 [==============================] - 262s 119ms/step - loss: 0.6733 - acc: 0.6000 - val_loss: 0.6732 - val_acc: 0.6000\n",
      "Epoch 27/96\n",
      "2200/2200 [==============================] - 261s 119ms/step - loss: 0.6733 - acc: 0.6000 - val_loss: 0.6731 - val_acc: 0.6000\n",
      "Epoch 28/96\n",
      "2200/2200 [==============================] - 259s 118ms/step - loss: 0.6734 - acc: 0.6000 - val_loss: 0.6731 - val_acc: 0.6000\n",
      "Epoch 29/96\n",
      "2200/2200 [==============================] - 263s 119ms/step - loss: 0.6734 - acc: 0.6000 - val_loss: 0.6731 - val_acc: 0.6000\n",
      "Epoch 30/96\n",
      "2200/2200 [==============================] - 260s 118ms/step - loss: 0.6733 - acc: 0.6000 - val_loss: 0.6730 - val_acc: 0.6000\n",
      "Epoch 31/96\n",
      "2200/2200 [==============================] - 260s 118ms/step - loss: 0.6737 - acc: 0.6000 - val_loss: 0.6731 - val_acc: 0.6000\n",
      "Epoch 32/96\n",
      "2200/2200 [==============================] - 262s 119ms/step - loss: 0.6733 - acc: 0.6000 - val_loss: 0.6730 - val_acc: 0.6000\n",
      "Epoch 33/96\n",
      "2200/2200 [==============================] - 262s 119ms/step - loss: 0.6733 - acc: 0.6000 - val_loss: 0.6731 - val_acc: 0.6000\n",
      "Epoch 34/96\n",
      "2200/2200 [==============================] - 262s 119ms/step - loss: 0.6733 - acc: 0.6000 - val_loss: 0.6731 - val_acc: 0.6000\n",
      "Epoch 35/96\n",
      "2200/2200 [==============================] - 262s 119ms/step - loss: 0.6735 - acc: 0.6000 - val_loss: 0.6733 - val_acc: 0.6000\n",
      "Epoch 36/96\n",
      "2200/2200 [==============================] - 263s 120ms/step - loss: 0.6733 - acc: 0.6000 - val_loss: 0.6733 - val_acc: 0.6000\n",
      "Epoch 37/96\n",
      "2200/2200 [==============================] - 262s 119ms/step - loss: 0.6733 - acc: 0.6000 - val_loss: 0.6731 - val_acc: 0.6000\n",
      "Epoch 38/96\n",
      "2200/2200 [==============================] - 262s 119ms/step - loss: 0.6733 - acc: 0.6000 - val_loss: 0.6732 - val_acc: 0.6000\n",
      "Epoch 39/96\n",
      "2200/2200 [==============================] - 261s 119ms/step - loss: 0.6734 - acc: 0.6000 - val_loss: 0.6733 - val_acc: 0.6000\n",
      "Epoch 40/96\n",
      "2200/2200 [==============================] - 261s 119ms/step - loss: 0.6733 - acc: 0.6000 - val_loss: 0.6730 - val_acc: 0.6000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/96\n",
      "2200/2200 [==============================] - 262s 119ms/step - loss: 0.6733 - acc: 0.6000 - val_loss: 0.6731 - val_acc: 0.6000\n",
      "Epoch 42/96\n",
      "2200/2200 [==============================] - 262s 119ms/step - loss: 0.6732 - acc: 0.6000 - val_loss: 0.6731 - val_acc: 0.6000\n",
      "Epoch 43/96\n",
      "2200/2200 [==============================] - 263s 120ms/step - loss: 0.6734 - acc: 0.6000 - val_loss: 0.6731 - val_acc: 0.6000\n",
      "Epoch 44/96\n",
      "2200/2200 [==============================] - 263s 120ms/step - loss: 0.6734 - acc: 0.6000 - val_loss: 0.6730 - val_acc: 0.6000\n",
      "Epoch 45/96\n",
      "2200/2200 [==============================] - 262s 119ms/step - loss: 0.6733 - acc: 0.6000 - val_loss: 0.6731 - val_acc: 0.6000\n",
      "Epoch 46/96\n",
      "2200/2200 [==============================] - 261s 119ms/step - loss: 0.6733 - acc: 0.6000 - val_loss: 0.6731 - val_acc: 0.6000\n",
      "Epoch 47/96\n",
      "2200/2200 [==============================] - 263s 119ms/step - loss: 0.6732 - acc: 0.6000 - val_loss: 0.6730 - val_acc: 0.6000\n",
      "Epoch 48/96\n",
      "2200/2200 [==============================] - 262s 119ms/step - loss: 0.6734 - acc: 0.6000 - val_loss: 0.6731 - val_acc: 0.6000\n",
      "Epoch 49/96\n",
      "2200/2200 [==============================] - 262s 119ms/step - loss: 0.6733 - acc: 0.6000 - val_loss: 0.6730 - val_acc: 0.6000\n",
      "Epoch 50/96\n",
      "2200/2200 [==============================] - 261s 119ms/step - loss: 0.6733 - acc: 0.6000 - val_loss: 0.6730 - val_acc: 0.6000\n",
      "Epoch 51/96\n",
      "2200/2200 [==============================] - 263s 119ms/step - loss: 0.6732 - acc: 0.6000 - val_loss: 0.6730 - val_acc: 0.6000\n",
      "Epoch 52/96\n",
      "2200/2200 [==============================] - 262s 119ms/step - loss: 0.6733 - acc: 0.6000 - val_loss: 0.6730 - val_acc: 0.6000\n",
      "Epoch 53/96\n",
      "2200/2200 [==============================] - 263s 119ms/step - loss: 0.6732 - acc: 0.6000 - val_loss: 0.6730 - val_acc: 0.6000\n",
      "Epoch 54/96\n",
      "2200/2200 [==============================] - 260s 118ms/step - loss: 0.6734 - acc: 0.6000 - val_loss: 0.6730 - val_acc: 0.6000\n",
      "Epoch 55/96\n",
      "2200/2200 [==============================] - 261s 119ms/step - loss: 0.6733 - acc: 0.6000 - val_loss: 0.6731 - val_acc: 0.6000\n",
      "Epoch 56/96\n",
      "2200/2200 [==============================] - 256s 116ms/step - loss: 0.6732 - acc: 0.6000 - val_loss: 0.6730 - val_acc: 0.6000\n",
      "Epoch 57/96\n",
      "2200/2200 [==============================] - 262s 119ms/step - loss: 0.6732 - acc: 0.6000 - val_loss: 0.6731 - val_acc: 0.6000\n",
      "Epoch 58/96\n",
      "2200/2200 [==============================] - 261s 119ms/step - loss: 0.6734 - acc: 0.6000 - val_loss: 0.6730 - val_acc: 0.6000\n",
      "Epoch 59/96\n",
      "2200/2200 [==============================] - 262s 119ms/step - loss: 0.6733 - acc: 0.6000 - val_loss: 0.6731 - val_acc: 0.6000\n",
      "Epoch 60/96\n",
      "2200/2200 [==============================] - 261s 119ms/step - loss: 0.6731 - acc: 0.6000 - val_loss: 0.6730 - val_acc: 0.6000\n",
      "Epoch 61/96\n",
      "2200/2200 [==============================] - 261s 119ms/step - loss: 0.6733 - acc: 0.6000 - val_loss: 0.6730 - val_acc: 0.6000\n",
      "Epoch 62/96\n",
      "2200/2200 [==============================] - 261s 119ms/step - loss: 0.6732 - acc: 0.6000 - val_loss: 0.6730 - val_acc: 0.6000\n",
      "Epoch 63/96\n",
      "2200/2200 [==============================] - 262s 119ms/step - loss: 0.6733 - acc: 0.6000 - val_loss: 0.6730 - val_acc: 0.6000\n",
      "Epoch 64/96\n",
      "2200/2200 [==============================] - 260s 118ms/step - loss: 0.6732 - acc: 0.6000 - val_loss: 0.6730 - val_acc: 0.6000\n",
      "Epoch 65/96\n",
      "2200/2200 [==============================] - 261s 119ms/step - loss: 0.6731 - acc: 0.6000 - val_loss: 0.6730 - val_acc: 0.6000\n",
      "Epoch 66/96\n",
      "2200/2200 [==============================] - 262s 119ms/step - loss: 0.6733 - acc: 0.6000 - val_loss: 0.6730 - val_acc: 0.6000\n",
      "Epoch 67/96\n",
      "2200/2200 [==============================] - 262s 119ms/step - loss: 0.6733 - acc: 0.6000 - val_loss: 0.6730 - val_acc: 0.6000\n",
      "Epoch 68/96\n",
      "2200/2200 [==============================] - 262s 119ms/step - loss: 0.6732 - acc: 0.6000 - val_loss: 0.6730 - val_acc: 0.6000\n",
      "Epoch 69/96\n",
      "2200/2200 [==============================] - 262s 119ms/step - loss: 0.6732 - acc: 0.6000 - val_loss: 0.6730 - val_acc: 0.6000\n",
      "Epoch 70/96\n",
      "2200/2200 [==============================] - 260s 118ms/step - loss: 0.6732 - acc: 0.6000 - val_loss: 0.6730 - val_acc: 0.6000\n",
      "Epoch 71/96\n",
      "2200/2200 [==============================] - 263s 120ms/step - loss: 0.6733 - acc: 0.6000 - val_loss: 0.6731 - val_acc: 0.6000\n",
      "Epoch 72/96\n",
      "2200/2200 [==============================] - 262s 119ms/step - loss: 0.6731 - acc: 0.6000 - val_loss: 0.6733 - val_acc: 0.6000\n",
      "Epoch 73/96\n",
      "2200/2200 [==============================] - 262s 119ms/step - loss: 0.6733 - acc: 0.6000 - val_loss: 0.6730 - val_acc: 0.6000\n",
      "Epoch 74/96\n",
      "2200/2200 [==============================] - 262s 119ms/step - loss: 0.6732 - acc: 0.6000 - val_loss: 0.6730 - val_acc: 0.6000\n",
      "Epoch 75/96\n",
      "2200/2200 [==============================] - 259s 118ms/step - loss: 0.6732 - acc: 0.6000 - val_loss: 0.6733 - val_acc: 0.6000\n",
      "Epoch 76/96\n",
      "2200/2200 [==============================] - 263s 120ms/step - loss: 0.6732 - acc: 0.6000 - val_loss: 0.6730 - val_acc: 0.6000\n",
      "Epoch 77/96\n",
      "2200/2200 [==============================] - 261s 118ms/step - loss: 0.6732 - acc: 0.6000 - val_loss: 0.6730 - val_acc: 0.6000\n",
      "Epoch 78/96\n",
      "2200/2200 [==============================] - 262s 119ms/step - loss: 0.6733 - acc: 0.6000 - val_loss: 0.6732 - val_acc: 0.6000\n",
      "Epoch 79/96\n",
      "2200/2200 [==============================] - 262s 119ms/step - loss: 0.6732 - acc: 0.6000 - val_loss: 0.6730 - val_acc: 0.6000\n",
      "Epoch 80/96\n",
      "2200/2200 [==============================] - 263s 119ms/step - loss: 0.6732 - acc: 0.6000 - val_loss: 0.6730 - val_acc: 0.6000\n",
      "Epoch 81/96\n",
      "2200/2200 [==============================] - 261s 119ms/step - loss: 0.6732 - acc: 0.6000 - val_loss: 0.6730 - val_acc: 0.6000\n",
      "Epoch 82/96\n",
      "2200/2200 [==============================] - 262s 119ms/step - loss: 0.6732 - acc: 0.6000 - val_loss: 0.6730 - val_acc: 0.6000\n",
      "Epoch 83/96\n",
      "2200/2200 [==============================] - 263s 120ms/step - loss: 0.6732 - acc: 0.6000 - val_loss: 0.6730 - val_acc: 0.6000\n",
      "Epoch 84/96\n",
      "2200/2200 [==============================] - 262s 119ms/step - loss: 0.6733 - acc: 0.6000 - val_loss: 0.6730 - val_acc: 0.6000\n",
      "Epoch 85/96\n",
      "2200/2200 [==============================] - 262s 119ms/step - loss: 0.6731 - acc: 0.6000 - val_loss: 0.6730 - val_acc: 0.6000\n",
      "Epoch 86/96\n",
      "2200/2200 [==============================] - 259s 118ms/step - loss: 0.6731 - acc: 0.6000 - val_loss: 0.6731 - val_acc: 0.6000\n",
      "Epoch 87/96\n",
      "2200/2200 [==============================] - 261s 119ms/step - loss: 0.6733 - acc: 0.6000 - val_loss: 0.6730 - val_acc: 0.6000\n",
      "Epoch 88/96\n",
      "2200/2200 [==============================] - 261s 118ms/step - loss: 0.6732 - acc: 0.6000 - val_loss: 0.6730 - val_acc: 0.6000\n",
      "Epoch 89/96\n",
      "2200/2200 [==============================] - 263s 120ms/step - loss: 0.6732 - acc: 0.6000 - val_loss: 0.6731 - val_acc: 0.6000\n",
      "Epoch 90/96\n",
      "2200/2200 [==============================] - 262s 119ms/step - loss: 0.6732 - acc: 0.6000 - val_loss: 0.6731 - val_acc: 0.6000\n",
      "Epoch 91/96\n",
      "2200/2200 [==============================] - 263s 120ms/step - loss: 0.6733 - acc: 0.6000 - val_loss: 0.6730 - val_acc: 0.6000\n",
      "Epoch 92/96\n",
      "2200/2200 [==============================] - 264s 120ms/step - loss: 0.6731 - acc: 0.6000 - val_loss: 0.6730 - val_acc: 0.6000\n",
      "Epoch 93/96\n",
      "2200/2200 [==============================] - 262s 119ms/step - loss: 0.6733 - acc: 0.6000 - val_loss: 0.6730 - val_acc: 0.6000\n",
      "Epoch 94/96\n",
      "2200/2200 [==============================] - 261s 119ms/step - loss: 0.6731 - acc: 0.6000 - val_loss: 0.6732 - val_acc: 0.6000\n",
      "Epoch 95/96\n",
      "2200/2200 [==============================] - 263s 120ms/step - loss: 0.6732 - acc: 0.6000 - val_loss: 0.6730 - val_acc: 0.6000\n",
      "Epoch 96/96\n",
      "2200/2200 [==============================] - 261s 119ms/step - loss: 0.6733 - acc: 0.6000 - val_loss: 0.6730 - val_acc: 0.6000\n",
      "Generate Model - Ini: 2020-03-08 13:50:29\tEnd: 2020-03-08 20:49:20\tTotal: 6:58:51.029581\n",
      "Total experiment - Ini: 2020-03-08 13:50:29\tEnd: 2020-03-08 20:49:20\tTotal: 6:58:51.302792\n",
      "Loading data... /home/vanessa/PycharmProjects/RecurrentNetworks/tokenizers/anx_dep_multilabel/SMHD_TR_2640_VS_5000_TF_0_DF_0_RSW_F_IT_2_RP_F\n",
      "Preprocess data... /home/vanessa/PycharmProjects/RecurrentNetworks/dataset/anx_dep_multilabel/SMHD_test_2640.df\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data - Ini: 2020-03-08 20:49:20\tEnd: 2020-03-08 20:49:50\tTotal: 0:00:29.492355\n",
      "2200/2200 [==============================] - 55s 25ms/step\n"
     ]
    }
   ],
   "source": [
    "print('Initializer experiment 1 (model SMHD_ml_gl_2640)\\n'+\\\n",
    "      'Set: kernel_initializer=glorot_uniform=xavier_uniform, dataset=SMHD_2640 multi-label')\n",
    "\n",
    "exp = ExperimentProcesses('lstm_exp14_L3')\n",
    "exp.pp_data.set_dataset_source(dataset_name='SMHD', label_set=['control', 'anxiety', 'depression'],\n",
    "                               total_registers=2640, subdirectory=\"anx_dep_multilabel\")\n",
    "\n",
    "generate_lstm_exp14(exp, 'exp14_ml_gl_2640')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Test lstm_exp14_L3_N64_B20_E96 + SMHD_*_1760"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializer experiment 1.1 (model SMHD_ml_gl_1760)\n",
      "Set: kernel_initializer=glorot_uniform=xavier_uniform, dataset=SMHD_1760 multi-label\n",
      "Loading data... /home/vanessa/PycharmProjects/RecurrentNetworks/tokenizers/anx_dep_multilabel/SMHD_TR_1760_VS_5000_TF_0_DF_0_RSW_F_IT_2_RP_F\n",
      "Preprocess data...\n",
      "Generate tokenizing\n",
      "Load data - Ini: 2020-03-08 20:51:02\tEnd: 2020-03-08 20:53:31\tTotal: 0:02:29.144501\n",
      "Training using single GPU or CPU..\n",
      "Train on 1760 samples, validate on 1760 samples\n",
      "Epoch 1/96\n",
      "1760/1760 [==============================] - 209s 119ms/step - loss: 0.6560 - acc: 0.5816 - val_loss: 0.6517 - val_acc: 0.5833\n",
      "Epoch 2/96\n",
      "1760/1760 [==============================] - 207s 117ms/step - loss: 0.6520 - acc: 0.5841 - val_loss: 0.6506 - val_acc: 0.5833\n",
      "Epoch 3/96\n",
      "1760/1760 [==============================] - 209s 119ms/step - loss: 0.6527 - acc: 0.5824 - val_loss: 0.6502 - val_acc: 0.5833\n",
      "Epoch 4/96\n",
      "1760/1760 [==============================] - 206s 117ms/step - loss: 0.6525 - acc: 0.5712 - val_loss: 0.6501 - val_acc: 0.5833\n",
      "Epoch 5/96\n",
      "1760/1760 [==============================] - 208s 118ms/step - loss: 0.6513 - acc: 0.5777 - val_loss: 0.6497 - val_acc: 0.5833\n",
      "Epoch 6/96\n",
      "1760/1760 [==============================] - 207s 118ms/step - loss: 0.6516 - acc: 0.5765 - val_loss: 0.6499 - val_acc: 0.5833\n",
      "Epoch 7/96\n",
      "1760/1760 [==============================] - 208s 118ms/step - loss: 0.6510 - acc: 0.5777 - val_loss: 0.6496 - val_acc: 0.5833\n",
      "Epoch 8/96\n",
      "1760/1760 [==============================] - 211s 120ms/step - loss: 0.6511 - acc: 0.5811 - val_loss: 0.6501 - val_acc: 0.5833\n",
      "Epoch 9/96\n",
      "1760/1760 [==============================] - 211s 120ms/step - loss: 0.6509 - acc: 0.5786 - val_loss: 0.6499 - val_acc: 0.5833\n",
      "Epoch 10/96\n",
      "1760/1760 [==============================] - 214s 121ms/step - loss: 0.6499 - acc: 0.5869 - val_loss: 0.6514 - val_acc: 0.5833\n",
      "Epoch 11/96\n",
      "1760/1760 [==============================] - 211s 120ms/step - loss: 0.6512 - acc: 0.5856 - val_loss: 0.6499 - val_acc: 0.5833\n",
      "Epoch 12/96\n",
      "1760/1760 [==============================] - 213s 121ms/step - loss: 0.6507 - acc: 0.5830 - val_loss: 0.6496 - val_acc: 0.5833\n",
      "Epoch 13/96\n",
      "1760/1760 [==============================] - 212s 120ms/step - loss: 0.6505 - acc: 0.5741 - val_loss: 0.6499 - val_acc: 0.5833\n",
      "Epoch 14/96\n",
      "1760/1760 [==============================] - 212s 120ms/step - loss: 0.6506 - acc: 0.5735 - val_loss: 0.6496 - val_acc: 0.5833\n",
      "Epoch 15/96\n",
      "1760/1760 [==============================] - 209s 119ms/step - loss: 0.6504 - acc: 0.5862 - val_loss: 0.6499 - val_acc: 0.5833\n",
      "Epoch 16/96\n",
      "1760/1760 [==============================] - 212s 120ms/step - loss: 0.6505 - acc: 0.5824 - val_loss: 0.6496 - val_acc: 0.5833\n",
      "Epoch 17/96\n",
      "1760/1760 [==============================] - 211s 120ms/step - loss: 0.6507 - acc: 0.5784 - val_loss: 0.6502 - val_acc: 0.5833\n",
      "Epoch 18/96\n",
      "1760/1760 [==============================] - 210s 119ms/step - loss: 0.6506 - acc: 0.5813 - val_loss: 0.6496 - val_acc: 0.5833\n",
      "Epoch 19/96\n",
      "1760/1760 [==============================] - 210s 119ms/step - loss: 0.6505 - acc: 0.5706 - val_loss: 0.6496 - val_acc: 0.5833\n",
      "Epoch 20/96\n",
      "1760/1760 [==============================] - 210s 119ms/step - loss: 0.6505 - acc: 0.5843 - val_loss: 0.6498 - val_acc: 0.5833\n",
      "Epoch 21/96\n",
      "1760/1760 [==============================] - 213s 121ms/step - loss: 0.6508 - acc: 0.5705 - val_loss: 0.6496 - val_acc: 0.5833\n",
      "Epoch 22/96\n",
      "1760/1760 [==============================] - 212s 120ms/step - loss: 0.6506 - acc: 0.5803 - val_loss: 0.6502 - val_acc: 0.5833\n",
      "Epoch 23/96\n",
      "1760/1760 [==============================] - 213s 121ms/step - loss: 0.6501 - acc: 0.5759 - val_loss: 0.6497 - val_acc: 0.5833\n",
      "Epoch 24/96\n",
      "1760/1760 [==============================] - 212s 120ms/step - loss: 0.6507 - acc: 0.5799 - val_loss: 0.6498 - val_acc: 0.5833\n",
      "Epoch 25/96\n",
      "1760/1760 [==============================] - 212s 120ms/step - loss: 0.6505 - acc: 0.5805 - val_loss: 0.6498 - val_acc: 0.5833\n",
      "Epoch 26/96\n",
      "1760/1760 [==============================] - 211s 120ms/step - loss: 0.6501 - acc: 0.5737 - val_loss: 0.6499 - val_acc: 0.5833\n",
      "Epoch 27/96\n",
      "1760/1760 [==============================] - 214s 122ms/step - loss: 0.6502 - acc: 0.5795 - val_loss: 0.6498 - val_acc: 0.5833\n",
      "Epoch 28/96\n",
      "1760/1760 [==============================] - 227s 129ms/step - loss: 0.6506 - acc: 0.5792 - val_loss: 0.6499 - val_acc: 0.5833\n",
      "Epoch 29/96\n",
      "1760/1760 [==============================] - 228s 129ms/step - loss: 0.6503 - acc: 0.5809 - val_loss: 0.6498 - val_acc: 0.5833\n",
      "Epoch 30/96\n",
      "1760/1760 [==============================] - 221s 125ms/step - loss: 0.6502 - acc: 0.5805 - val_loss: 0.6496 - val_acc: 0.5833\n",
      "Epoch 31/96\n",
      "1760/1760 [==============================] - 219s 124ms/step - loss: 0.6497 - acc: 0.5877 - val_loss: 0.6496 - val_acc: 0.5833\n",
      "Epoch 32/96\n",
      "1760/1760 [==============================] - 245s 139ms/step - loss: 0.6498 - acc: 0.5837 - val_loss: 0.6498 - val_acc: 0.5833\n",
      "Epoch 33/96\n",
      "1760/1760 [==============================] - 229s 130ms/step - loss: 0.6509 - acc: 0.5725 - val_loss: 0.6499 - val_acc: 0.5833\n",
      "Epoch 34/96\n",
      "1760/1760 [==============================] - 228s 130ms/step - loss: 0.6507 - acc: 0.5780 - val_loss: 0.6496 - val_acc: 0.5833\n",
      "Epoch 35/96\n",
      "1760/1760 [==============================] - 228s 130ms/step - loss: 0.6502 - acc: 0.5790 - val_loss: 0.6496 - val_acc: 0.5833\n",
      "Epoch 36/96\n",
      "1760/1760 [==============================] - 228s 129ms/step - loss: 0.6502 - acc: 0.5873 - val_loss: 0.6496 - val_acc: 0.5833\n",
      "Epoch 37/96\n",
      "1760/1760 [==============================] - 227s 129ms/step - loss: 0.6503 - acc: 0.5782 - val_loss: 0.6496 - val_acc: 0.5833\n",
      "Epoch 38/96\n",
      "1760/1760 [==============================] - 215s 122ms/step - loss: 0.6505 - acc: 0.5629 - val_loss: 0.6498 - val_acc: 0.5833\n",
      "Epoch 39/96\n",
      "1760/1760 [==============================] - 211s 120ms/step - loss: 0.6507 - acc: 0.5680 - val_loss: 0.6496 - val_acc: 0.5833\n",
      "Epoch 40/96\n",
      "1760/1760 [==============================] - 212s 120ms/step - loss: 0.6501 - acc: 0.5759 - val_loss: 0.6497 - val_acc: 0.5833\n",
      "Epoch 41/96\n",
      "1760/1760 [==============================] - 211s 120ms/step - loss: 0.6502 - acc: 0.5777 - val_loss: 0.6496 - val_acc: 0.5833\n",
      "Epoch 42/96\n",
      "1760/1760 [==============================] - 213s 121ms/step - loss: 0.6502 - acc: 0.5752 - val_loss: 0.6496 - val_acc: 0.5833\n",
      "Epoch 43/96\n",
      "1760/1760 [==============================] - 213s 121ms/step - loss: 0.6503 - acc: 0.5824 - val_loss: 0.6496 - val_acc: 0.5833\n",
      "Epoch 44/96\n",
      "1760/1760 [==============================] - 213s 121ms/step - loss: 0.6501 - acc: 0.5759 - val_loss: 0.6498 - val_acc: 0.5833\n",
      "Epoch 45/96\n",
      "1760/1760 [==============================] - 209s 119ms/step - loss: 0.6502 - acc: 0.5854 - val_loss: 0.6497 - val_acc: 0.5833\n",
      "Epoch 46/96\n",
      "1760/1760 [==============================] - 211s 120ms/step - loss: 0.6501 - acc: 0.5830 - val_loss: 0.6496 - val_acc: 0.5833\n",
      "Epoch 47/96\n",
      "1760/1760 [==============================] - 213s 121ms/step - loss: 0.6500 - acc: 0.5801 - val_loss: 0.6496 - val_acc: 0.5833\n",
      "Epoch 48/96\n",
      "1760/1760 [==============================] - 213s 121ms/step - loss: 0.6502 - acc: 0.5748 - val_loss: 0.6496 - val_acc: 0.5833\n",
      "Epoch 49/96\n",
      "1760/1760 [==============================] - 213s 121ms/step - loss: 0.6505 - acc: 0.5788 - val_loss: 0.6497 - val_acc: 0.5833\n",
      "Epoch 50/96\n",
      "1760/1760 [==============================] - 213s 121ms/step - loss: 0.6502 - acc: 0.5816 - val_loss: 0.6497 - val_acc: 0.5833\n",
      "Epoch 51/96\n",
      "1760/1760 [==============================] - 213s 121ms/step - loss: 0.6500 - acc: 0.5852 - val_loss: 0.6496 - val_acc: 0.5833\n",
      "Epoch 52/96\n",
      "1760/1760 [==============================] - 213s 121ms/step - loss: 0.6499 - acc: 0.5775 - val_loss: 0.6499 - val_acc: 0.5833\n",
      "Epoch 53/96\n",
      "1760/1760 [==============================] - 212s 121ms/step - loss: 0.6504 - acc: 0.5748 - val_loss: 0.6497 - val_acc: 0.5833\n",
      "Epoch 54/96\n",
      "1760/1760 [==============================] - 213s 121ms/step - loss: 0.6503 - acc: 0.5814 - val_loss: 0.6498 - val_acc: 0.5833\n",
      "Epoch 55/96\n",
      "1760/1760 [==============================] - 212s 120ms/step - loss: 0.6503 - acc: 0.5684 - val_loss: 0.6497 - val_acc: 0.5833\n",
      "Epoch 56/96\n",
      "1760/1760 [==============================] - 212s 121ms/step - loss: 0.6500 - acc: 0.5883 - val_loss: 0.6496 - val_acc: 0.5833\n",
      "Epoch 57/96\n",
      "1760/1760 [==============================] - 213s 121ms/step - loss: 0.6502 - acc: 0.5818 - val_loss: 0.6496 - val_acc: 0.5833\n",
      "Epoch 58/96\n",
      "1760/1760 [==============================] - 212s 121ms/step - loss: 0.6502 - acc: 0.5860 - val_loss: 0.6498 - val_acc: 0.5833\n",
      "Epoch 59/96\n",
      "1760/1760 [==============================] - 212s 121ms/step - loss: 0.6499 - acc: 0.5795 - val_loss: 0.6496 - val_acc: 0.5833\n",
      "Epoch 60/96\n",
      "1760/1760 [==============================] - 214s 121ms/step - loss: 0.6499 - acc: 0.5746 - val_loss: 0.6497 - val_acc: 0.5833\n",
      "Epoch 61/96\n",
      "1760/1760 [==============================] - 213s 121ms/step - loss: 0.6498 - acc: 0.5833 - val_loss: 0.6496 - val_acc: 0.5833\n",
      "Epoch 62/96\n",
      "1760/1760 [==============================] - 211s 120ms/step - loss: 0.6502 - acc: 0.5765 - val_loss: 0.6496 - val_acc: 0.5833\n",
      "Epoch 63/96\n",
      "1760/1760 [==============================] - 214s 122ms/step - loss: 0.6502 - acc: 0.5799 - val_loss: 0.6496 - val_acc: 0.5833\n",
      "Epoch 64/96\n",
      "1760/1760 [==============================] - 213s 121ms/step - loss: 0.6502 - acc: 0.5797 - val_loss: 0.6496 - val_acc: 0.5833\n",
      "Epoch 65/96\n",
      "1760/1760 [==============================] - 214s 121ms/step - loss: 0.6502 - acc: 0.5805 - val_loss: 0.6496 - val_acc: 0.5833\n",
      "Epoch 66/96\n",
      "1760/1760 [==============================] - 214s 122ms/step - loss: 0.6501 - acc: 0.5759 - val_loss: 0.6497 - val_acc: 0.5833\n",
      "Epoch 67/96\n",
      "1760/1760 [==============================] - 213s 121ms/step - loss: 0.6505 - acc: 0.5775 - val_loss: 0.6497 - val_acc: 0.5833\n",
      "Epoch 68/96\n",
      "1760/1760 [==============================] - 213s 121ms/step - loss: 0.6502 - acc: 0.5737 - val_loss: 0.6497 - val_acc: 0.5833\n",
      "Epoch 69/96\n",
      "1760/1760 [==============================] - 214s 122ms/step - loss: 0.6503 - acc: 0.5748 - val_loss: 0.6496 - val_acc: 0.5833\n",
      "Epoch 00069: early stopping\n",
      "Generate Model - Ini: 2020-03-08 20:53:56\tEnd: 2020-03-09 01:00:25\tTotal: 4:06:28.649605\n",
      "Total experiment - Ini: 2020-03-08 20:53:56\tEnd: 2020-03-09 01:00:25\tTotal: 4:06:28.792612\n",
      "Loading data... /home/vanessa/PycharmProjects/RecurrentNetworks/tokenizers/anx_dep_multilabel/SMHD_TR_1760_VS_5000_TF_0_DF_0_RSW_F_IT_2_RP_F\n",
      "Preprocess data... /home/vanessa/PycharmProjects/RecurrentNetworks/dataset/anx_dep_multilabel/SMHD_test_1760.df\n",
      "Load data - Ini: 2020-03-09 01:00:25\tEnd: 2020-03-09 01:00:45\tTotal: 0:00:20.162086\n",
      "1760/1760 [==============================] - 44s 25ms/step\n"
     ]
    }
   ],
   "source": [
    "print('Initializer experiment 1.1 (model SMHD_ml_gl_1760)\\n'+\\\n",
    "      'Set: kernel_initializer=glorot_uniform=xavier_uniform, dataset=SMHD_1760 multi-label')\n",
    "\n",
    "exp = ExperimentProcesses('lstm_exp14_L3')\n",
    "exp.pp_data.set_dataset_source(dataset_name='SMHD', label_set=['control', 'anxiety', 'depression'],\n",
    "                               total_registers=1760, subdirectory=\"anx_dep_multilabel\")\n",
    "\n",
    "generate_lstm_exp14(exp, 'exp14_ml_gl_1760')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0 Train, validate and test experiment lstm_exp9_var_L3_N16_B40_E32_D0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model(exp, name_model, we_file_name, kernel_function='glorot_uniform'):\n",
    "    exp.pp_data.vocabulary_size = 5000\n",
    "\n",
    "    exp.pp_data.embedding_size = 300\n",
    "    exp.pp_data.max_posts = 1750\n",
    "    exp.pp_data.max_terms_by_post = 300\n",
    "    exp.pp_data.binary_classifier = True\n",
    "    exp.pp_data.format_input_data = dn.InputData.POSTS_ONLY_TEXT\n",
    "    exp.pp_data.remove_stopwords = False\n",
    "    exp.pp_data.delete_low_tfid = False\n",
    "    exp.pp_data.min_df = 0\n",
    "    exp.pp_data.min_tf = 0\n",
    "    exp.pp_data.random_posts = False\n",
    "    exp.pp_data.random_users = False\n",
    "    exp.pp_data.tokenizing_type = 'WE'\n",
    "    exp.pp_data.load_dataset_type = dn.LoadDataset.TRAIN_DATA_MODEL\n",
    "    exp.pp_data.type_prediction_label= dn.TypePredictionLabel.MULTI_LABEL_CATEGORICAL\n",
    "\n",
    "    exp.use_custom_metrics = False\n",
    "    exp.use_valid_set_for_train = True\n",
    "    exp.valid_split_from_train_set = 0.0\n",
    "    exp.imbalanced_classes = False\n",
    "\n",
    "    lstm = ModelClass(1)\n",
    "    lstm.loss_function = 'binary_crossentropy'\n",
    "    lstm.optmizer_function = 'adam'\n",
    "    lstm.epochs = 15\n",
    "    lstm.batch_size = 32\n",
    "    lstm.patience_train = 10\n",
    "    lstm.use_embedding_pre_train = exp.pp_data.use_embedding\n",
    "    lstm.embed_trainable = (lstm.use_embedding_pre_train == (dn.UseEmbedding.RAND or dn.UseEmbedding.NON_STATIC))\n",
    "\n",
    "    neuronios_by_layer = [16]\n",
    "    epochs = [32]\n",
    "    batch_sizes = [40]\n",
    "    dropouts = [0.2]\n",
    "\n",
    "    np.random.seed(dn.SEED)\n",
    "\n",
    "    time_ini_rep = datetime.datetime.now()\n",
    "    x_train, y_train, x_valid, y_valid, num_words, embedding_matrix = exp.pp_data.load_data()\n",
    "    exp.set_period_time_end(time_ini_rep, 'Load data')\n",
    "\n",
    "    for neuronios in neuronios_by_layer:\n",
    "        for batch_size in batch_sizes:\n",
    "            for epoch in epochs:\n",
    "                for dropout in dropouts:\n",
    "                    lstm.epochs = epoch\n",
    "                    lstm.batch_size = batch_size\n",
    "                    lstm.patience_train = epoch / 2\n",
    "                    exp.experiment_name = name_model + '_lstm_exp9_var_L3' + '_N' + str(neuronios) + '_B' +\\\n",
    "                                          str(batch_size) + '_E' + str(epoch) + '_D' + str(dropout) + '_' +\\\n",
    "                                          we_file_name\n",
    "\n",
    "                    lstm.model = Sequential()\n",
    "                    lstm.model.add(Embedding(exp.pp_data.vocabulary_size, exp.pp_data.embedding_size,\n",
    "                                             trainable=lstm.embed_trainable, name='emb_' + name_model))\n",
    "                    lstm.model.add(LSTM(neuronios, kernel_initializer=kernel_function,\n",
    "                                        activation='tanh', dropout=dropout, recurrent_dropout=dropout,\n",
    "                                        return_sequences=True, name='dense_1_' + name_model))\n",
    "                    lstm.model.add(LSTM(neuronios, kernel_initializer=kernel_function,\n",
    "                                        activation='tanh', dropout=dropout, recurrent_dropout=dropout,\n",
    "                                        return_sequences=True, name='dense_2_' + name_model))\n",
    "                    lstm.model.add(LSTM(neuronios, kernel_initializer=kernel_function,\n",
    "                                        activation='tanh', dropout=dropout, recurrent_dropout=dropout,\n",
    "                                        name='dense_3_' + name_model))\n",
    "                    lstm.model.add(Dense(3,\n",
    "                                         activation='sigmoid',\n",
    "                                         name='dense_4_' + name_model))\n",
    "\n",
    "                    time_ini_exp = datetime.datetime.now()\n",
    "                    exp.generate_model_hypeparams(lstm, x_train, y_train, x_valid, y_valid, embedding_matrix)\n",
    "                    exp.set_period_time_end(time_ini_exp, 'Total experiment')\n",
    "\n",
    "    del x_train, y_train, x_valid, y_valid, num_words, embedding_matrix\n",
    "\n",
    "    # Test\n",
    "    exp.pp_data.load_dataset_type = dn.LoadDataset.TEST_DATA_MODEL\n",
    "    np.random.seed(dn.SEED)\n",
    "    time_ini_rep = datetime.datetime.now()\n",
    "    x_test, y_test = exp.pp_data.load_data()\n",
    "    exp.set_period_time_end(time_ini_rep, 'Load data')\n",
    "\n",
    "    for neuronios in neuronios_by_layer:\n",
    "        for batch_size in batch_sizes:\n",
    "            for epoch in epochs:\n",
    "                for dropout in dropouts:\n",
    "                    lstm.epochs = epoch\n",
    "                    lstm.batch_size = batch_size\n",
    "                    lstm.patience_train = epoch / 2\n",
    "                    exp.experiment_name = name_model + '_lstm_exp9_var_L3' + '_N' + str(neuronios) + '_B' +\\ \n",
    "                                          str(batch_size) + '_E' + str(epoch) + '_D' + str(dropout) + '_' +\\ \n",
    "                                          we_file_name\n",
    "\n",
    "                    lstm.model = exp.load_model(dn.PATH_PROJECT + exp.experiment_name + '.h5')\n",
    "                    exp.save_geral_configs()\n",
    "                    exp.save_summary_model(lstm.model)\n",
    "                    exp.predict_samples(lstm, x_test, y_test)\n",
    "\n",
    "    del x_test, y_test, lstm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Test lstm_exp9_var_L3_N16_B40_E32_D0.2 + word embeddings SMHD all users (SMHD_*_1760)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializer experiment 1.1 (model SMHD_ml_gl_1760)\n",
      "Set: kernel_initializer=glorot_uniform=xavier_uniform, dataset=SMHD_1760 multi-label\n",
      "Loading data... /home/vanessa/PycharmProjects/RecurrentNetworks/tokenizers/anx_dep_multilabel/SMHD_TR_1760_VS_5000_TF_0_DF_0_RSW_F_IT_1_RP_F\n",
      "Preprocess data... /home/vanessa/PycharmProjects/RecurrentNetworks/dataset/anx_dep_multilabel/SMHD_train_1760.df\n",
      "Load data - Ini: 2020-03-09 09:09:44\tEnd: 2020-03-09 09:10:01\tTotal: 0:00:17.053792\n",
      "Training using single GPU or CPU..\n",
      "Train on 1760 samples, validate on 1760 samples\n",
      "Epoch 1/32\n",
      "1760/1760 [==============================] - 434s 247ms/step - loss: 0.6666 - acc: 0.5771 - val_loss: 0.6493 - val_acc: 0.5958\n",
      "Epoch 2/32\n",
      "1760/1760 [==============================] - 429s 244ms/step - loss: 0.6529 - acc: 0.5811 - val_loss: 0.6484 - val_acc: 0.5858\n",
      "Epoch 3/32\n",
      "1760/1760 [==============================] - 422s 240ms/step - loss: 0.6484 - acc: 0.5869 - val_loss: 0.6447 - val_acc: 0.6055\n",
      "Epoch 4/32\n",
      "1760/1760 [==============================] - 431s 245ms/step - loss: 0.6391 - acc: 0.6119 - val_loss: 0.6321 - val_acc: 0.6307\n",
      "Epoch 5/32\n",
      "1760/1760 [==============================] - 423s 240ms/step - loss: 0.6310 - acc: 0.6259 - val_loss: 0.6318 - val_acc: 0.6347\n",
      "Epoch 6/32\n",
      "1760/1760 [==============================] - 410s 233ms/step - loss: 0.6250 - acc: 0.6371 - val_loss: 0.6283 - val_acc: 0.6311\n",
      "Epoch 7/32\n",
      "1760/1760 [==============================] - 411s 234ms/step - loss: 0.6200 - acc: 0.6428 - val_loss: 0.6226 - val_acc: 0.6356\n",
      "Epoch 8/32\n",
      "1760/1760 [==============================] - 411s 233ms/step - loss: 0.6131 - acc: 0.6492 - val_loss: 0.6209 - val_acc: 0.6422\n",
      "Epoch 9/32\n",
      "1760/1760 [==============================] - 411s 234ms/step - loss: 0.6164 - acc: 0.6438 - val_loss: 0.6671 - val_acc: 0.6070\n",
      "Epoch 10/32\n",
      "1760/1760 [==============================] - 412s 234ms/step - loss: 0.6330 - acc: 0.6277 - val_loss: 0.6262 - val_acc: 0.6309\n",
      "Epoch 11/32\n",
      "1760/1760 [==============================] - 412s 234ms/step - loss: 0.6137 - acc: 0.6504 - val_loss: 0.6227 - val_acc: 0.6398\n",
      "Epoch 12/32\n",
      "1760/1760 [==============================] - 411s 233ms/step - loss: 0.6086 - acc: 0.6553 - val_loss: 0.6212 - val_acc: 0.6426\n",
      "Epoch 13/32\n",
      "1760/1760 [==============================] - 412s 234ms/step - loss: 0.6058 - acc: 0.6576 - val_loss: 0.6204 - val_acc: 0.6426\n",
      "Epoch 14/32\n",
      "1760/1760 [==============================] - 410s 233ms/step - loss: 0.6081 - acc: 0.6534 - val_loss: 0.6310 - val_acc: 0.6242\n",
      "Epoch 15/32\n",
      "1760/1760 [==============================] - 412s 234ms/step - loss: 0.6051 - acc: 0.6608 - val_loss: 0.6196 - val_acc: 0.6445\n",
      "Epoch 16/32\n",
      "1760/1760 [==============================] - 410s 233ms/step - loss: 0.6066 - acc: 0.6591 - val_loss: 0.6240 - val_acc: 0.6373\n",
      "Epoch 17/32\n",
      "1760/1760 [==============================] - 411s 234ms/step - loss: 0.6011 - acc: 0.6642 - val_loss: 0.6222 - val_acc: 0.6383\n",
      "Epoch 18/32\n",
      "1760/1760 [==============================] - 443s 252ms/step - loss: 0.6002 - acc: 0.6614 - val_loss: 0.6189 - val_acc: 0.6441\n",
      "Epoch 19/32\n",
      "1760/1760 [==============================] - 425s 241ms/step - loss: 0.5982 - acc: 0.6617 - val_loss: 0.6189 - val_acc: 0.6415\n",
      "Epoch 20/32\n",
      "1760/1760 [==============================] - 421s 239ms/step - loss: 0.5910 - acc: 0.6737 - val_loss: 0.6184 - val_acc: 0.6479\n",
      "Epoch 21/32\n",
      "1760/1760 [==============================] - 419s 238ms/step - loss: 0.5899 - acc: 0.6803 - val_loss: 0.6175 - val_acc: 0.6477\n",
      "Epoch 22/32\n",
      "1760/1760 [==============================] - 415s 236ms/step - loss: 0.5879 - acc: 0.6767 - val_loss: 0.6179 - val_acc: 0.6441\n",
      "Epoch 23/32\n",
      "1760/1760 [==============================] - 414s 235ms/step - loss: 0.5787 - acc: 0.6862 - val_loss: 0.6183 - val_acc: 0.6464\n",
      "Epoch 24/32\n",
      "1760/1760 [==============================] - 414s 235ms/step - loss: 0.5837 - acc: 0.6811 - val_loss: 0.6240 - val_acc: 0.6456\n",
      "Epoch 25/32\n",
      "1760/1760 [==============================] - 409s 233ms/step - loss: 0.5840 - acc: 0.6860 - val_loss: 0.6511 - val_acc: 0.6292\n",
      "Epoch 26/32\n",
      "1760/1760 [==============================] - 407s 231ms/step - loss: 0.5887 - acc: 0.6797 - val_loss: 0.6238 - val_acc: 0.6445\n",
      "Epoch 27/32\n",
      "1760/1760 [==============================] - 408s 232ms/step - loss: 0.5725 - acc: 0.6903 - val_loss: 0.6161 - val_acc: 0.6551\n",
      "Epoch 28/32\n",
      "1760/1760 [==============================] - 408s 232ms/step - loss: 0.5744 - acc: 0.6915 - val_loss: 0.6166 - val_acc: 0.6481\n",
      "Epoch 29/32\n",
      "1760/1760 [==============================] - 408s 232ms/step - loss: 0.5706 - acc: 0.6981 - val_loss: 0.6282 - val_acc: 0.6347\n",
      "Epoch 30/32\n",
      "1760/1760 [==============================] - 412s 234ms/step - loss: 0.5660 - acc: 0.6973 - val_loss: 0.6199 - val_acc: 0.6538\n",
      "Epoch 31/32\n",
      "1760/1760 [==============================] - 413s 235ms/step - loss: 0.5605 - acc: 0.6998 - val_loss: 0.6193 - val_acc: 0.6532\n",
      "Epoch 32/32\n",
      "1760/1760 [==============================] - 412s 234ms/step - loss: 0.5571 - acc: 0.7025 - val_loss: 0.6230 - val_acc: 0.6441\n",
      "Generate Model - Ini: 2020-03-09 09:10:03\tEnd: 2020-03-09 12:51:47\tTotal: 3:41:43.780755\n",
      "Total experiment - Ini: 2020-03-09 09:10:02\tEnd: 2020-03-09 12:51:47\tTotal: 3:41:45.391457\n",
      "Loading data... /home/vanessa/PycharmProjects/RecurrentNetworks/tokenizers/anx_dep_multilabel/SMHD_TR_1760_VS_5000_TF_0_DF_0_RSW_F_IT_1_RP_F\n",
      "Preprocess data... /home/vanessa/PycharmProjects/RecurrentNetworks/dataset/anx_dep_multilabel/SMHD_test_1760.df\n",
      "Load data - Ini: 2020-03-09 12:51:47\tEnd: 2020-03-09 12:51:56\tTotal: 0:00:08.628486\n",
      "1760/1760 [==============================] - 55s 31ms/step\n",
      "Loading data... /home/vanessa/PycharmProjects/RecurrentNetworks/tokenizers/anx_dep_multilabel/SMHD_TR_1760_VS_5000_TF_0_DF_0_RSW_F_IT_1_RP_F\n",
      "Preprocess data... /home/vanessa/PycharmProjects/RecurrentNetworks/dataset/anx_dep_multilabel/SMHD_train_1760.df\n",
      "Load data - Ini: 2020-03-09 12:52:58\tEnd: 2020-03-09 12:53:15\tTotal: 0:00:17.408151\n",
      "Training using single GPU or CPU..\n",
      "Train on 1760 samples, validate on 1760 samples\n",
      "Epoch 1/32\n",
      "1760/1760 [==============================] - 432s 246ms/step - loss: 0.6712 - acc: 0.5816 - val_loss: 0.6496 - val_acc: 0.5833\n",
      "Epoch 2/32\n",
      "1760/1760 [==============================] - 404s 229ms/step - loss: 0.6526 - acc: 0.5811 - val_loss: 0.6486 - val_acc: 0.5975\n",
      "Epoch 3/32\n",
      "1760/1760 [==============================] - 404s 229ms/step - loss: 0.6488 - acc: 0.5818 - val_loss: 0.6481 - val_acc: 0.5807\n",
      "Epoch 4/32\n",
      "1760/1760 [==============================] - 410s 233ms/step - loss: 0.6469 - acc: 0.5949 - val_loss: 0.6463 - val_acc: 0.6129\n",
      "Epoch 5/32\n",
      "1760/1760 [==============================] - 409s 232ms/step - loss: 0.6408 - acc: 0.6034 - val_loss: 0.6448 - val_acc: 0.6117\n",
      "Epoch 6/32\n",
      "1760/1760 [==============================] - 408s 232ms/step - loss: 0.6335 - acc: 0.6284 - val_loss: 0.6522 - val_acc: 0.5941\n",
      "Epoch 7/32\n",
      "1760/1760 [==============================] - 402s 228ms/step - loss: 0.6308 - acc: 0.6258 - val_loss: 0.6434 - val_acc: 0.6144\n",
      "Epoch 8/32\n",
      "1760/1760 [==============================] - 400s 228ms/step - loss: 0.6196 - acc: 0.6445 - val_loss: 0.6415 - val_acc: 0.6157\n",
      "Epoch 9/32\n",
      "1760/1760 [==============================] - 400s 227ms/step - loss: 0.6139 - acc: 0.6542 - val_loss: 0.6438 - val_acc: 0.6188\n",
      "Epoch 10/32\n",
      "1760/1760 [==============================] - 400s 227ms/step - loss: 0.6340 - acc: 0.6284 - val_loss: 0.6347 - val_acc: 0.6259\n",
      "Epoch 11/32\n",
      "1760/1760 [==============================] - 399s 227ms/step - loss: 0.6261 - acc: 0.6352 - val_loss: 0.6405 - val_acc: 0.6165\n",
      "Epoch 12/32\n",
      "1760/1760 [==============================] - 401s 228ms/step - loss: 0.6165 - acc: 0.6458 - val_loss: 0.6452 - val_acc: 0.6144\n",
      "Epoch 13/32\n",
      "1760/1760 [==============================] - 401s 228ms/step - loss: 0.6086 - acc: 0.6557 - val_loss: 0.6497 - val_acc: 0.6015\n",
      "Epoch 14/32\n",
      "1760/1760 [==============================] - 402s 228ms/step - loss: 0.6087 - acc: 0.6563 - val_loss: 0.6446 - val_acc: 0.6182\n",
      "Epoch 15/32\n",
      "1760/1760 [==============================] - 400s 227ms/step - loss: 0.6058 - acc: 0.6559 - val_loss: 0.6473 - val_acc: 0.6148\n",
      "Epoch 16/32\n",
      "1760/1760 [==============================] - 401s 228ms/step - loss: 0.6082 - acc: 0.6581 - val_loss: 0.6457 - val_acc: 0.6161\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/32\n",
      "1760/1760 [==============================] - 401s 228ms/step - loss: 0.6066 - acc: 0.6542 - val_loss: 0.6533 - val_acc: 0.5977\n",
      "Epoch 18/32\n",
      "1760/1760 [==============================] - 401s 228ms/step - loss: 0.5987 - acc: 0.6689 - val_loss: 0.6512 - val_acc: 0.6180\n",
      "Epoch 19/32\n",
      "1760/1760 [==============================] - 401s 228ms/step - loss: 0.6017 - acc: 0.6644 - val_loss: 0.6491 - val_acc: 0.6144\n",
      "Epoch 20/32\n",
      "1760/1760 [==============================] - 401s 228ms/step - loss: 0.6026 - acc: 0.6621 - val_loss: 0.6519 - val_acc: 0.6093\n",
      "Epoch 21/32\n",
      "1760/1760 [==============================] - 401s 228ms/step - loss: 0.5974 - acc: 0.6669 - val_loss: 0.6593 - val_acc: 0.5941\n",
      "Epoch 22/32\n",
      "1760/1760 [==============================] - 401s 228ms/step - loss: 0.5947 - acc: 0.6714 - val_loss: 0.6513 - val_acc: 0.6106\n",
      "Epoch 23/32\n",
      "1760/1760 [==============================] - 399s 227ms/step - loss: 0.5964 - acc: 0.6680 - val_loss: 0.6480 - val_acc: 0.6186\n",
      "Epoch 24/32\n",
      "1760/1760 [==============================] - 400s 227ms/step - loss: 0.5974 - acc: 0.6693 - val_loss: 0.6491 - val_acc: 0.6188\n",
      "Epoch 25/32\n",
      "1760/1760 [==============================] - 400s 227ms/step - loss: 0.5944 - acc: 0.6727 - val_loss: 0.6530 - val_acc: 0.6127\n",
      "Epoch 26/32\n",
      "1760/1760 [==============================] - 401s 228ms/step - loss: 0.5925 - acc: 0.6750 - val_loss: 0.6586 - val_acc: 0.6125\n",
      "Epoch 00026: early stopping\n",
      "Generate Model - Ini: 2020-03-09 12:53:16\tEnd: 2020-03-09 15:48:01\tTotal: 2:54:45.193328\n",
      "Total experiment - Ini: 2020-03-09 12:53:16\tEnd: 2020-03-09 15:48:01\tTotal: 2:54:45.193915\n",
      "Loading data... /home/vanessa/PycharmProjects/RecurrentNetworks/tokenizers/anx_dep_multilabel/SMHD_TR_1760_VS_5000_TF_0_DF_0_RSW_F_IT_1_RP_F\n",
      "Preprocess data... /home/vanessa/PycharmProjects/RecurrentNetworks/dataset/anx_dep_multilabel/SMHD_test_1760.df\n",
      "Load data - Ini: 2020-03-09 15:48:01\tEnd: 2020-03-09 15:48:11\tTotal: 0:00:09.706965\n",
      "1760/1760 [==============================] - 53s 30ms/step\n",
      "Loading data... /home/vanessa/PycharmProjects/RecurrentNetworks/tokenizers/anx_dep_multilabel/SMHD_TR_1760_VS_5000_TF_0_DF_0_RSW_F_IT_1_RP_F\n",
      "Preprocess data... /home/vanessa/PycharmProjects/RecurrentNetworks/dataset/anx_dep_multilabel/SMHD_train_1760.df\n",
      "Load Pre-train embeddings\n",
      "Load data - Ini: 2020-03-09 15:49:14\tEnd: 2020-03-09 15:49:36\tTotal: 0:00:22.472976\n",
      "Training using single GPU or CPU..\n",
      "Train on 1760 samples, validate on 1760 samples\n",
      "Epoch 1/32\n",
      "1760/1760 [==============================] - 435s 247ms/step - loss: 0.6712 - acc: 0.5780 - val_loss: 0.6504 - val_acc: 0.5833\n",
      "Epoch 2/32\n",
      "1760/1760 [==============================] - 423s 240ms/step - loss: 0.6519 - acc: 0.5773 - val_loss: 0.6490 - val_acc: 0.5833\n",
      "Epoch 3/32\n",
      "1760/1760 [==============================] - 426s 242ms/step - loss: 0.6497 - acc: 0.5814 - val_loss: 0.6481 - val_acc: 0.5814\n",
      "Epoch 4/32\n",
      "1760/1760 [==============================] - 409s 232ms/step - loss: 0.6485 - acc: 0.5811 - val_loss: 0.6468 - val_acc: 0.5977\n",
      "Epoch 5/32\n",
      "1760/1760 [==============================] - 411s 234ms/step - loss: 0.6442 - acc: 0.5902 - val_loss: 0.6425 - val_acc: 0.6047\n",
      "Epoch 6/32\n",
      "1760/1760 [==============================] - 412s 234ms/step - loss: 0.6366 - acc: 0.6106 - val_loss: 0.6412 - val_acc: 0.6193\n",
      "Epoch 7/32\n",
      "1760/1760 [==============================] - 412s 234ms/step - loss: 0.6303 - acc: 0.6254 - val_loss: 0.6440 - val_acc: 0.6112\n",
      "Epoch 8/32\n",
      "1760/1760 [==============================] - 417s 237ms/step - loss: 0.6247 - acc: 0.6386 - val_loss: 0.6430 - val_acc: 0.6097\n",
      "Epoch 9/32\n",
      "1760/1760 [==============================] - 424s 241ms/step - loss: 0.6224 - acc: 0.6409 - val_loss: 0.6422 - val_acc: 0.6165\n",
      "Epoch 10/32\n",
      "1760/1760 [==============================] - 423s 240ms/step - loss: 0.6203 - acc: 0.6400 - val_loss: 0.6404 - val_acc: 0.6222\n",
      "Epoch 11/32\n",
      "1760/1760 [==============================] - 423s 240ms/step - loss: 0.6200 - acc: 0.6396 - val_loss: 0.6410 - val_acc: 0.6186\n",
      "Epoch 12/32\n",
      "1760/1760 [==============================] - 422s 240ms/step - loss: 0.6131 - acc: 0.6511 - val_loss: 0.6459 - val_acc: 0.6140\n",
      "Epoch 13/32\n",
      "1760/1760 [==============================] - 423s 240ms/step - loss: 0.6073 - acc: 0.6523 - val_loss: 0.6470 - val_acc: 0.6080\n",
      "Epoch 14/32\n",
      "1760/1760 [==============================] - 421s 239ms/step - loss: 0.6199 - acc: 0.6464 - val_loss: 0.6409 - val_acc: 0.6134\n",
      "Epoch 15/32\n",
      "1760/1760 [==============================] - 423s 240ms/step - loss: 0.6093 - acc: 0.6506 - val_loss: 0.6504 - val_acc: 0.6093\n",
      "Epoch 16/32\n",
      "1760/1760 [==============================] - 422s 240ms/step - loss: 0.6065 - acc: 0.6544 - val_loss: 0.6480 - val_acc: 0.6150\n",
      "Epoch 17/32\n",
      "1760/1760 [==============================] - 421s 239ms/step - loss: 0.6045 - acc: 0.6589 - val_loss: 0.6537 - val_acc: 0.5994\n",
      "Epoch 18/32\n",
      "1760/1760 [==============================] - 422s 240ms/step - loss: 0.6041 - acc: 0.6610 - val_loss: 0.6523 - val_acc: 0.6044\n",
      "Epoch 19/32\n",
      "1760/1760 [==============================] - 423s 240ms/step - loss: 0.6010 - acc: 0.6661 - val_loss: 0.6538 - val_acc: 0.6004\n",
      "Epoch 20/32\n",
      "1760/1760 [==============================] - 423s 240ms/step - loss: 0.5968 - acc: 0.6703 - val_loss: 0.6629 - val_acc: 0.6053\n",
      "Epoch 21/32\n",
      "1760/1760 [==============================] - 423s 240ms/step - loss: 0.6034 - acc: 0.6585 - val_loss: 0.6520 - val_acc: 0.6049\n",
      "Epoch 22/32\n",
      "1760/1760 [==============================] - 423s 240ms/step - loss: 0.6008 - acc: 0.6625 - val_loss: 0.6528 - val_acc: 0.6097\n",
      "Epoch 23/32\n",
      "1760/1760 [==============================] - 423s 240ms/step - loss: 0.5937 - acc: 0.6729 - val_loss: 0.6635 - val_acc: 0.6036\n",
      "Epoch 24/32\n",
      "1760/1760 [==============================] - 423s 240ms/step - loss: 0.5934 - acc: 0.6716 - val_loss: 0.6553 - val_acc: 0.6036\n",
      "Epoch 25/32\n",
      "1760/1760 [==============================] - 418s 237ms/step - loss: 0.5906 - acc: 0.6801 - val_loss: 0.6672 - val_acc: 0.5879\n",
      "Epoch 26/32\n",
      "1760/1760 [==============================] - 408s 232ms/step - loss: 0.5921 - acc: 0.6758 - val_loss: 0.6613 - val_acc: 0.6042\n",
      "Epoch 00026: early stopping\n",
      "Generate Model - Ini: 2020-03-09 15:49:41\tEnd: 2020-03-09 18:51:58\tTotal: 3:02:17.073863\n",
      "Total experiment - Ini: 2020-03-09 15:49:37\tEnd: 2020-03-09 18:51:58\tTotal: 3:02:21.058573\n",
      "Loading data... /home/vanessa/PycharmProjects/RecurrentNetworks/tokenizers/anx_dep_multilabel/SMHD_TR_1760_VS_5000_TF_0_DF_0_RSW_F_IT_1_RP_F\n",
      "Preprocess data... /home/vanessa/PycharmProjects/RecurrentNetworks/dataset/anx_dep_multilabel/SMHD_test_1760.df\n",
      "Load data - Ini: 2020-03-09 18:51:58\tEnd: 2020-03-09 18:52:07\tTotal: 0:00:08.960395\n",
      "1760/1760 [==============================] - 55s 31ms/step\n",
      "Loading data... /home/vanessa/PycharmProjects/RecurrentNetworks/tokenizers/anx_dep_multilabel/SMHD_TR_1760_VS_5000_TF_0_DF_0_RSW_F_IT_1_RP_F\n",
      "Preprocess data... /home/vanessa/PycharmProjects/RecurrentNetworks/dataset/anx_dep_multilabel/SMHD_train_1760.df\n",
      "Load data - Ini: 2020-03-09 18:53:14\tEnd: 2020-03-09 18:53:31\tTotal: 0:00:17.618198\n",
      "Training using single GPU or CPU..\n",
      "Train on 1760 samples, validate on 1760 samples\n",
      "Epoch 1/32\n",
      "1760/1760 [==============================] - 455s 259ms/step - loss: 0.6712 - acc: 0.5816 - val_loss: 0.6496 - val_acc: 0.5833\n",
      "Epoch 2/32\n",
      "1760/1760 [==============================] - 422s 240ms/step - loss: 0.6526 - acc: 0.5809 - val_loss: 0.6486 - val_acc: 0.5973\n",
      "Epoch 3/32\n",
      "1760/1760 [==============================] - 412s 234ms/step - loss: 0.6488 - acc: 0.5816 - val_loss: 0.6481 - val_acc: 0.5807\n",
      "Epoch 4/32\n",
      "1760/1760 [==============================] - 418s 237ms/step - loss: 0.6469 - acc: 0.5949 - val_loss: 0.6463 - val_acc: 0.6131\n",
      "Epoch 5/32\n",
      "1760/1760 [==============================] - 393s 223ms/step - loss: 0.6408 - acc: 0.6030 - val_loss: 0.6448 - val_acc: 0.6117\n",
      "Epoch 6/32\n",
      "1760/1760 [==============================] - 390s 221ms/step - loss: 0.6339 - acc: 0.6284 - val_loss: 0.6528 - val_acc: 0.5941\n",
      "Epoch 7/32\n",
      "1760/1760 [==============================] - 389s 221ms/step - loss: 0.6315 - acc: 0.6244 - val_loss: 0.6434 - val_acc: 0.6152\n",
      "Epoch 8/32\n",
      "1760/1760 [==============================] - 393s 223ms/step - loss: 0.6188 - acc: 0.6443 - val_loss: 0.6438 - val_acc: 0.6170\n",
      "Epoch 9/32\n",
      "1760/1760 [==============================] - 398s 226ms/step - loss: 0.6155 - acc: 0.6496 - val_loss: 0.6520 - val_acc: 0.6189\n",
      "Epoch 10/32\n",
      "1760/1760 [==============================] - 397s 226ms/step - loss: 0.6165 - acc: 0.6443 - val_loss: 0.6445 - val_acc: 0.6199\n",
      "Epoch 11/32\n",
      "1760/1760 [==============================] - 396s 225ms/step - loss: 0.6117 - acc: 0.6504 - val_loss: 0.6445 - val_acc: 0.6121\n",
      "Epoch 12/32\n",
      "1760/1760 [==============================] - 397s 226ms/step - loss: 0.6118 - acc: 0.6502 - val_loss: 0.6472 - val_acc: 0.6138\n",
      "Epoch 13/32\n",
      "1760/1760 [==============================] - 395s 225ms/step - loss: 0.6087 - acc: 0.6559 - val_loss: 0.6452 - val_acc: 0.6170\n",
      "Epoch 14/32\n",
      "1760/1760 [==============================] - 396s 225ms/step - loss: 0.6056 - acc: 0.6612 - val_loss: 0.6499 - val_acc: 0.6076\n",
      "Epoch 15/32\n",
      "1760/1760 [==============================] - 395s 225ms/step - loss: 0.6047 - acc: 0.6566 - val_loss: 0.6470 - val_acc: 0.6206\n",
      "Epoch 16/32\n",
      "1760/1760 [==============================] - 396s 225ms/step - loss: 0.6058 - acc: 0.6606 - val_loss: 0.6498 - val_acc: 0.6114\n",
      "Epoch 17/32\n",
      "1760/1760 [==============================] - 395s 225ms/step - loss: 0.6024 - acc: 0.6638 - val_loss: 0.6508 - val_acc: 0.6072\n",
      "Epoch 18/32\n",
      "1760/1760 [==============================] - 395s 225ms/step - loss: 0.6393 - acc: 0.6244 - val_loss: 0.7175 - val_acc: 0.5833\n",
      "Epoch 19/32\n",
      "1760/1760 [==============================] - 395s 224ms/step - loss: 0.6569 - acc: 0.5886 - val_loss: 0.6495 - val_acc: 0.5820\n",
      "Epoch 20/32\n",
      "1760/1760 [==============================] - 395s 225ms/step - loss: 0.6497 - acc: 0.5998 - val_loss: 0.6492 - val_acc: 0.5828\n",
      "Epoch 21/32\n",
      "1760/1760 [==============================] - 395s 225ms/step - loss: 0.6482 - acc: 0.5985 - val_loss: 0.6490 - val_acc: 0.5902\n",
      "Epoch 22/32\n",
      "1760/1760 [==============================] - 394s 224ms/step - loss: 0.6452 - acc: 0.6036 - val_loss: 0.6483 - val_acc: 0.5991\n",
      "Epoch 23/32\n",
      "1760/1760 [==============================] - 395s 224ms/step - loss: 0.6453 - acc: 0.6070 - val_loss: 0.6475 - val_acc: 0.6055\n",
      "Epoch 00023: early stopping\n",
      "Generate Model - Ini: 2020-03-09 18:53:32\tEnd: 2020-03-09 21:27:09\tTotal: 2:33:37.570178\n",
      "Total experiment - Ini: 2020-03-09 18:53:32\tEnd: 2020-03-09 21:27:09\tTotal: 2:33:37.570751\n",
      "Loading data... /home/vanessa/PycharmProjects/RecurrentNetworks/tokenizers/anx_dep_multilabel/SMHD_TR_1760_VS_5000_TF_0_DF_0_RSW_F_IT_1_RP_F\n",
      "Preprocess data... /home/vanessa/PycharmProjects/RecurrentNetworks/dataset/anx_dep_multilabel/SMHD_test_1760.df\n",
      "Load data - Ini: 2020-03-09 21:27:09\tEnd: 2020-03-09 21:27:18\tTotal: 0:00:08.605496\n",
      "1760/1760 [==============================] - 60s 34ms/step\n",
      "Loading data... /home/vanessa/PycharmProjects/RecurrentNetworks/tokenizers/anx_dep_multilabel/SMHD_TR_1760_VS_5000_TF_0_DF_0_RSW_F_IT_1_RP_F\n",
      "Preprocess data... /home/vanessa/PycharmProjects/RecurrentNetworks/dataset/anx_dep_multilabel/SMHD_train_1760.df\n",
      "Load Pre-train embeddings\n",
      "Found 1566378 word vectors.\n",
      "Load data - Ini: 2020-03-09 21:28:31\tEnd: 2020-03-09 21:32:14\tTotal: 0:03:42.965010\n",
      "Training using single GPU or CPU..\n",
      "Train on 1760 samples, validate on 1760 samples\n",
      "Epoch 1/32\n",
      "1760/1760 [==============================] - 480s 273ms/step - loss: 0.6678 - acc: 0.5830 - val_loss: 0.6507 - val_acc: 0.5835\n",
      "Epoch 2/32\n",
      "1760/1760 [==============================] - 416s 236ms/step - loss: 0.6505 - acc: 0.5826 - val_loss: 0.6491 - val_acc: 0.5818\n",
      "Epoch 3/32\n",
      "1760/1760 [==============================] - 414s 235ms/step - loss: 0.6476 - acc: 0.5850 - val_loss: 0.6486 - val_acc: 0.5907\n",
      "Epoch 4/32\n",
      "1760/1760 [==============================] - 415s 236ms/step - loss: 0.6443 - acc: 0.5977 - val_loss: 0.6527 - val_acc: 0.6042\n",
      "Epoch 5/32\n",
      "1760/1760 [==============================] - 414s 235ms/step - loss: 0.6429 - acc: 0.6097 - val_loss: 0.6495 - val_acc: 0.6023\n",
      "Epoch 6/32\n",
      "1760/1760 [==============================] - 415s 236ms/step - loss: 0.6386 - acc: 0.6133 - val_loss: 0.6506 - val_acc: 0.6013\n",
      "Epoch 7/32\n",
      "1760/1760 [==============================] - 415s 236ms/step - loss: 0.6400 - acc: 0.6112 - val_loss: 0.6490 - val_acc: 0.6091\n",
      "Epoch 8/32\n",
      "1760/1760 [==============================] - 415s 236ms/step - loss: 0.6373 - acc: 0.6138 - val_loss: 0.6491 - val_acc: 0.6063\n",
      "Epoch 9/32\n",
      "1760/1760 [==============================] - 424s 241ms/step - loss: 0.6347 - acc: 0.6265 - val_loss: 0.6487 - val_acc: 0.6080\n",
      "Epoch 10/32\n",
      "1760/1760 [==============================] - 418s 237ms/step - loss: 0.6289 - acc: 0.6326 - val_loss: 0.6489 - val_acc: 0.6127\n",
      "Epoch 11/32\n",
      "1760/1760 [==============================] - 422s 240ms/step - loss: 0.6338 - acc: 0.6256 - val_loss: 0.6450 - val_acc: 0.6129\n",
      "Epoch 12/32\n",
      "1760/1760 [==============================] - 415s 236ms/step - loss: 0.6316 - acc: 0.6294 - val_loss: 0.6450 - val_acc: 0.6112\n",
      "Epoch 13/32\n",
      "1760/1760 [==============================] - 417s 237ms/step - loss: 0.6278 - acc: 0.6328 - val_loss: 0.6465 - val_acc: 0.6123\n",
      "Epoch 14/32\n",
      "1760/1760 [==============================] - 418s 237ms/step - loss: 0.6284 - acc: 0.6362 - val_loss: 0.6468 - val_acc: 0.6114\n",
      "Epoch 15/32\n",
      "1760/1760 [==============================] - 420s 239ms/step - loss: 0.6225 - acc: 0.6402 - val_loss: 0.6526 - val_acc: 0.6114\n",
      "Epoch 16/32\n",
      "1760/1760 [==============================] - 408s 232ms/step - loss: 0.6341 - acc: 0.6278 - val_loss: 0.6456 - val_acc: 0.6085\n",
      "Epoch 17/32\n",
      "1760/1760 [==============================] - 403s 229ms/step - loss: 0.6267 - acc: 0.6333 - val_loss: 0.6477 - val_acc: 0.6150\n",
      "Epoch 18/32\n",
      "1760/1760 [==============================] - 402s 228ms/step - loss: 0.6270 - acc: 0.6394 - val_loss: 0.6471 - val_acc: 0.6123\n",
      "Epoch 19/32\n",
      "1760/1760 [==============================] - 402s 229ms/step - loss: 0.6215 - acc: 0.6420 - val_loss: 0.6485 - val_acc: 0.6123\n",
      "Epoch 20/32\n",
      "1760/1760 [==============================] - 401s 228ms/step - loss: 0.6234 - acc: 0.6345 - val_loss: 0.6485 - val_acc: 0.6106\n",
      "Epoch 21/32\n",
      "1760/1760 [==============================] - 394s 224ms/step - loss: 0.6244 - acc: 0.6379 - val_loss: 0.6472 - val_acc: 0.6106\n",
      "Epoch 22/32\n",
      "1760/1760 [==============================] - 394s 224ms/step - loss: 0.6206 - acc: 0.6396 - val_loss: 0.6514 - val_acc: 0.6100\n",
      "Epoch 23/32\n",
      "1760/1760 [==============================] - 391s 222ms/step - loss: 0.6199 - acc: 0.6422 - val_loss: 0.6536 - val_acc: 0.6087\n",
      "Epoch 24/32\n",
      "1760/1760 [==============================] - 391s 222ms/step - loss: 0.6185 - acc: 0.6441 - val_loss: 0.6462 - val_acc: 0.6133\n",
      "Epoch 25/32\n",
      "1760/1760 [==============================] - 387s 220ms/step - loss: 0.6154 - acc: 0.6475 - val_loss: 0.6495 - val_acc: 0.6116\n",
      "Epoch 26/32\n",
      "1760/1760 [==============================] - 387s 220ms/step - loss: 0.6176 - acc: 0.6441 - val_loss: 0.6487 - val_acc: 0.6117\n",
      "Epoch 27/32\n",
      "1760/1760 [==============================] - 389s 221ms/step - loss: 0.6161 - acc: 0.6500 - val_loss: 0.6500 - val_acc: 0.6076\n",
      "Epoch 28/32\n",
      "1760/1760 [==============================] - 385s 219ms/step - loss: 0.6147 - acc: 0.6515 - val_loss: 0.6516 - val_acc: 0.6045\n",
      "Epoch 00028: early stopping\n",
      "Generate Model - Ini: 2020-03-09 21:34:47\tEnd: 2020-03-10 00:46:25\tTotal: 3:11:38.125278\n",
      "Total experiment - Ini: 2020-03-09 21:33:06\tEnd: 2020-03-10 00:46:25\tTotal: 3:13:19.722717\n",
      "Loading data... /home/vanessa/PycharmProjects/RecurrentNetworks/tokenizers/anx_dep_multilabel/SMHD_TR_1760_VS_5000_TF_0_DF_0_RSW_F_IT_1_RP_F\n",
      "Preprocess data... /home/vanessa/PycharmProjects/RecurrentNetworks/dataset/anx_dep_multilabel/SMHD_test_1760.df\n",
      "Load data - Ini: 2020-03-10 00:46:26\tEnd: 2020-03-10 00:46:41\tTotal: 0:00:15.051626\n",
      "1760/1760 [==============================] - 57s 32ms/step\n",
      "Loading data... /home/vanessa/PycharmProjects/RecurrentNetworks/tokenizers/anx_dep_multilabel/SMHD_TR_1760_VS_5000_TF_0_DF_0_RSW_F_IT_1_RP_F\n",
      "Preprocess data... /home/vanessa/PycharmProjects/RecurrentNetworks/dataset/anx_dep_multilabel/SMHD_train_1760.df\n",
      "Load data - Ini: 2020-03-10 00:48:35\tEnd: 2020-03-10 00:48:58\tTotal: 0:00:23.513175\n",
      "Training using single GPU or CPU..\n",
      "Train on 1760 samples, validate on 1760 samples\n",
      "Epoch 1/32\n",
      "1760/1760 [==============================] - 450s 256ms/step - loss: 0.6712 - acc: 0.5816 - val_loss: 0.6496 - val_acc: 0.5833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/32\n",
      "1760/1760 [==============================] - 424s 241ms/step - loss: 0.6526 - acc: 0.5811 - val_loss: 0.6486 - val_acc: 0.5975\n",
      "Epoch 3/32\n",
      "1760/1760 [==============================] - 408s 232ms/step - loss: 0.6488 - acc: 0.5818 - val_loss: 0.6481 - val_acc: 0.5807\n",
      "Epoch 4/32\n",
      "1760/1760 [==============================] - 405s 230ms/step - loss: 0.6469 - acc: 0.5949 - val_loss: 0.6463 - val_acc: 0.6129\n",
      "Epoch 5/32\n",
      "1760/1760 [==============================] - 399s 227ms/step - loss: 0.6408 - acc: 0.6034 - val_loss: 0.6448 - val_acc: 0.6117\n",
      "Epoch 6/32\n",
      "1760/1760 [==============================] - 402s 229ms/step - loss: 0.6335 - acc: 0.6284 - val_loss: 0.6522 - val_acc: 0.5938\n",
      "Epoch 7/32\n",
      "1760/1760 [==============================] - 402s 228ms/step - loss: 0.6307 - acc: 0.6263 - val_loss: 0.6433 - val_acc: 0.6150\n",
      "Epoch 8/32\n",
      "1760/1760 [==============================] - 395s 225ms/step - loss: 0.6198 - acc: 0.6384 - val_loss: 0.6419 - val_acc: 0.6172\n",
      "Epoch 9/32\n",
      "1760/1760 [==============================] - 394s 224ms/step - loss: 0.6176 - acc: 0.6473 - val_loss: 0.6479 - val_acc: 0.6161\n",
      "Epoch 10/32\n",
      "1760/1760 [==============================] - 393s 223ms/step - loss: 0.6141 - acc: 0.6487 - val_loss: 0.6421 - val_acc: 0.6205\n",
      "Epoch 11/32\n",
      "1760/1760 [==============================] - 393s 223ms/step - loss: 0.6115 - acc: 0.6489 - val_loss: 0.6428 - val_acc: 0.6138\n",
      "Epoch 12/32\n",
      "1760/1760 [==============================] - 394s 224ms/step - loss: 0.6121 - acc: 0.6496 - val_loss: 0.6465 - val_acc: 0.6114\n",
      "Epoch 13/32\n",
      "1760/1760 [==============================] - 394s 224ms/step - loss: 0.6038 - acc: 0.6633 - val_loss: 0.6473 - val_acc: 0.6182\n",
      "Epoch 14/32\n",
      "1760/1760 [==============================] - 391s 222ms/step - loss: 0.6091 - acc: 0.6527 - val_loss: 0.6490 - val_acc: 0.6072\n",
      "Epoch 15/32\n",
      "1760/1760 [==============================] - 402s 228ms/step - loss: 0.6043 - acc: 0.6585 - val_loss: 0.6475 - val_acc: 0.6186\n",
      "Epoch 16/32\n",
      "1760/1760 [==============================] - 407s 232ms/step - loss: 0.6044 - acc: 0.6593 - val_loss: 0.6488 - val_acc: 0.6170\n",
      "Epoch 17/32\n",
      "1760/1760 [==============================] - 416s 236ms/step - loss: 0.6031 - acc: 0.6617 - val_loss: 0.6546 - val_acc: 0.5992\n",
      "Epoch 18/32\n",
      "1760/1760 [==============================] - 413s 234ms/step - loss: 0.5948 - acc: 0.6722 - val_loss: 0.6522 - val_acc: 0.6134\n",
      "Epoch 19/32\n",
      "1760/1760 [==============================] - 414s 235ms/step - loss: 0.6007 - acc: 0.6665 - val_loss: 0.6509 - val_acc: 0.6087\n",
      "Epoch 20/32\n",
      "1760/1760 [==============================] - 413s 235ms/step - loss: 0.6006 - acc: 0.6648 - val_loss: 0.6526 - val_acc: 0.6136\n",
      "Epoch 21/32\n",
      "1760/1760 [==============================] - 414s 235ms/step - loss: 0.5948 - acc: 0.6699 - val_loss: 0.6522 - val_acc: 0.6148\n",
      "Epoch 22/32\n",
      "1760/1760 [==============================] - 414s 235ms/step - loss: 0.5951 - acc: 0.6693 - val_loss: 0.6526 - val_acc: 0.6134\n",
      "Epoch 23/32\n",
      "1760/1760 [==============================] - 416s 236ms/step - loss: 0.5907 - acc: 0.6767 - val_loss: 0.6557 - val_acc: 0.6093\n",
      "Epoch 24/32\n",
      "1760/1760 [==============================] - 413s 235ms/step - loss: 0.5958 - acc: 0.6729 - val_loss: 0.6529 - val_acc: 0.6110\n",
      "Epoch 00024: early stopping\n",
      "Generate Model - Ini: 2020-03-10 00:48:59\tEnd: 2020-03-10 03:32:24\tTotal: 2:43:25.678964\n",
      "Total experiment - Ini: 2020-03-10 00:48:59\tEnd: 2020-03-10 03:32:24\tTotal: 2:43:25.679532\n",
      "Loading data... /home/vanessa/PycharmProjects/RecurrentNetworks/tokenizers/anx_dep_multilabel/SMHD_TR_1760_VS_5000_TF_0_DF_0_RSW_F_IT_1_RP_F\n",
      "Preprocess data... /home/vanessa/PycharmProjects/RecurrentNetworks/dataset/anx_dep_multilabel/SMHD_test_1760.df\n",
      "Load data - Ini: 2020-03-10 03:32:24\tEnd: 2020-03-10 03:32:33\tTotal: 0:00:08.824508\n",
      "1760/1760 [==============================] - 53s 30ms/step\n"
     ]
    }
   ],
   "source": [
    "print('Initializer experiment 1.1 (model SMHD_ml_gl_1760)\\n'+\\\n",
    "      'Set: kernel_initializer=glorot_uniform=xavier_uniform, dataset=SMHD_1760 multi-label')\n",
    "\n",
    "exp = ExperimentProcesses('lstm_exp9_var_L3')\n",
    "exp.pp_data.set_dataset_source(dataset_name='SMHD', label_set=['control', 'anxiety', 'depression'],\n",
    "                               total_registers=1760, subdirectory=\"anx_dep_multilabel\")\n",
    "\n",
    "embedding_types = [dn.EmbeddingType.WORD2VEC_CUSTOM, dn.EmbeddingType.GLOVE_CUSTOM]\n",
    "use_embeddings = [dn.UseEmbedding.STATIC, dn.UseEmbedding.NON_STATIC]\n",
    "\n",
    "for embedding_type in embedding_types:\n",
    "    if embedding_type == dn.EmbeddingType.WORD2VEC_CUSTOM:\n",
    "        word_embedding_custom_files = ['SMHD-Skipgram-AllUsers-300.bin', 'SMHD-CBOW-AllUsers-300.bin']\n",
    "    else:\n",
    "        word_embedding_custom_files = ['SMHD-glove-AllUsers-300.pkl']\n",
    "\n",
    "    for word_embedding_custom_file in word_embedding_custom_files:\n",
    "        for use_embedding in use_embeddings:\n",
    "            exp.pp_data.embedding_type = embedding_type\n",
    "            exp.pp_data.use_embedding = use_embedding\n",
    "            exp.pp_data.word_embedding_custom_file = word_embedding_custom_file\n",
    "            exp.pp_data.load_dataset_type = dn.LoadDataset.TRAIN_DATA_MODEL\n",
    "\n",
    "            we_file_name = 'ET_' + str(embedding_type.value) + '_UE_' + str(use_embedding.value) +\\\n",
    "                           '_EF_' + word_embedding_custom_file.split('.')[0] + '_glove6B300d_glorot'\n",
    "\n",
    "            generate_model(exp, 'exp9_' + we_file_name[0:13] + we_file_name[18:30] + '_1760', we_file_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Test lstm_exp9_var_L3_N16_B40_E32_D0.2 + word embeddings SMHD all users (SMHD_*_2640)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializer experiment 2.2 (model SMHD_ml_gl_2640)\n",
      "Set: kernel_initializer=glorot_uniform=xavier_uniform, dataset=SMHD_2640 multi-label\n",
      "Loading data... /home/vanessa/PycharmProjects/RecurrentNetworks/tokenizers/anx_dep_multilabel/SMHD_TR_2640_VS_5000_TF_0_DF_0_RSW_F_IT_1_RP_F\n",
      "Preprocess data... /home/vanessa/PycharmProjects/RecurrentNetworks/dataset/anx_dep_multilabel/SMHD_train_2640.df\n",
      "Load Pre-train embeddings\n",
      "Load data - Ini: 2020-03-10 14:48:34\tEnd: 2020-03-10 14:49:01\tTotal: 0:00:26.907298\n",
      "WARNING:tensorflow:From /home/vanessa/anaconda3/envs/tensorflow_cpu/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/vanessa/anaconda3/envs/tensorflow_cpu/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/vanessa/anaconda3/envs/tensorflow_cpu/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/vanessa/anaconda3/envs/tensorflow_cpu/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/vanessa/anaconda3/envs/tensorflow_cpu/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /home/vanessa/anaconda3/envs/tensorflow_cpu/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "Training using single GPU or CPU..\n",
      "WARNING:tensorflow:From /home/vanessa/anaconda3/envs/tensorflow_cpu/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/vanessa/anaconda3/envs/tensorflow_cpu/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Train on 2200 samples, validate on 2200 samples\n",
      "Epoch 1/32\n",
      "2200/2200 [==============================] - 537s 244ms/step - loss: 0.6754 - acc: 0.5942 - val_loss: 0.6733 - val_acc: 0.6000\n",
      "Epoch 2/32\n",
      "2200/2200 [==============================] - 542s 246ms/step - loss: 0.6726 - acc: 0.6002 - val_loss: 0.6672 - val_acc: 0.6000\n",
      "Epoch 3/32\n",
      "2200/2200 [==============================] - 518s 235ms/step - loss: 0.6617 - acc: 0.6015 - val_loss: 0.6419 - val_acc: 0.6192\n",
      "Epoch 4/32\n",
      "2200/2200 [==============================] - 502s 228ms/step - loss: 0.6487 - acc: 0.6198 - val_loss: 0.6341 - val_acc: 0.6392\n",
      "Epoch 5/32\n",
      "2200/2200 [==============================] - 502s 228ms/step - loss: 0.6410 - acc: 0.6300 - val_loss: 0.6293 - val_acc: 0.6485\n",
      "Epoch 6/32\n",
      "2200/2200 [==============================] - 503s 228ms/step - loss: 0.6366 - acc: 0.6292 - val_loss: 0.6297 - val_acc: 0.6423\n",
      "Epoch 7/32\n",
      "2200/2200 [==============================] - 503s 229ms/step - loss: 0.6299 - acc: 0.6423 - val_loss: 0.5990 - val_acc: 0.6809\n",
      "Epoch 8/32\n",
      "2200/2200 [==============================] - 502s 228ms/step - loss: 0.6433 - acc: 0.6283 - val_loss: 0.6357 - val_acc: 0.6320\n",
      "Epoch 9/32\n",
      "2200/2200 [==============================] - 503s 229ms/step - loss: 0.6284 - acc: 0.6458 - val_loss: 0.6674 - val_acc: 0.6036\n",
      "Epoch 10/32\n",
      "2200/2200 [==============================] - 504s 229ms/step - loss: 0.6291 - acc: 0.6467 - val_loss: 0.6047 - val_acc: 0.6761\n",
      "Epoch 11/32\n",
      "2200/2200 [==============================] - 505s 229ms/step - loss: 0.6201 - acc: 0.6532 - val_loss: 0.6078 - val_acc: 0.6665\n",
      "Epoch 12/32\n",
      "2200/2200 [==============================] - 504s 229ms/step - loss: 0.6294 - acc: 0.6420 - val_loss: 0.6014 - val_acc: 0.6762\n",
      "Epoch 13/32\n",
      "2200/2200 [==============================] - 503s 229ms/step - loss: 0.6437 - acc: 0.6239 - val_loss: 0.6639 - val_acc: 0.6032\n",
      "Epoch 14/32\n",
      "2200/2200 [==============================] - 503s 229ms/step - loss: 0.6588 - acc: 0.6106 - val_loss: 0.6502 - val_acc: 0.6112\n",
      "Epoch 15/32\n",
      "2200/2200 [==============================] - 501s 228ms/step - loss: 0.6410 - acc: 0.6221 - val_loss: 0.6288 - val_acc: 0.6438\n",
      "Epoch 16/32\n",
      "2200/2200 [==============================] - 501s 228ms/step - loss: 0.6199 - acc: 0.6571 - val_loss: 0.6097 - val_acc: 0.6612\n",
      "Epoch 17/32\n",
      "2200/2200 [==============================] - 499s 227ms/step - loss: 0.6107 - acc: 0.6655 - val_loss: 0.6153 - val_acc: 0.6605\n",
      "Epoch 18/32\n",
      "2200/2200 [==============================] - 507s 230ms/step - loss: 0.6007 - acc: 0.6717 - val_loss: 0.6037 - val_acc: 0.6680\n",
      "Epoch 19/32\n",
      "2200/2200 [==============================] - 503s 228ms/step - loss: 0.6006 - acc: 0.6748 - val_loss: 0.5979 - val_acc: 0.6782\n",
      "Epoch 20/32\n",
      "2200/2200 [==============================] - 501s 228ms/step - loss: 0.5973 - acc: 0.6826 - val_loss: 0.5965 - val_acc: 0.6805\n",
      "Epoch 21/32\n",
      "2200/2200 [==============================] - 501s 228ms/step - loss: 0.5900 - acc: 0.6873 - val_loss: 0.5949 - val_acc: 0.6836\n",
      "Epoch 22/32\n",
      "2200/2200 [==============================] - 499s 227ms/step - loss: 0.5880 - acc: 0.6908 - val_loss: 0.5874 - val_acc: 0.6891\n",
      "Epoch 23/32\n",
      "2200/2200 [==============================] - 501s 228ms/step - loss: 0.5847 - acc: 0.6894 - val_loss: 0.5993 - val_acc: 0.6789\n",
      "Epoch 24/32\n",
      "2200/2200 [==============================] - 502s 228ms/step - loss: 0.5849 - acc: 0.6888 - val_loss: 0.5842 - val_acc: 0.6933\n",
      "Epoch 25/32\n",
      "2200/2200 [==============================] - 501s 228ms/step - loss: 0.5720 - acc: 0.7033 - val_loss: 0.5968 - val_acc: 0.6823\n",
      "Epoch 26/32\n",
      "2200/2200 [==============================] - 502s 228ms/step - loss: 0.5694 - acc: 0.7085 - val_loss: 0.5883 - val_acc: 0.6908\n",
      "Epoch 27/32\n",
      "2200/2200 [==============================] - 504s 229ms/step - loss: 0.5724 - acc: 0.7035 - val_loss: 0.5930 - val_acc: 0.6797\n",
      "Epoch 28/32\n",
      "2200/2200 [==============================] - 501s 228ms/step - loss: 0.5631 - acc: 0.7124 - val_loss: 0.5742 - val_acc: 0.7058\n",
      "Epoch 29/32\n",
      "2200/2200 [==============================] - 504s 229ms/step - loss: 0.5993 - acc: 0.6730 - val_loss: 0.6540 - val_acc: 0.6094\n",
      "Epoch 30/32\n",
      "2200/2200 [==============================] - 502s 228ms/step - loss: 0.6328 - acc: 0.6362 - val_loss: 0.6405 - val_acc: 0.6333\n",
      "Epoch 31/32\n",
      "2200/2200 [==============================] - 501s 228ms/step - loss: 0.6235 - acc: 0.6467 - val_loss: 0.6226 - val_acc: 0.6521\n",
      "Epoch 32/32\n",
      "2200/2200 [==============================] - 501s 228ms/step - loss: 0.5868 - acc: 0.6880 - val_loss: 0.5816 - val_acc: 0.6982\n",
      "Generate Model - Ini: 2020-03-10 14:49:02\tEnd: 2020-03-10 19:18:27\tTotal: 4:29:24.842348\n",
      "Total experiment - Ini: 2020-03-10 14:49:02\tEnd: 2020-03-10 19:18:27\tTotal: 4:29:25.005526\n",
      "Loading data... /home/vanessa/PycharmProjects/RecurrentNetworks/tokenizers/anx_dep_multilabel/SMHD_TR_2640_VS_5000_TF_0_DF_0_RSW_F_IT_1_RP_F\n",
      "Preprocess data... /home/vanessa/PycharmProjects/RecurrentNetworks/dataset/anx_dep_multilabel/SMHD_test_2640.df\n",
      "Load data - Ini: 2020-03-10 19:18:27\tEnd: 2020-03-10 19:18:38\tTotal: 0:00:11.051905\n",
      "2200/2200 [==============================] - 65s 29ms/step\n",
      "Loading data... /home/vanessa/PycharmProjects/RecurrentNetworks/tokenizers/anx_dep_multilabel/SMHD_TR_2640_VS_5000_TF_0_DF_0_RSW_F_IT_1_RP_F\n",
      "Preprocess data... /home/vanessa/PycharmProjects/RecurrentNetworks/dataset/anx_dep_multilabel/SMHD_train_2640.df\n",
      "Load data - Ini: 2020-03-10 19:19:47\tEnd: 2020-03-10 19:20:09\tTotal: 0:00:22.181940\n",
      "Training using single GPU or CPU..\n",
      "Train on 2200 samples, validate on 2200 samples\n",
      "Epoch 1/32\n",
      "2200/2200 [==============================] - 561s 255ms/step - loss: 0.6802 - acc: 0.5941 - val_loss: 0.6730 - val_acc: 0.6000\n",
      "Epoch 2/32\n",
      "2200/2200 [==============================] - 542s 246ms/step - loss: 0.6744 - acc: 0.6000 - val_loss: 0.6729 - val_acc: 0.6000\n",
      "Epoch 3/32\n",
      "2200/2200 [==============================] - 527s 239ms/step - loss: 0.6742 - acc: 0.6000 - val_loss: 0.6726 - val_acc: 0.6000\n",
      "Epoch 4/32\n",
      "2200/2200 [==============================] - 530s 241ms/step - loss: 0.6700 - acc: 0.6006 - val_loss: 0.6654 - val_acc: 0.6000\n",
      "Epoch 5/32\n",
      "2200/2200 [==============================] - 531s 242ms/step - loss: 0.6587 - acc: 0.6080 - val_loss: 0.6558 - val_acc: 0.6100\n",
      "Epoch 6/32\n",
      "2200/2200 [==============================] - 529s 240ms/step - loss: 0.6482 - acc: 0.6200 - val_loss: 0.6500 - val_acc: 0.6200\n",
      "Epoch 7/32\n",
      "2200/2200 [==============================] - 522s 237ms/step - loss: 0.6423 - acc: 0.6291 - val_loss: 0.6346 - val_acc: 0.6377\n",
      "Epoch 8/32\n",
      "2200/2200 [==============================] - 513s 233ms/step - loss: 0.6602 - acc: 0.5991 - val_loss: 0.6530 - val_acc: 0.6202\n",
      "Epoch 9/32\n",
      "2200/2200 [==============================] - 509s 232ms/step - loss: 0.6519 - acc: 0.6202 - val_loss: 0.6443 - val_acc: 0.6212\n",
      "Epoch 10/32\n",
      "2200/2200 [==============================] - 510s 232ms/step - loss: 0.6431 - acc: 0.6186 - val_loss: 0.6516 - val_acc: 0.6161\n",
      "Epoch 11/32\n",
      "2200/2200 [==============================] - 509s 231ms/step - loss: 0.6365 - acc: 0.6345 - val_loss: 0.6507 - val_acc: 0.6144\n",
      "Epoch 12/32\n",
      "2200/2200 [==============================] - 509s 231ms/step - loss: 0.6322 - acc: 0.6335 - val_loss: 0.6485 - val_acc: 0.6217\n",
      "Epoch 13/32\n",
      "2200/2200 [==============================] - 510s 232ms/step - loss: 0.6329 - acc: 0.6355 - val_loss: 0.6431 - val_acc: 0.6280\n",
      "Epoch 14/32\n",
      "2200/2200 [==============================] - 510s 232ms/step - loss: 0.6342 - acc: 0.6327 - val_loss: 0.6455 - val_acc: 0.6229\n",
      "Epoch 15/32\n",
      "2200/2200 [==============================] - 512s 233ms/step - loss: 0.6302 - acc: 0.6424 - val_loss: 0.6401 - val_acc: 0.6355\n",
      "Epoch 16/32\n",
      "2200/2200 [==============================] - 509s 231ms/step - loss: 0.6217 - acc: 0.6532 - val_loss: 0.6393 - val_acc: 0.6352\n",
      "Epoch 17/32\n",
      "2200/2200 [==============================] - 520s 236ms/step - loss: 0.6306 - acc: 0.6403 - val_loss: 0.6396 - val_acc: 0.6320\n",
      "Epoch 18/32\n",
      "2200/2200 [==============================] - 528s 240ms/step - loss: 0.6319 - acc: 0.6371 - val_loss: 0.6406 - val_acc: 0.6277\n",
      "Epoch 19/32\n",
      "2200/2200 [==============================] - 525s 239ms/step - loss: 0.6284 - acc: 0.6397 - val_loss: 0.6534 - val_acc: 0.6227\n",
      "Epoch 20/32\n",
      "2200/2200 [==============================] - 527s 239ms/step - loss: 0.6551 - acc: 0.6189 - val_loss: 0.6473 - val_acc: 0.6221\n",
      "Epoch 21/32\n",
      "2200/2200 [==============================] - 528s 240ms/step - loss: 0.6428 - acc: 0.6261 - val_loss: 0.6495 - val_acc: 0.6170\n",
      "Epoch 22/32\n",
      "2200/2200 [==============================] - 527s 239ms/step - loss: 0.6267 - acc: 0.6520 - val_loss: 0.6476 - val_acc: 0.6194\n",
      "Epoch 23/32\n",
      "2200/2200 [==============================] - 528s 240ms/step - loss: 0.6485 - acc: 0.6235 - val_loss: 0.7205 - val_acc: 0.5333\n",
      "Epoch 00023: early stopping\n",
      "Generate Model - Ini: 2020-03-10 19:20:10\tEnd: 2020-03-10 22:40:28\tTotal: 3:20:18.298992\n",
      "Total experiment - Ini: 2020-03-10 19:20:10\tEnd: 2020-03-10 22:40:28\tTotal: 3:20:18.299567\n",
      "Loading data... /home/vanessa/PycharmProjects/RecurrentNetworks/tokenizers/anx_dep_multilabel/SMHD_TR_2640_VS_5000_TF_0_DF_0_RSW_F_IT_1_RP_F\n",
      "Preprocess data... /home/vanessa/PycharmProjects/RecurrentNetworks/dataset/anx_dep_multilabel/SMHD_test_2640.df\n",
      "Load data - Ini: 2020-03-10 22:40:28\tEnd: 2020-03-10 22:40:40\tTotal: 0:00:11.420597\n",
      "2200/2200 [==============================] - 64s 29ms/step\n",
      "Loading data... /home/vanessa/PycharmProjects/RecurrentNetworks/tokenizers/anx_dep_multilabel/SMHD_TR_2640_VS_5000_TF_0_DF_0_RSW_F_IT_1_RP_F\n",
      "Preprocess data... /home/vanessa/PycharmProjects/RecurrentNetworks/dataset/anx_dep_multilabel/SMHD_train_2640.df\n",
      "Load Pre-train embeddings\n",
      "Load data - Ini: 2020-03-10 22:41:51\tEnd: 2020-03-10 22:42:16\tTotal: 0:00:25.320574\n",
      "Training using single GPU or CPU..\n",
      "Train on 2200 samples, validate on 2200 samples\n",
      "Epoch 1/32\n",
      "2200/2200 [==============================] - 546s 248ms/step - loss: 0.6807 - acc: 0.5926 - val_loss: 0.6734 - val_acc: 0.6000\n",
      "Epoch 2/32\n",
      "2200/2200 [==============================] - 527s 239ms/step - loss: 0.6736 - acc: 0.6000 - val_loss: 0.6728 - val_acc: 0.6000\n",
      "Epoch 3/32\n",
      "2200/2200 [==============================] - 520s 237ms/step - loss: 0.6733 - acc: 0.6000 - val_loss: 0.6724 - val_acc: 0.6000\n",
      "Epoch 4/32\n",
      "2200/2200 [==============================] - 513s 233ms/step - loss: 0.6707 - acc: 0.6002 - val_loss: 0.6660 - val_acc: 0.6002\n",
      "Epoch 5/32\n",
      "2200/2200 [==============================] - 499s 227ms/step - loss: 0.6550 - acc: 0.6083 - val_loss: 0.6563 - val_acc: 0.6114\n",
      "Epoch 6/32\n",
      "2200/2200 [==============================] - 497s 226ms/step - loss: 0.6475 - acc: 0.6115 - val_loss: 0.6560 - val_acc: 0.6074\n",
      "Epoch 7/32\n",
      "2200/2200 [==============================] - 495s 225ms/step - loss: 0.6432 - acc: 0.6217 - val_loss: 0.6755 - val_acc: 0.6030\n",
      "Epoch 8/32\n",
      "2200/2200 [==============================] - 498s 226ms/step - loss: 0.6446 - acc: 0.6176 - val_loss: 0.6557 - val_acc: 0.6105\n",
      "Epoch 9/32\n",
      "2200/2200 [==============================] - 497s 226ms/step - loss: 0.6422 - acc: 0.6183 - val_loss: 0.6540 - val_acc: 0.6105\n",
      "Epoch 10/32\n",
      "2200/2200 [==============================] - 499s 227ms/step - loss: 0.6726 - acc: 0.5911 - val_loss: 0.6609 - val_acc: 0.6023\n",
      "Epoch 11/32\n",
      "2200/2200 [==============================] - 503s 228ms/step - loss: 0.6376 - acc: 0.6294 - val_loss: 0.6593 - val_acc: 0.5973\n",
      "Epoch 12/32\n",
      "2200/2200 [==============================] - 502s 228ms/step - loss: 0.6331 - acc: 0.6373 - val_loss: 0.6543 - val_acc: 0.6052\n",
      "Epoch 13/32\n",
      "2200/2200 [==============================] - 504s 229ms/step - loss: 0.6369 - acc: 0.6353 - val_loss: 0.6520 - val_acc: 0.6098\n",
      "Epoch 14/32\n",
      "2200/2200 [==============================] - 502s 228ms/step - loss: 0.6347 - acc: 0.6392 - val_loss: 0.6497 - val_acc: 0.6156\n",
      "Epoch 15/32\n",
      "2200/2200 [==============================] - 503s 229ms/step - loss: 0.6308 - acc: 0.6397 - val_loss: 0.6498 - val_acc: 0.6106\n",
      "Epoch 16/32\n",
      "2200/2200 [==============================] - 503s 229ms/step - loss: 0.6354 - acc: 0.6342 - val_loss: 0.6688 - val_acc: 0.6023\n",
      "Epoch 17/32\n",
      "2200/2200 [==============================] - 534s 243ms/step - loss: 0.6483 - acc: 0.6186 - val_loss: 0.6520 - val_acc: 0.6086\n",
      "Epoch 18/32\n",
      "2200/2200 [==============================] - 503s 229ms/step - loss: 0.6289 - acc: 0.6405 - val_loss: 0.6531 - val_acc: 0.6058\n",
      "Epoch 19/32\n",
      "2200/2200 [==============================] - 500s 227ms/step - loss: 0.6252 - acc: 0.6491 - val_loss: 0.6536 - val_acc: 0.6092\n",
      "Epoch 20/32\n",
      "2200/2200 [==============================] - 501s 228ms/step - loss: 0.6365 - acc: 0.6347 - val_loss: 0.6485 - val_acc: 0.6120\n",
      "Epoch 21/32\n",
      "2200/2200 [==============================] - 502s 228ms/step - loss: 0.6310 - acc: 0.6426 - val_loss: 0.6490 - val_acc: 0.6147\n",
      "Epoch 22/32\n",
      "2200/2200 [==============================] - 503s 229ms/step - loss: 0.6631 - acc: 0.5979 - val_loss: 0.6558 - val_acc: 0.6044\n",
      "Epoch 23/32\n",
      "2200/2200 [==============================] - 502s 228ms/step - loss: 0.6392 - acc: 0.6300 - val_loss: 0.6565 - val_acc: 0.6058\n",
      "Epoch 24/32\n",
      "2200/2200 [==============================] - 503s 228ms/step - loss: 0.6391 - acc: 0.6295 - val_loss: 0.6509 - val_acc: 0.6136\n",
      "Epoch 25/32\n",
      "2200/2200 [==============================] - 504s 229ms/step - loss: 0.6289 - acc: 0.6480 - val_loss: 0.6528 - val_acc: 0.6092\n",
      "Epoch 26/32\n",
      "2200/2200 [==============================] - 504s 229ms/step - loss: 0.6382 - acc: 0.6286 - val_loss: 0.6449 - val_acc: 0.6189\n",
      "Epoch 27/32\n",
      "2200/2200 [==============================] - 504s 229ms/step - loss: 0.6353 - acc: 0.6398 - val_loss: 0.6509 - val_acc: 0.6148\n",
      "Epoch 28/32\n",
      "2200/2200 [==============================] - 505s 229ms/step - loss: 0.6294 - acc: 0.6423 - val_loss: 0.6522 - val_acc: 0.6112\n",
      "Epoch 29/32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2200/2200 [==============================] - 503s 229ms/step - loss: 0.6227 - acc: 0.6486 - val_loss: 0.6516 - val_acc: 0.6148\n",
      "Epoch 30/32\n",
      "2200/2200 [==============================] - 503s 229ms/step - loss: 0.6219 - acc: 0.6479 - val_loss: 0.6570 - val_acc: 0.6091\n",
      "Epoch 31/32\n",
      "2200/2200 [==============================] - 502s 228ms/step - loss: 0.6239 - acc: 0.6442 - val_loss: 0.6514 - val_acc: 0.6185\n",
      "Epoch 32/32\n",
      "2200/2200 [==============================] - 502s 228ms/step - loss: 0.6237 - acc: 0.6515 - val_loss: 0.6523 - val_acc: 0.6118\n",
      "Generate Model - Ini: 2020-03-10 22:42:19\tEnd: 2020-03-11 03:12:06\tTotal: 4:29:47.202715\n",
      "Total experiment - Ini: 2020-03-10 22:42:17\tEnd: 2020-03-11 03:12:06\tTotal: 4:29:49.589882\n",
      "Loading data... /home/vanessa/PycharmProjects/RecurrentNetworks/tokenizers/anx_dep_multilabel/SMHD_TR_2640_VS_5000_TF_0_DF_0_RSW_F_IT_1_RP_F\n",
      "Preprocess data... /home/vanessa/PycharmProjects/RecurrentNetworks/dataset/anx_dep_multilabel/SMHD_test_2640.df\n",
      "Load data - Ini: 2020-03-11 03:12:06\tEnd: 2020-03-11 03:12:18\tTotal: 0:00:11.567169\n",
      "2200/2200 [==============================] - 70s 32ms/step\n",
      "Loading data... /home/vanessa/PycharmProjects/RecurrentNetworks/tokenizers/anx_dep_multilabel/SMHD_TR_2640_VS_5000_TF_0_DF_0_RSW_F_IT_1_RP_F\n",
      "Preprocess data... /home/vanessa/PycharmProjects/RecurrentNetworks/dataset/anx_dep_multilabel/SMHD_train_2640.df\n",
      "Load data - Ini: 2020-03-11 03:13:37\tEnd: 2020-03-11 03:14:00\tTotal: 0:00:23.014744\n",
      "Training using single GPU or CPU..\n",
      "Train on 2200 samples, validate on 2200 samples\n",
      "Epoch 1/32\n",
      "2200/2200 [==============================] - 536s 244ms/step - loss: 0.6802 - acc: 0.5941 - val_loss: 0.6730 - val_acc: 0.6000\n",
      "Epoch 2/32\n",
      "2200/2200 [==============================] - 524s 238ms/step - loss: 0.6744 - acc: 0.6000 - val_loss: 0.6729 - val_acc: 0.6000\n",
      "Epoch 3/32\n",
      "2200/2200 [==============================] - 525s 239ms/step - loss: 0.6742 - acc: 0.6000 - val_loss: 0.6726 - val_acc: 0.6000\n",
      "Epoch 4/32\n",
      "2200/2200 [==============================] - 525s 239ms/step - loss: 0.6700 - acc: 0.6006 - val_loss: 0.6654 - val_acc: 0.6000\n",
      "Epoch 5/32\n",
      "2200/2200 [==============================] - 525s 239ms/step - loss: 0.6587 - acc: 0.6080 - val_loss: 0.6558 - val_acc: 0.6100\n",
      "Epoch 6/32\n",
      "2200/2200 [==============================] - 523s 238ms/step - loss: 0.6482 - acc: 0.6200 - val_loss: 0.6500 - val_acc: 0.6200\n",
      "Epoch 7/32\n",
      "2200/2200 [==============================] - 526s 239ms/step - loss: 0.6423 - acc: 0.6291 - val_loss: 0.6346 - val_acc: 0.6377\n",
      "Epoch 8/32\n",
      "2200/2200 [==============================] - 516s 235ms/step - loss: 0.6602 - acc: 0.5991 - val_loss: 0.6530 - val_acc: 0.6202\n",
      "Epoch 9/32\n",
      "2200/2200 [==============================] - 517s 235ms/step - loss: 0.6519 - acc: 0.6202 - val_loss: 0.6443 - val_acc: 0.6212\n",
      "Epoch 10/32\n",
      "2200/2200 [==============================] - 515s 234ms/step - loss: 0.6410 - acc: 0.6208 - val_loss: 0.6352 - val_acc: 0.6311\n",
      "Epoch 11/32\n",
      "2200/2200 [==============================] - 516s 234ms/step - loss: 0.6452 - acc: 0.6226 - val_loss: 0.6613 - val_acc: 0.6048\n",
      "Epoch 12/32\n",
      "2200/2200 [==============================] - 516s 234ms/step - loss: 0.6346 - acc: 0.6352 - val_loss: 0.6470 - val_acc: 0.6226\n",
      "Epoch 13/32\n",
      "2200/2200 [==============================] - 517s 235ms/step - loss: 0.6406 - acc: 0.6277 - val_loss: 0.6329 - val_acc: 0.6424\n",
      "Epoch 14/32\n",
      "2200/2200 [==============================] - 515s 234ms/step - loss: 0.6441 - acc: 0.6227 - val_loss: 0.6240 - val_acc: 0.6547\n",
      "Epoch 15/32\n",
      "2200/2200 [==============================] - 514s 234ms/step - loss: 0.6516 - acc: 0.6073 - val_loss: 0.6297 - val_acc: 0.6429\n",
      "Epoch 16/32\n",
      "2200/2200 [==============================] - 515s 234ms/step - loss: 0.6438 - acc: 0.6197 - val_loss: 0.6250 - val_acc: 0.6409\n",
      "Epoch 17/32\n",
      "2200/2200 [==============================] - 513s 233ms/step - loss: 0.6404 - acc: 0.6214 - val_loss: 0.6249 - val_acc: 0.6415\n",
      "Epoch 18/32\n",
      "2200/2200 [==============================] - 515s 234ms/step - loss: 0.6457 - acc: 0.6215 - val_loss: 0.6145 - val_acc: 0.6679\n",
      "Epoch 19/32\n",
      "2200/2200 [==============================] - 502s 228ms/step - loss: 0.6371 - acc: 0.6330 - val_loss: 0.6128 - val_acc: 0.6697\n",
      "Epoch 20/32\n",
      "2200/2200 [==============================] - 504s 229ms/step - loss: 0.6622 - acc: 0.5926 - val_loss: 0.6399 - val_acc: 0.6382\n",
      "Epoch 21/32\n",
      "2200/2200 [==============================] - 503s 229ms/step - loss: 0.6559 - acc: 0.6023 - val_loss: 0.6625 - val_acc: 0.5952\n",
      "Epoch 22/32\n",
      "2200/2200 [==============================] - 503s 229ms/step - loss: 0.6469 - acc: 0.6153 - val_loss: 0.6205 - val_acc: 0.6421\n",
      "Epoch 23/32\n",
      "2200/2200 [==============================] - 504s 229ms/step - loss: 0.6547 - acc: 0.6108 - val_loss: 0.6133 - val_acc: 0.6447\n",
      "Epoch 24/32\n",
      "2200/2200 [==============================] - 501s 228ms/step - loss: 0.6477 - acc: 0.6114 - val_loss: 0.6409 - val_acc: 0.6350\n",
      "Epoch 25/32\n",
      "2200/2200 [==============================] - 502s 228ms/step - loss: 0.6530 - acc: 0.6135 - val_loss: 0.6541 - val_acc: 0.6115\n",
      "Epoch 26/32\n",
      "2200/2200 [==============================] - 504s 229ms/step - loss: 0.6382 - acc: 0.6276 - val_loss: 0.6494 - val_acc: 0.6189\n",
      "Epoch 27/32\n",
      "2200/2200 [==============================] - 504s 229ms/step - loss: 0.6304 - acc: 0.6420 - val_loss: 0.6449 - val_acc: 0.6273\n",
      "Epoch 28/32\n",
      "2200/2200 [==============================] - 504s 229ms/step - loss: 0.6375 - acc: 0.6302 - val_loss: 0.6439 - val_acc: 0.6202\n",
      "Epoch 29/32\n",
      "2200/2200 [==============================] - 502s 228ms/step - loss: 0.6348 - acc: 0.6300 - val_loss: 0.6378 - val_acc: 0.6177\n",
      "Epoch 30/32\n",
      "2200/2200 [==============================] - 503s 229ms/step - loss: 0.6370 - acc: 0.6312 - val_loss: 0.6341 - val_acc: 0.6476\n",
      "Epoch 31/32\n",
      "2200/2200 [==============================] - 504s 229ms/step - loss: 0.6201 - acc: 0.6558 - val_loss: 0.6332 - val_acc: 0.6495\n",
      "Epoch 32/32\n",
      "2200/2200 [==============================] - 502s 228ms/step - loss: 0.6432 - acc: 0.6297 - val_loss: 0.6471 - val_acc: 0.6258\n",
      "Generate Model - Ini: 2020-03-11 03:14:01\tEnd: 2020-03-11 07:47:25\tTotal: 4:33:23.775969\n",
      "Total experiment - Ini: 2020-03-11 03:14:01\tEnd: 2020-03-11 07:47:25\tTotal: 4:33:23.776902\n",
      "Loading data... /home/vanessa/PycharmProjects/RecurrentNetworks/tokenizers/anx_dep_multilabel/SMHD_TR_2640_VS_5000_TF_0_DF_0_RSW_F_IT_1_RP_F\n",
      "Preprocess data... /home/vanessa/PycharmProjects/RecurrentNetworks/dataset/anx_dep_multilabel/SMHD_test_2640.df\n",
      "Load data - Ini: 2020-03-11 07:47:25\tEnd: 2020-03-11 07:47:36\tTotal: 0:00:11.626281\n",
      "2200/2200 [==============================] - 65s 29ms/step\n",
      "Loading data... /home/vanessa/PycharmProjects/RecurrentNetworks/tokenizers/anx_dep_multilabel/SMHD_TR_2640_VS_5000_TF_0_DF_0_RSW_F_IT_1_RP_F\n",
      "Preprocess data... /home/vanessa/PycharmProjects/RecurrentNetworks/dataset/anx_dep_multilabel/SMHD_train_2640.df\n",
      "Load Pre-train embeddings\n",
      "Found 1566378 word vectors.\n",
      "Load data - Ini: 2020-03-11 07:48:52\tEnd: 2020-03-11 07:53:17\tTotal: 0:04:25.368048\n",
      "Training using single GPU or CPU..\n",
      "Train on 2200 samples, validate on 2200 samples\n",
      "Epoch 1/32\n",
      "2200/2200 [==============================] - 568s 258ms/step - loss: 0.6828 - acc: 0.5891 - val_loss: 0.6740 - val_acc: 0.6000\n",
      "Epoch 2/32\n",
      "2200/2200 [==============================] - 540s 245ms/step - loss: 0.6749 - acc: 0.6000 - val_loss: 0.6728 - val_acc: 0.6000\n",
      "Epoch 3/32\n",
      "2200/2200 [==============================] - 539s 245ms/step - loss: 0.6733 - acc: 0.6000 - val_loss: 0.6722 - val_acc: 0.6000\n",
      "Epoch 4/32\n",
      "2200/2200 [==============================] - 514s 234ms/step - loss: 0.6716 - acc: 0.6002 - val_loss: 0.6691 - val_acc: 0.6000\n",
      "Epoch 5/32\n",
      "2200/2200 [==============================] - 489s 222ms/step - loss: 0.6639 - acc: 0.6021 - val_loss: 0.6639 - val_acc: 0.6018\n",
      "Epoch 6/32\n",
      "2200/2200 [==============================] - 482s 219ms/step - loss: 0.6600 - acc: 0.6071 - val_loss: 0.6653 - val_acc: 0.5986\n",
      "Epoch 7/32\n",
      "2200/2200 [==============================] - 481s 219ms/step - loss: 0.6583 - acc: 0.6009 - val_loss: 0.6694 - val_acc: 0.5908\n",
      "Epoch 8/32\n",
      "2200/2200 [==============================] - 486s 221ms/step - loss: 0.6521 - acc: 0.6132 - val_loss: 0.6676 - val_acc: 0.5965\n",
      "Epoch 9/32\n",
      "2200/2200 [==============================] - 528s 240ms/step - loss: 0.6500 - acc: 0.6202 - val_loss: 0.6670 - val_acc: 0.5918\n",
      "Epoch 10/32\n",
      "2200/2200 [==============================] - 494s 225ms/step - loss: 0.6492 - acc: 0.6212 - val_loss: 0.6629 - val_acc: 0.6027\n",
      "Epoch 11/32\n",
      "2200/2200 [==============================] - 492s 224ms/step - loss: 0.6472 - acc: 0.6205 - val_loss: 0.6672 - val_acc: 0.5892\n",
      "Epoch 12/32\n",
      "2200/2200 [==============================] - 492s 224ms/step - loss: 0.6458 - acc: 0.6236 - val_loss: 0.6668 - val_acc: 0.5965\n",
      "Epoch 13/32\n",
      "2200/2200 [==============================] - 495s 225ms/step - loss: 0.6456 - acc: 0.6248 - val_loss: 0.6726 - val_acc: 0.5914\n",
      "Epoch 14/32\n",
      "2200/2200 [==============================] - 498s 226ms/step - loss: 0.6466 - acc: 0.6208 - val_loss: 0.6639 - val_acc: 0.5994\n",
      "Epoch 15/32\n",
      "2200/2200 [==============================] - 493s 224ms/step - loss: 0.6399 - acc: 0.6320 - val_loss: 0.6743 - val_acc: 0.5894\n",
      "Epoch 16/32\n",
      "2200/2200 [==============================] - 492s 224ms/step - loss: 0.6446 - acc: 0.6227 - val_loss: 0.6628 - val_acc: 0.6000\n",
      "Epoch 17/32\n",
      "2200/2200 [==============================] - 492s 224ms/step - loss: 0.6416 - acc: 0.6265 - val_loss: 0.6716 - val_acc: 0.5939\n",
      "Epoch 18/32\n",
      "2200/2200 [==============================] - 493s 224ms/step - loss: 0.6380 - acc: 0.6289 - val_loss: 0.6660 - val_acc: 0.5976\n",
      "Epoch 19/32\n",
      "2200/2200 [==============================] - 493s 224ms/step - loss: 0.6368 - acc: 0.6344 - val_loss: 0.6704 - val_acc: 0.5944\n",
      "Epoch 20/32\n",
      "2200/2200 [==============================] - 495s 225ms/step - loss: 0.6362 - acc: 0.6312 - val_loss: 0.6630 - val_acc: 0.6059\n",
      "Epoch 21/32\n",
      "2200/2200 [==============================] - 494s 224ms/step - loss: 0.6418 - acc: 0.6211 - val_loss: 0.6679 - val_acc: 0.5968\n",
      "Epoch 22/32\n",
      "2200/2200 [==============================] - 494s 225ms/step - loss: 0.6300 - acc: 0.6374 - val_loss: 0.6683 - val_acc: 0.5986\n",
      "Epoch 23/32\n",
      "2200/2200 [==============================] - 495s 225ms/step - loss: 0.6394 - acc: 0.6291 - val_loss: 0.6536 - val_acc: 0.6164\n",
      "Epoch 24/32\n",
      "2200/2200 [==============================] - 497s 226ms/step - loss: 0.6390 - acc: 0.6252 - val_loss: 0.6622 - val_acc: 0.6076\n",
      "Epoch 25/32\n",
      "2200/2200 [==============================] - 497s 226ms/step - loss: 0.6307 - acc: 0.6361 - val_loss: 0.6680 - val_acc: 0.5920\n",
      "Epoch 26/32\n",
      "2200/2200 [==============================] - 496s 225ms/step - loss: 0.6297 - acc: 0.6427 - val_loss: 0.6668 - val_acc: 0.5941\n",
      "Epoch 27/32\n",
      "2200/2200 [==============================] - 496s 225ms/step - loss: 0.6292 - acc: 0.6379 - val_loss: 0.6675 - val_acc: 0.6027\n",
      "Epoch 28/32\n",
      "2200/2200 [==============================] - 505s 230ms/step - loss: 0.6203 - acc: 0.6509 - val_loss: 0.6763 - val_acc: 0.6009\n",
      "Epoch 29/32\n",
      "2200/2200 [==============================] - 538s 245ms/step - loss: 0.6393 - acc: 0.6230 - val_loss: 0.6732 - val_acc: 0.6011\n",
      "Epoch 30/32\n",
      "2200/2200 [==============================] - 554s 252ms/step - loss: 0.6375 - acc: 0.6279 - val_loss: 0.6630 - val_acc: 0.6070\n",
      "Epoch 31/32\n",
      "2200/2200 [==============================] - 581s 264ms/step - loss: 0.6296 - acc: 0.6402 - val_loss: 0.6817 - val_acc: 0.5936\n",
      "Epoch 32/32\n",
      "2200/2200 [==============================] - 569s 259ms/step - loss: 0.6423 - acc: 0.6255 - val_loss: 0.6673 - val_acc: 0.5982\n",
      "Generate Model - Ini: 2020-03-11 07:54:47\tEnd: 2020-03-11 12:26:28\tTotal: 4:31:40.624027\n",
      "Total experiment - Ini: 2020-03-11 07:53:46\tEnd: 2020-03-11 12:26:28\tTotal: 4:32:41.528387\n",
      "Loading data... /home/vanessa/PycharmProjects/RecurrentNetworks/tokenizers/anx_dep_multilabel/SMHD_TR_2640_VS_5000_TF_0_DF_0_RSW_F_IT_1_RP_F\n",
      "Preprocess data... /home/vanessa/PycharmProjects/RecurrentNetworks/dataset/anx_dep_multilabel/SMHD_test_2640.df\n",
      "Load data - Ini: 2020-03-11 12:26:28\tEnd: 2020-03-11 12:26:43\tTotal: 0:00:14.905479\n",
      "2200/2200 [==============================] - 80s 36ms/step\n",
      "Loading data... /home/vanessa/PycharmProjects/RecurrentNetworks/tokenizers/anx_dep_multilabel/SMHD_TR_2640_VS_5000_TF_0_DF_0_RSW_F_IT_1_RP_F\n",
      "Preprocess data... /home/vanessa/PycharmProjects/RecurrentNetworks/dataset/anx_dep_multilabel/SMHD_train_2640.df\n",
      "Load data - Ini: 2020-03-11 12:28:43\tEnd: 2020-03-11 12:29:10\tTotal: 0:00:27.056402\n",
      "Training using single GPU or CPU..\n",
      "Train on 2200 samples, validate on 2200 samples\n",
      "Epoch 1/32\n",
      "2200/2200 [==============================] - 548s 249ms/step - loss: 0.6802 - acc: 0.5941 - val_loss: 0.6730 - val_acc: 0.6000\n",
      "Epoch 2/32\n",
      "2200/2200 [==============================] - 520s 236ms/step - loss: 0.6744 - acc: 0.6000 - val_loss: 0.6729 - val_acc: 0.6000\n",
      "Epoch 3/32\n",
      "2200/2200 [==============================] - 525s 238ms/step - loss: 0.6742 - acc: 0.6000 - val_loss: 0.6726 - val_acc: 0.6000\n",
      "Epoch 4/32\n",
      "2200/2200 [==============================] - 526s 239ms/step - loss: 0.6700 - acc: 0.6006 - val_loss: 0.6654 - val_acc: 0.6000\n",
      "Epoch 5/32\n",
      "2200/2200 [==============================] - 551s 250ms/step - loss: 0.6587 - acc: 0.6080 - val_loss: 0.6558 - val_acc: 0.6100\n",
      "Epoch 6/32\n",
      "2200/2200 [==============================] - 611s 278ms/step - loss: 0.6482 - acc: 0.6200 - val_loss: 0.6500 - val_acc: 0.6200\n",
      "Epoch 7/32\n",
      "2200/2200 [==============================] - 587s 267ms/step - loss: 0.6402 - acc: 0.6294 - val_loss: 0.6445 - val_acc: 0.6189\n",
      "Epoch 8/32\n",
      "2200/2200 [==============================] - 575s 262ms/step - loss: 0.6427 - acc: 0.6244 - val_loss: 0.6487 - val_acc: 0.6176\n",
      "Epoch 9/32\n",
      "2200/2200 [==============================] - 543s 247ms/step - loss: 0.6399 - acc: 0.6286 - val_loss: 0.6493 - val_acc: 0.6182\n",
      "Epoch 10/32\n",
      "2200/2200 [==============================] - 542s 246ms/step - loss: 0.6339 - acc: 0.6367 - val_loss: 0.6361 - val_acc: 0.6371\n",
      "Epoch 11/32\n",
      "2200/2200 [==============================] - 552s 251ms/step - loss: 0.6403 - acc: 0.6335 - val_loss: 0.6529 - val_acc: 0.6167\n",
      "Epoch 12/32\n",
      "2200/2200 [==============================] - 538s 245ms/step - loss: 0.6331 - acc: 0.6353 - val_loss: 0.6216 - val_acc: 0.6456\n",
      "Epoch 13/32\n",
      "2200/2200 [==============================] - 539s 245ms/step - loss: 0.6399 - acc: 0.6265 - val_loss: 0.6552 - val_acc: 0.6192\n",
      "Epoch 14/32\n",
      "2200/2200 [==============================] - 535s 243ms/step - loss: 0.6911 - acc: 0.5844 - val_loss: 0.6733 - val_acc: 0.6000\n",
      "Epoch 15/32\n",
      "2200/2200 [==============================] - 560s 254ms/step - loss: 0.6736 - acc: 0.6017 - val_loss: 0.6731 - val_acc: 0.6000\n",
      "Epoch 16/32\n",
      "2200/2200 [==============================] - 529s 240ms/step - loss: 0.6721 - acc: 0.6002 - val_loss: 0.6734 - val_acc: 0.6000\n",
      "Epoch 17/32\n",
      "2200/2200 [==============================] - 509s 231ms/step - loss: 0.6719 - acc: 0.6017 - val_loss: 0.6730 - val_acc: 0.6000\n",
      "Epoch 18/32\n",
      "2200/2200 [==============================] - 512s 233ms/step - loss: 0.6713 - acc: 0.6002 - val_loss: 0.6730 - val_acc: 0.6000\n",
      "Epoch 19/32\n",
      "2200/2200 [==============================] - 520s 237ms/step - loss: 0.6707 - acc: 0.5998 - val_loss: 0.6733 - val_acc: 0.6000\n",
      "Epoch 20/32\n",
      "2200/2200 [==============================] - 501s 228ms/step - loss: 0.6723 - acc: 0.6000 - val_loss: 0.6732 - val_acc: 0.6000\n",
      "Epoch 21/32\n",
      "2200/2200 [==============================] - 497s 226ms/step - loss: 0.6715 - acc: 0.6017 - val_loss: 0.6733 - val_acc: 0.6000\n",
      "Epoch 22/32\n",
      "2200/2200 [==============================] - 499s 227ms/step - loss: 0.6720 - acc: 0.6005 - val_loss: 0.6731 - val_acc: 0.6000\n",
      "Epoch 23/32\n",
      "2200/2200 [==============================] - 498s 226ms/step - loss: 0.6702 - acc: 0.6009 - val_loss: 0.6730 - val_acc: 0.6000\n",
      "Epoch 24/32\n",
      "2200/2200 [==============================] - 498s 226ms/step - loss: 0.6707 - acc: 0.5997 - val_loss: 0.6734 - val_acc: 0.6000\n",
      "Epoch 25/32\n",
      "2200/2200 [==============================] - 498s 226ms/step - loss: 0.6680 - acc: 0.5998 - val_loss: 0.6729 - val_acc: 0.6002\n",
      "Epoch 26/32\n",
      "2200/2200 [==============================] - 499s 227ms/step - loss: 0.6670 - acc: 0.6008 - val_loss: 0.6646 - val_acc: 0.6062\n",
      "Epoch 27/32\n",
      "2200/2200 [==============================] - 498s 226ms/step - loss: 0.6626 - acc: 0.6014 - val_loss: 0.6287 - val_acc: 0.6132\n",
      "Epoch 28/32\n",
      "2200/2200 [==============================] - 498s 226ms/step - loss: 0.6657 - acc: 0.5879 - val_loss: 0.6718 - val_acc: 0.6000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00028: early stopping\n",
      "Generate Model - Ini: 2020-03-11 12:29:12\tEnd: 2020-03-11 16:36:10\tTotal: 4:06:58.517568\n",
      "Total experiment - Ini: 2020-03-11 12:29:12\tEnd: 2020-03-11 16:36:10\tTotal: 4:06:58.518239\n",
      "Loading data... /home/vanessa/PycharmProjects/RecurrentNetworks/tokenizers/anx_dep_multilabel/SMHD_TR_2640_VS_5000_TF_0_DF_0_RSW_F_IT_1_RP_F\n",
      "Preprocess data... /home/vanessa/PycharmProjects/RecurrentNetworks/dataset/anx_dep_multilabel/SMHD_test_2640.df\n",
      "Load data - Ini: 2020-03-11 16:36:10\tEnd: 2020-03-11 16:36:22\tTotal: 0:00:11.545395\n",
      "2200/2200 [==============================] - 72s 33ms/step\n"
     ]
    }
   ],
   "source": [
    "print('Initializer experiment 2.2 (model SMHD_ml_gl_2640)\\n'+\\\n",
    "      'Set: kernel_initializer=glorot_uniform=xavier_uniform, dataset=SMHD_2640 multi-label')\n",
    "\n",
    "exp = ExperimentProcesses('lstm_exp9_var_L3')\n",
    "exp.pp_data.set_dataset_source(dataset_name='SMHD', label_set=['control', 'anxiety', 'depression'],\n",
    "                               total_registers=2640, subdirectory=\"anx_dep_multilabel\")\n",
    "\n",
    "embedding_types = [dn.EmbeddingType.WORD2VEC_CUSTOM, dn.EmbeddingType.GLOVE_CUSTOM]\n",
    "use_embeddings = [dn.UseEmbedding.STATIC, dn.UseEmbedding.NON_STATIC]\n",
    "\n",
    "for embedding_type in embedding_types:\n",
    "    if embedding_type == dn.EmbeddingType.WORD2VEC_CUSTOM:\n",
    "        word_embedding_custom_files = ['SMHD-Skipgram-AllUsers-300.bin', 'SMHD-CBOW-AllUsers-300.bin']\n",
    "    else:\n",
    "        word_embedding_custom_files = ['SMHD-glove-AllUsers-300.pkl']\n",
    "\n",
    "    for word_embedding_custom_file in word_embedding_custom_files:\n",
    "        for use_embedding in use_embeddings:\n",
    "            exp.pp_data.embedding_type = embedding_type\n",
    "            exp.pp_data.use_embedding = use_embedding\n",
    "            exp.pp_data.word_embedding_custom_file = word_embedding_custom_file\n",
    "            exp.pp_data.load_dataset_type = dn.LoadDataset.TRAIN_DATA_MODEL\n",
    "\n",
    "            we_file_name = 'ET_' + str(embedding_type.value) + '_UE_' + str(use_embedding.value) +\\\n",
    "                           '_EF_' + word_embedding_custom_file.split('.')[0] + '_glove6B300d_glorot'\n",
    "\n",
    "            generate_model(exp, 'exp9_' + we_file_name[0:13] + we_file_name[18:30] + '_2640', we_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Test lstm_exp9_var_L3_N16_B40_E32_D0.2 + word embeddings SMHD 3 patologias (SMHD_*_2640)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializer experiment 2.3 (model SMHD_ml_gl_2640)\n",
      "Set: kernel_initializer=glorot_uniform=xavier_uniform, dataset=SMHD_2640 multi-label\n",
      "Loading data... /home/vanessa/PycharmProjects/RecurrentNetworks/tokenizers/anx_dep_multilabel/SMHD_TR_2640_VS_5000_TF_0_DF_0_RSW_F_IT_1_RP_F\n",
      "Preprocess data... /home/vanessa/PycharmProjects/RecurrentNetworks/dataset/anx_dep_multilabel/SMHD_train_2640.df\n",
      "Load Pre-train embeddings\n",
      "Load data - Ini: 2020-03-11 16:37:52\tEnd: 2020-03-11 16:38:16\tTotal: 0:00:24.774836\n",
      "Training using single GPU or CPU..\n",
      "Train on 2200 samples, validate on 2200 samples\n",
      "Epoch 1/32\n",
      "2200/2200 [==============================] - 567s 258ms/step - loss: 0.6773 - acc: 0.5908 - val_loss: 0.6719 - val_acc: 0.6000\n",
      "Epoch 2/32\n",
      "2200/2200 [==============================] - 536s 244ms/step - loss: 0.6701 - acc: 0.6003 - val_loss: 0.6612 - val_acc: 0.6029\n",
      "Epoch 3/32\n",
      "2200/2200 [==============================] - 531s 241ms/step - loss: 0.6590 - acc: 0.6032 - val_loss: 0.6446 - val_acc: 0.6289\n",
      "Epoch 4/32\n",
      "2200/2200 [==============================] - 534s 243ms/step - loss: 0.6481 - acc: 0.6248 - val_loss: 0.6413 - val_acc: 0.6276\n",
      "Epoch 5/32\n",
      "2200/2200 [==============================] - 530s 241ms/step - loss: 0.6401 - acc: 0.6288 - val_loss: 0.6117 - val_acc: 0.6677\n",
      "Epoch 6/32\n",
      "2200/2200 [==============================] - 529s 241ms/step - loss: 0.6381 - acc: 0.6273 - val_loss: 0.6181 - val_acc: 0.6571\n",
      "Epoch 7/32\n",
      "2200/2200 [==============================] - 531s 241ms/step - loss: 0.6342 - acc: 0.6405 - val_loss: 0.6632 - val_acc: 0.6074\n",
      "Epoch 8/32\n",
      "2200/2200 [==============================] - 528s 240ms/step - loss: 0.6421 - acc: 0.6235 - val_loss: 0.6499 - val_acc: 0.6267\n",
      "Epoch 9/32\n",
      "2200/2200 [==============================] - 532s 242ms/step - loss: 0.6345 - acc: 0.6311 - val_loss: 0.6320 - val_acc: 0.6305\n",
      "Epoch 10/32\n",
      "2200/2200 [==============================] - 529s 241ms/step - loss: 0.6385 - acc: 0.6242 - val_loss: 0.6324 - val_acc: 0.6236\n",
      "Epoch 11/32\n",
      "2200/2200 [==============================] - 532s 242ms/step - loss: 0.6306 - acc: 0.6406 - val_loss: 0.6225 - val_acc: 0.6464\n",
      "Epoch 12/32\n",
      "2200/2200 [==============================] - 539s 245ms/step - loss: 0.6256 - acc: 0.6420 - val_loss: 0.6209 - val_acc: 0.6527\n",
      "Epoch 13/32\n",
      "2200/2200 [==============================] - 515s 234ms/step - loss: 0.6160 - acc: 0.6564 - val_loss: 0.6135 - val_acc: 0.6580\n",
      "Epoch 14/32\n",
      "2200/2200 [==============================] - 519s 236ms/step - loss: 0.6184 - acc: 0.6571 - val_loss: 0.6182 - val_acc: 0.6456\n",
      "Epoch 15/32\n",
      "2200/2200 [==============================] - 504s 229ms/step - loss: 0.6411 - acc: 0.6288 - val_loss: 0.6627 - val_acc: 0.6094\n",
      "Epoch 16/32\n",
      "2200/2200 [==============================] - 485s 221ms/step - loss: 0.6412 - acc: 0.6259 - val_loss: 0.6295 - val_acc: 0.6418\n",
      "Epoch 17/32\n",
      "2200/2200 [==============================] - 484s 220ms/step - loss: 0.6188 - acc: 0.6541 - val_loss: 0.6193 - val_acc: 0.6552\n",
      "Epoch 18/32\n",
      "2200/2200 [==============================] - 484s 220ms/step - loss: 0.6182 - acc: 0.6489 - val_loss: 0.6450 - val_acc: 0.6280\n",
      "Epoch 19/32\n",
      "2200/2200 [==============================] - 494s 224ms/step - loss: 0.6111 - acc: 0.6664 - val_loss: 0.6000 - val_acc: 0.6755\n",
      "Epoch 20/32\n",
      "2200/2200 [==============================] - 500s 227ms/step - loss: 0.6049 - acc: 0.6691 - val_loss: 0.5969 - val_acc: 0.6861\n",
      "Epoch 21/32\n",
      "2200/2200 [==============================] - 501s 228ms/step - loss: 0.6025 - acc: 0.6732 - val_loss: 0.6260 - val_acc: 0.6353\n",
      "Epoch 22/32\n",
      "2200/2200 [==============================] - 500s 227ms/step - loss: 0.6069 - acc: 0.6561 - val_loss: 0.6283 - val_acc: 0.6267\n",
      "Epoch 23/32\n",
      "2200/2200 [==============================] - 499s 227ms/step - loss: 0.6183 - acc: 0.6397 - val_loss: 0.6067 - val_acc: 0.6727\n",
      "Epoch 24/32\n",
      "2200/2200 [==============================] - 500s 227ms/step - loss: 0.6135 - acc: 0.6517 - val_loss: 0.5975 - val_acc: 0.6800\n",
      "Epoch 25/32\n",
      "2200/2200 [==============================] - 499s 227ms/step - loss: 0.5947 - acc: 0.6847 - val_loss: 0.5891 - val_acc: 0.6859\n",
      "Epoch 26/32\n",
      "2200/2200 [==============================] - 501s 228ms/step - loss: 0.6144 - acc: 0.6523 - val_loss: 0.6258 - val_acc: 0.6350\n",
      "Epoch 27/32\n",
      "2200/2200 [==============================] - 501s 228ms/step - loss: 0.6152 - acc: 0.6436 - val_loss: 0.5993 - val_acc: 0.6758\n",
      "Epoch 28/32\n",
      "2200/2200 [==============================] - 501s 228ms/step - loss: 0.6117 - acc: 0.6503 - val_loss: 0.6108 - val_acc: 0.6611\n",
      "Epoch 29/32\n",
      "2200/2200 [==============================] - 501s 228ms/step - loss: 0.6120 - acc: 0.6676 - val_loss: 0.6103 - val_acc: 0.6583\n",
      "Epoch 30/32\n",
      "2200/2200 [==============================] - 499s 227ms/step - loss: 0.6206 - acc: 0.6479 - val_loss: 0.5916 - val_acc: 0.6908\n",
      "Epoch 31/32\n",
      "2200/2200 [==============================] - 500s 227ms/step - loss: 0.6137 - acc: 0.6521 - val_loss: 0.6013 - val_acc: 0.6748\n",
      "Epoch 32/32\n",
      "2200/2200 [==============================] - 499s 227ms/step - loss: 0.5844 - acc: 0.6892 - val_loss: 0.5939 - val_acc: 0.6852\n",
      "Generate Model - Ini: 2020-03-11 16:38:25\tEnd: 2020-03-11 21:12:00\tTotal: 4:33:35.652936\n",
      "Total experiment - Ini: 2020-03-11 16:38:18\tEnd: 2020-03-11 21:12:00\tTotal: 4:33:42.916639\n",
      "Loading data... /home/vanessa/PycharmProjects/RecurrentNetworks/tokenizers/anx_dep_multilabel/SMHD_TR_2640_VS_5000_TF_0_DF_0_RSW_F_IT_1_RP_F\n",
      "Preprocess data... /home/vanessa/PycharmProjects/RecurrentNetworks/dataset/anx_dep_multilabel/SMHD_test_2640.df\n",
      "Load data - Ini: 2020-03-11 21:12:00\tEnd: 2020-03-11 21:12:12\tTotal: 0:00:11.378052\n",
      "2200/2200 [==============================] - 75s 34ms/step\n",
      "Loading data... /home/vanessa/PycharmProjects/RecurrentNetworks/tokenizers/anx_dep_multilabel/SMHD_TR_2640_VS_5000_TF_0_DF_0_RSW_F_IT_1_RP_F\n",
      "Preprocess data... /home/vanessa/PycharmProjects/RecurrentNetworks/dataset/anx_dep_multilabel/SMHD_train_2640.df\n",
      "Load data - Ini: 2020-03-11 21:13:44\tEnd: 2020-03-11 21:14:07\tTotal: 0:00:22.828332\n",
      "Training using single GPU or CPU..\n",
      "Train on 2200 samples, validate on 2200 samples\n",
      "Epoch 1/32\n",
      "2200/2200 [==============================] - 556s 253ms/step - loss: 0.6802 - acc: 0.5941 - val_loss: 0.6730 - val_acc: 0.6000\n",
      "Epoch 2/32\n",
      "2200/2200 [==============================] - 522s 237ms/step - loss: 0.6744 - acc: 0.6000 - val_loss: 0.6729 - val_acc: 0.6000\n",
      "Epoch 3/32\n",
      "2200/2200 [==============================] - 522s 237ms/step - loss: 0.6742 - acc: 0.6000 - val_loss: 0.6726 - val_acc: 0.6000\n",
      "Epoch 4/32\n",
      "2200/2200 [==============================] - 523s 238ms/step - loss: 0.6698 - acc: 0.6008 - val_loss: 0.6645 - val_acc: 0.5997\n",
      "Epoch 5/32\n",
      "2200/2200 [==============================] - 517s 235ms/step - loss: 0.6579 - acc: 0.6038 - val_loss: 0.6537 - val_acc: 0.6086\n",
      "Epoch 6/32\n",
      "2200/2200 [==============================] - 523s 238ms/step - loss: 0.6484 - acc: 0.6226 - val_loss: 0.6462 - val_acc: 0.6235\n",
      "Epoch 7/32\n",
      "2200/2200 [==============================] - 524s 238ms/step - loss: 0.6469 - acc: 0.6229 - val_loss: 0.6518 - val_acc: 0.6153\n",
      "Epoch 8/32\n",
      "2200/2200 [==============================] - 526s 239ms/step - loss: 0.6398 - acc: 0.6303 - val_loss: 0.6527 - val_acc: 0.6215\n",
      "Epoch 9/32\n",
      "2200/2200 [==============================] - 525s 239ms/step - loss: 0.6422 - acc: 0.6273 - val_loss: 0.6486 - val_acc: 0.6179\n",
      "Epoch 10/32\n",
      "2200/2200 [==============================] - 529s 240ms/step - loss: 0.6290 - acc: 0.6447 - val_loss: 0.6480 - val_acc: 0.6230\n",
      "Epoch 11/32\n",
      "2200/2200 [==============================] - 523s 238ms/step - loss: 0.6266 - acc: 0.6503 - val_loss: 0.6468 - val_acc: 0.6218\n",
      "Epoch 12/32\n",
      "2200/2200 [==============================] - 523s 238ms/step - loss: 0.6432 - acc: 0.6202 - val_loss: 0.6547 - val_acc: 0.6091\n",
      "Epoch 13/32\n",
      "2200/2200 [==============================] - 525s 239ms/step - loss: 0.6317 - acc: 0.6400 - val_loss: 0.6700 - val_acc: 0.6080\n",
      "Epoch 14/32\n",
      "2200/2200 [==============================] - 524s 238ms/step - loss: 0.6462 - acc: 0.6218 - val_loss: 0.6516 - val_acc: 0.6124\n",
      "Epoch 15/32\n",
      "2200/2200 [==============================] - 527s 239ms/step - loss: 0.6290 - acc: 0.6423 - val_loss: 0.6472 - val_acc: 0.6217\n",
      "Epoch 16/32\n",
      "2200/2200 [==============================] - 530s 241ms/step - loss: 0.6251 - acc: 0.6470 - val_loss: 0.6509 - val_acc: 0.6224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/32\n",
      "2200/2200 [==============================] - 529s 240ms/step - loss: 0.6272 - acc: 0.6453 - val_loss: 0.6499 - val_acc: 0.6156\n",
      "Epoch 18/32\n",
      "2200/2200 [==============================] - 519s 236ms/step - loss: 0.6240 - acc: 0.6456 - val_loss: 0.6518 - val_acc: 0.6191\n",
      "Epoch 19/32\n",
      "2200/2200 [==============================] - 525s 239ms/step - loss: 0.6237 - acc: 0.6491 - val_loss: 0.6562 - val_acc: 0.6124\n",
      "Epoch 20/32\n",
      "2200/2200 [==============================] - 524s 238ms/step - loss: 0.6262 - acc: 0.6338 - val_loss: 0.6568 - val_acc: 0.6153\n",
      "Epoch 21/32\n",
      "2200/2200 [==============================] - 521s 237ms/step - loss: 0.6261 - acc: 0.6432 - val_loss: 0.6504 - val_acc: 0.6233\n",
      "Epoch 22/32\n",
      "2200/2200 [==============================] - 521s 237ms/step - loss: 0.6234 - acc: 0.6514 - val_loss: 0.6456 - val_acc: 0.6232\n",
      "Epoch 23/32\n",
      "2200/2200 [==============================] - 518s 235ms/step - loss: 0.6297 - acc: 0.6444 - val_loss: 0.6461 - val_acc: 0.6208\n",
      "Epoch 24/32\n",
      "2200/2200 [==============================] - 550s 250ms/step - loss: 0.6223 - acc: 0.6459 - val_loss: 0.6534 - val_acc: 0.6088\n",
      "Epoch 25/32\n",
      "2200/2200 [==============================] - 521s 237ms/step - loss: 0.6144 - acc: 0.6555 - val_loss: 0.6512 - val_acc: 0.6198\n",
      "Epoch 26/32\n",
      "2200/2200 [==============================] - 520s 237ms/step - loss: 0.6126 - acc: 0.6580 - val_loss: 0.6580 - val_acc: 0.6161\n",
      "Epoch 27/32\n",
      "2200/2200 [==============================] - 521s 237ms/step - loss: 0.6183 - acc: 0.6495 - val_loss: 0.6521 - val_acc: 0.6194\n",
      "Epoch 28/32\n",
      "2200/2200 [==============================] - 509s 231ms/step - loss: 0.6181 - acc: 0.6465 - val_loss: 0.6504 - val_acc: 0.6208\n",
      "Epoch 29/32\n",
      "2200/2200 [==============================] - 495s 225ms/step - loss: 0.6148 - acc: 0.6488 - val_loss: 0.6483 - val_acc: 0.6227\n",
      "Epoch 30/32\n",
      "2200/2200 [==============================] - 494s 225ms/step - loss: 0.6516 - acc: 0.6167 - val_loss: 0.6654 - val_acc: 0.5997\n",
      "Epoch 31/32\n",
      "2200/2200 [==============================] - 496s 225ms/step - loss: 0.6453 - acc: 0.6236 - val_loss: 0.6541 - val_acc: 0.6156\n",
      "Epoch 32/32\n",
      "2200/2200 [==============================] - 491s 223ms/step - loss: 0.6300 - acc: 0.6402 - val_loss: 0.6567 - val_acc: 0.6120\n",
      "Generate Model - Ini: 2020-03-11 21:14:07\tEnd: 2020-03-12 01:52:17\tTotal: 4:38:09.648901\n",
      "Total experiment - Ini: 2020-03-11 21:14:07\tEnd: 2020-03-12 01:52:17\tTotal: 4:38:09.649495\n",
      "Loading data... /home/vanessa/PycharmProjects/RecurrentNetworks/tokenizers/anx_dep_multilabel/SMHD_TR_2640_VS_5000_TF_0_DF_0_RSW_F_IT_1_RP_F\n",
      "Preprocess data... /home/vanessa/PycharmProjects/RecurrentNetworks/dataset/anx_dep_multilabel/SMHD_test_2640.df\n",
      "Load data - Ini: 2020-03-12 01:52:17\tEnd: 2020-03-12 01:52:28\tTotal: 0:00:11.575014\n",
      "2200/2200 [==============================] - 74s 34ms/step\n",
      "Loading data... /home/vanessa/PycharmProjects/RecurrentNetworks/tokenizers/anx_dep_multilabel/SMHD_TR_2640_VS_5000_TF_0_DF_0_RSW_F_IT_1_RP_F\n",
      "Preprocess data... /home/vanessa/PycharmProjects/RecurrentNetworks/dataset/anx_dep_multilabel/SMHD_train_2640.df\n",
      "Load Pre-train embeddings\n",
      "Load data - Ini: 2020-03-12 01:54:01\tEnd: 2020-03-12 01:54:26\tTotal: 0:00:24.071185\n",
      "Training using single GPU or CPU..\n",
      "Train on 2200 samples, validate on 2200 samples\n",
      "Epoch 1/32\n",
      "2200/2200 [==============================] - 555s 252ms/step - loss: 0.6817 - acc: 0.5812 - val_loss: 0.6745 - val_acc: 0.6000\n",
      "Epoch 2/32\n",
      "2200/2200 [==============================] - 536s 244ms/step - loss: 0.6746 - acc: 0.6000 - val_loss: 0.6725 - val_acc: 0.6000\n",
      "Epoch 3/32\n",
      "2200/2200 [==============================] - 524s 238ms/step - loss: 0.6716 - acc: 0.6000 - val_loss: 0.6706 - val_acc: 0.6000\n",
      "Epoch 4/32\n",
      "2200/2200 [==============================] - 519s 236ms/step - loss: 0.6658 - acc: 0.6036 - val_loss: 0.6668 - val_acc: 0.6017\n",
      "Epoch 5/32\n",
      "2200/2200 [==============================] - 516s 235ms/step - loss: 0.6662 - acc: 0.6012 - val_loss: 0.6674 - val_acc: 0.6000\n",
      "Epoch 6/32\n",
      "2200/2200 [==============================] - 517s 235ms/step - loss: 0.6566 - acc: 0.6064 - val_loss: 0.6647 - val_acc: 0.5995\n",
      "Epoch 7/32\n",
      "2200/2200 [==============================] - 512s 233ms/step - loss: 0.6444 - acc: 0.6256 - val_loss: 0.6674 - val_acc: 0.5912\n",
      "Epoch 8/32\n",
      "2200/2200 [==============================] - 515s 234ms/step - loss: 0.6482 - acc: 0.6230 - val_loss: 0.6648 - val_acc: 0.6023\n",
      "Epoch 9/32\n",
      "2200/2200 [==============================] - 508s 231ms/step - loss: 0.6512 - acc: 0.6168 - val_loss: 0.6588 - val_acc: 0.5947\n",
      "Epoch 10/32\n",
      "2200/2200 [==============================] - 508s 231ms/step - loss: 0.6612 - acc: 0.6026 - val_loss: 0.6612 - val_acc: 0.6042\n",
      "Epoch 11/32\n",
      "2200/2200 [==============================] - 508s 231ms/step - loss: 0.6433 - acc: 0.6236 - val_loss: 0.6611 - val_acc: 0.6020\n",
      "Epoch 12/32\n",
      "2200/2200 [==============================] - 508s 231ms/step - loss: 0.6358 - acc: 0.6345 - val_loss: 0.6653 - val_acc: 0.5998\n",
      "Epoch 13/32\n",
      "2200/2200 [==============================] - 512s 233ms/step - loss: 0.6392 - acc: 0.6276 - val_loss: 0.6653 - val_acc: 0.5938\n",
      "Epoch 14/32\n",
      "2200/2200 [==============================] - 521s 237ms/step - loss: 0.6345 - acc: 0.6348 - val_loss: 0.6641 - val_acc: 0.6015\n",
      "Epoch 15/32\n",
      "2200/2200 [==============================] - 521s 237ms/step - loss: 0.6338 - acc: 0.6359 - val_loss: 0.6685 - val_acc: 0.5968\n",
      "Epoch 16/32\n",
      "2200/2200 [==============================] - 522s 237ms/step - loss: 0.6323 - acc: 0.6338 - val_loss: 0.6638 - val_acc: 0.5980\n",
      "Epoch 17/32\n",
      "2200/2200 [==============================] - 522s 237ms/step - loss: 0.6281 - acc: 0.6439 - val_loss: 0.6637 - val_acc: 0.5967\n",
      "Epoch 18/32\n",
      "2200/2200 [==============================] - 522s 237ms/step - loss: 0.6275 - acc: 0.6477 - val_loss: 0.6645 - val_acc: 0.5986\n",
      "Epoch 19/32\n",
      "2200/2200 [==============================] - 522s 237ms/step - loss: 0.6240 - acc: 0.6445 - val_loss: 0.6658 - val_acc: 0.5985\n",
      "Epoch 20/32\n",
      "2200/2200 [==============================] - 521s 237ms/step - loss: 0.6253 - acc: 0.6465 - val_loss: 0.6613 - val_acc: 0.6030\n",
      "Epoch 21/32\n",
      "2200/2200 [==============================] - 519s 236ms/step - loss: 0.6192 - acc: 0.6582 - val_loss: 0.6640 - val_acc: 0.6023\n",
      "Epoch 22/32\n",
      "2200/2200 [==============================] - 521s 237ms/step - loss: 0.6187 - acc: 0.6570 - val_loss: 0.6679 - val_acc: 0.5983\n",
      "Epoch 23/32\n",
      "2200/2200 [==============================] - 521s 237ms/step - loss: 0.6203 - acc: 0.6553 - val_loss: 0.6659 - val_acc: 0.5994\n",
      "Epoch 24/32\n",
      "2200/2200 [==============================] - 521s 237ms/step - loss: 0.6170 - acc: 0.6576 - val_loss: 0.6636 - val_acc: 0.6008\n",
      "Epoch 25/32\n",
      "2200/2200 [==============================] - 520s 236ms/step - loss: 0.6196 - acc: 0.6542 - val_loss: 0.6628 - val_acc: 0.6136\n",
      "Epoch 00025: early stopping\n",
      "Generate Model - Ini: 2020-03-12 01:54:37\tEnd: 2020-03-12 05:31:19\tTotal: 3:36:42.225885\n",
      "Total experiment - Ini: 2020-03-12 01:54:26\tEnd: 2020-03-12 05:31:19\tTotal: 3:36:52.664750\n",
      "Loading data... /home/vanessa/PycharmProjects/RecurrentNetworks/tokenizers/anx_dep_multilabel/SMHD_TR_2640_VS_5000_TF_0_DF_0_RSW_F_IT_1_RP_F\n",
      "Preprocess data... /home/vanessa/PycharmProjects/RecurrentNetworks/dataset/anx_dep_multilabel/SMHD_test_2640.df\n",
      "Load data - Ini: 2020-03-12 05:31:19\tEnd: 2020-03-12 05:31:30\tTotal: 0:00:11.270742\n",
      "2200/2200 [==============================] - 66s 30ms/step\n",
      "Loading data... /home/vanessa/PycharmProjects/RecurrentNetworks/tokenizers/anx_dep_multilabel/SMHD_TR_2640_VS_5000_TF_0_DF_0_RSW_F_IT_1_RP_F\n",
      "Preprocess data... /home/vanessa/PycharmProjects/RecurrentNetworks/dataset/anx_dep_multilabel/SMHD_train_2640.df\n",
      "Load data - Ini: 2020-03-12 05:32:57\tEnd: 2020-03-12 05:33:21\tTotal: 0:00:23.775851\n",
      "Training using single GPU or CPU..\n",
      "Train on 2200 samples, validate on 2200 samples\n",
      "Epoch 1/32\n",
      "2200/2200 [==============================] - 555s 252ms/step - loss: 0.6803 - acc: 0.5941 - val_loss: 0.6730 - val_acc: 0.6000\n",
      "Epoch 2/32\n",
      "2200/2200 [==============================] - 522s 237ms/step - loss: 0.6744 - acc: 0.6000 - val_loss: 0.6729 - val_acc: 0.6000\n",
      "Epoch 3/32\n",
      "2200/2200 [==============================] - 521s 237ms/step - loss: 0.6742 - acc: 0.6000 - val_loss: 0.6726 - val_acc: 0.6000\n",
      "Epoch 4/32\n",
      "2200/2200 [==============================] - 519s 236ms/step - loss: 0.6704 - acc: 0.6002 - val_loss: 0.6680 - val_acc: 0.6000\n",
      "Epoch 5/32\n",
      "2200/2200 [==============================] - 515s 234ms/step - loss: 0.6695 - acc: 0.5880 - val_loss: 0.6864 - val_acc: 0.6000\n",
      "Epoch 6/32\n",
      "2200/2200 [==============================] - 512s 233ms/step - loss: 0.6733 - acc: 0.5988 - val_loss: 0.6684 - val_acc: 0.6000\n",
      "Epoch 7/32\n",
      "2200/2200 [==============================] - 513s 233ms/step - loss: 0.6703 - acc: 0.5980 - val_loss: 0.6600 - val_acc: 0.6000\n",
      "Epoch 8/32\n",
      "2200/2200 [==============================] - 512s 233ms/step - loss: 0.6669 - acc: 0.5992 - val_loss: 0.6544 - val_acc: 0.6248\n",
      "Epoch 9/32\n",
      "2200/2200 [==============================] - 513s 233ms/step - loss: 0.6635 - acc: 0.6038 - val_loss: 0.6635 - val_acc: 0.6165\n",
      "Epoch 10/32\n",
      "2200/2200 [==============================] - 513s 233ms/step - loss: 0.6735 - acc: 0.5912 - val_loss: 0.6724 - val_acc: 0.6000\n",
      "Epoch 11/32\n",
      "2200/2200 [==============================] - 513s 233ms/step - loss: 0.6715 - acc: 0.5995 - val_loss: 0.6715 - val_acc: 0.6000\n",
      "Epoch 12/32\n",
      "2200/2200 [==============================] - 514s 233ms/step - loss: 0.6724 - acc: 0.6006 - val_loss: 0.6666 - val_acc: 0.6000\n",
      "Epoch 13/32\n",
      "2200/2200 [==============================] - 512s 233ms/step - loss: 0.6708 - acc: 0.5995 - val_loss: 0.6656 - val_acc: 0.6000\n",
      "Epoch 14/32\n",
      "2200/2200 [==============================] - 513s 233ms/step - loss: 0.6677 - acc: 0.5995 - val_loss: 0.6534 - val_acc: 0.6000\n",
      "Epoch 15/32\n",
      "2200/2200 [==============================] - 514s 233ms/step - loss: 0.6686 - acc: 0.5986 - val_loss: 0.6533 - val_acc: 0.6000\n",
      "Epoch 16/32\n",
      "2200/2200 [==============================] - 514s 234ms/step - loss: 0.6665 - acc: 0.5995 - val_loss: 0.6511 - val_acc: 0.6000\n",
      "Epoch 17/32\n",
      "2200/2200 [==============================] - 513s 233ms/step - loss: 0.6669 - acc: 0.5980 - val_loss: 0.6511 - val_acc: 0.6000\n",
      "Epoch 18/32\n",
      "2200/2200 [==============================] - 512s 233ms/step - loss: 0.6644 - acc: 0.5980 - val_loss: 0.6550 - val_acc: 0.6000\n",
      "Epoch 19/32\n",
      "2200/2200 [==============================] - 513s 233ms/step - loss: 0.6642 - acc: 0.6012 - val_loss: 0.6612 - val_acc: 0.6194\n",
      "Epoch 20/32\n",
      "2200/2200 [==============================] - 511s 232ms/step - loss: 0.6660 - acc: 0.5959 - val_loss: 0.6489 - val_acc: 0.6000\n",
      "Epoch 21/32\n",
      "2200/2200 [==============================] - 509s 231ms/step - loss: 0.6636 - acc: 0.6017 - val_loss: 0.6774 - val_acc: 0.6012\n",
      "Epoch 22/32\n",
      "2200/2200 [==============================] - 510s 232ms/step - loss: 0.6688 - acc: 0.6055 - val_loss: 0.6727 - val_acc: 0.6006\n",
      "Epoch 23/32\n",
      "2200/2200 [==============================] - 510s 232ms/step - loss: 0.6618 - acc: 0.6129 - val_loss: 0.6633 - val_acc: 0.6138\n",
      "Epoch 24/32\n",
      "2200/2200 [==============================] - 511s 232ms/step - loss: 0.6695 - acc: 0.5933 - val_loss: 0.6649 - val_acc: 0.5992\n",
      "Epoch 25/32\n",
      "2200/2200 [==============================] - 511s 232ms/step - loss: 0.6652 - acc: 0.5970 - val_loss: 0.6603 - val_acc: 0.6000\n",
      "Epoch 26/32\n",
      "2200/2200 [==============================] - 510s 232ms/step - loss: 0.6709 - acc: 0.5989 - val_loss: 0.6755 - val_acc: 0.6000\n",
      "Epoch 27/32\n",
      "2200/2200 [==============================] - 510s 232ms/step - loss: 0.6667 - acc: 0.6048 - val_loss: 0.6720 - val_acc: 0.6029\n",
      "Epoch 28/32\n",
      "2200/2200 [==============================] - 511s 232ms/step - loss: 0.6631 - acc: 0.6112 - val_loss: 0.6737 - val_acc: 0.6021\n",
      "Epoch 29/32\n",
      "2200/2200 [==============================] - 509s 231ms/step - loss: 0.6634 - acc: 0.6089 - val_loss: 0.6694 - val_acc: 0.6067\n",
      "Epoch 30/32\n",
      "2200/2200 [==============================] - 504s 229ms/step - loss: 0.6607 - acc: 0.6118 - val_loss: 0.6507 - val_acc: 0.6109\n",
      "Epoch 31/32\n",
      "2200/2200 [==============================] - 503s 229ms/step - loss: 0.6557 - acc: 0.6132 - val_loss: 0.6495 - val_acc: 0.6156\n",
      "Epoch 32/32\n",
      "2200/2200 [==============================] - 505s 229ms/step - loss: 0.6494 - acc: 0.6173 - val_loss: 0.6407 - val_acc: 0.6279\n",
      "Generate Model - Ini: 2020-03-12 05:33:22\tEnd: 2020-03-12 10:07:28\tTotal: 4:34:06.033418\n",
      "Total experiment - Ini: 2020-03-12 05:33:22\tEnd: 2020-03-12 10:07:28\tTotal: 4:34:06.034086\n",
      "Loading data... /home/vanessa/PycharmProjects/RecurrentNetworks/tokenizers/anx_dep_multilabel/SMHD_TR_2640_VS_5000_TF_0_DF_0_RSW_F_IT_1_RP_F\n",
      "Preprocess data... /home/vanessa/PycharmProjects/RecurrentNetworks/dataset/anx_dep_multilabel/SMHD_test_2640.df\n",
      "Load data - Ini: 2020-03-12 10:07:28\tEnd: 2020-03-12 10:07:40\tTotal: 0:00:11.684782\n",
      "2200/2200 [==============================] - 69s 31ms/step\n",
      "Loading data... /home/vanessa/PycharmProjects/RecurrentNetworks/tokenizers/anx_dep_multilabel/SMHD_TR_2640_VS_5000_TF_0_DF_0_RSW_F_IT_1_RP_F\n",
      "Preprocess data... /home/vanessa/PycharmProjects/RecurrentNetworks/dataset/anx_dep_multilabel/SMHD_train_2640.df\n",
      "Load Pre-train embeddings\n",
      "Found 688071 word vectors.\n",
      "Load data - Ini: 2020-03-12 10:09:12\tEnd: 2020-03-12 10:09:43\tTotal: 0:00:31.219673\n",
      "Training using single GPU or CPU..\n",
      "Train on 2200 samples, validate on 2200 samples\n",
      "Epoch 1/32\n",
      "2200/2200 [==============================] - 547s 249ms/step - loss: 0.6790 - acc: 0.5973 - val_loss: 0.6732 - val_acc: 0.6000\n",
      "Epoch 2/32\n",
      "2200/2200 [==============================] - 519s 236ms/step - loss: 0.6740 - acc: 0.5998 - val_loss: 0.6726 - val_acc: 0.6000\n",
      "Epoch 3/32\n",
      "2200/2200 [==============================] - 505s 230ms/step - loss: 0.6725 - acc: 0.6000 - val_loss: 0.6713 - val_acc: 0.6000\n",
      "Epoch 4/32\n",
      "2200/2200 [==============================] - 494s 225ms/step - loss: 0.6686 - acc: 0.6002 - val_loss: 0.6664 - val_acc: 0.5961\n",
      "Epoch 5/32\n",
      "2200/2200 [==============================] - 496s 225ms/step - loss: 0.6637 - acc: 0.5965 - val_loss: 0.6638 - val_acc: 0.6017\n",
      "Epoch 6/32\n",
      "2200/2200 [==============================] - 495s 225ms/step - loss: 0.6593 - acc: 0.6088 - val_loss: 0.6591 - val_acc: 0.6059\n",
      "Epoch 7/32\n",
      "2200/2200 [==============================] - 495s 225ms/step - loss: 0.6506 - acc: 0.6132 - val_loss: 0.6727 - val_acc: 0.5885\n",
      "Epoch 8/32\n",
      "2200/2200 [==============================] - 496s 225ms/step - loss: 0.6592 - acc: 0.6006 - val_loss: 0.6591 - val_acc: 0.6048\n",
      "Epoch 9/32\n",
      "2200/2200 [==============================] - 495s 225ms/step - loss: 0.6511 - acc: 0.6088 - val_loss: 0.6592 - val_acc: 0.5986\n",
      "Epoch 10/32\n",
      "2200/2200 [==============================] - 495s 225ms/step - loss: 0.6769 - acc: 0.5950 - val_loss: 0.6731 - val_acc: 0.6000\n",
      "Epoch 11/32\n",
      "2200/2200 [==============================] - 496s 225ms/step - loss: 0.6734 - acc: 0.6000 - val_loss: 0.6730 - val_acc: 0.6000\n",
      "Epoch 12/32\n",
      "2200/2200 [==============================] - 497s 226ms/step - loss: 0.6730 - acc: 0.6002 - val_loss: 0.6730 - val_acc: 0.6000\n",
      "Epoch 13/32\n",
      "2200/2200 [==============================] - 496s 225ms/step - loss: 0.6723 - acc: 0.6000 - val_loss: 0.6728 - val_acc: 0.6000\n",
      "Epoch 14/32\n",
      "2200/2200 [==============================] - 496s 225ms/step - loss: 0.6727 - acc: 0.5997 - val_loss: 0.6727 - val_acc: 0.6000\n",
      "Epoch 15/32\n",
      "2200/2200 [==============================] - 496s 226ms/step - loss: 0.6719 - acc: 0.6002 - val_loss: 0.6726 - val_acc: 0.5998\n",
      "Epoch 16/32\n",
      "2200/2200 [==============================] - 497s 226ms/step - loss: 0.6705 - acc: 0.6005 - val_loss: 0.6719 - val_acc: 0.5998\n",
      "Epoch 17/32\n",
      "2200/2200 [==============================] - 499s 227ms/step - loss: 0.6688 - acc: 0.6021 - val_loss: 0.6660 - val_acc: 0.6026\n",
      "Epoch 18/32\n",
      "2200/2200 [==============================] - 497s 226ms/step - loss: 0.6625 - acc: 0.6076 - val_loss: 0.6611 - val_acc: 0.6083\n",
      "Epoch 19/32\n",
      "2200/2200 [==============================] - 496s 225ms/step - loss: 0.6598 - acc: 0.6056 - val_loss: 0.6629 - val_acc: 0.5891\n",
      "Epoch 20/32\n",
      "2200/2200 [==============================] - 497s 226ms/step - loss: 0.6597 - acc: 0.5992 - val_loss: 0.6633 - val_acc: 0.6038\n",
      "Epoch 21/32\n",
      "2200/2200 [==============================] - 496s 225ms/step - loss: 0.6685 - acc: 0.6018 - val_loss: 0.6651 - val_acc: 0.6058\n",
      "Epoch 22/32\n",
      "2200/2200 [==============================] - 498s 226ms/step - loss: 0.6604 - acc: 0.6073 - val_loss: 0.6556 - val_acc: 0.6123\n",
      "Epoch 23/32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2200/2200 [==============================] - 497s 226ms/step - loss: 0.6631 - acc: 0.6027 - val_loss: 0.6685 - val_acc: 0.6008\n",
      "Epoch 24/32\n",
      "2200/2200 [==============================] - 498s 226ms/step - loss: 0.6636 - acc: 0.6011 - val_loss: 0.6691 - val_acc: 0.6033\n",
      "Epoch 25/32\n",
      "2200/2200 [==============================] - 497s 226ms/step - loss: 0.6619 - acc: 0.6073 - val_loss: 0.6574 - val_acc: 0.6111\n",
      "Epoch 26/32\n",
      "2200/2200 [==============================] - 496s 225ms/step - loss: 0.6565 - acc: 0.6083 - val_loss: 0.6422 - val_acc: 0.6289\n",
      "Epoch 27/32\n",
      "2200/2200 [==============================] - 497s 226ms/step - loss: 0.6575 - acc: 0.6068 - val_loss: 0.6380 - val_acc: 0.6344\n",
      "Epoch 28/32\n",
      "2200/2200 [==============================] - 497s 226ms/step - loss: 0.6539 - acc: 0.6114 - val_loss: 0.6533 - val_acc: 0.6155\n",
      "Epoch 29/32\n",
      "2200/2200 [==============================] - 498s 226ms/step - loss: 0.6541 - acc: 0.6092 - val_loss: 0.6624 - val_acc: 0.6098\n",
      "Epoch 30/32\n",
      "2200/2200 [==============================] - 497s 226ms/step - loss: 0.6508 - acc: 0.6129 - val_loss: 0.6429 - val_acc: 0.6202\n",
      "Epoch 31/32\n",
      "2200/2200 [==============================] - 496s 225ms/step - loss: 0.6521 - acc: 0.6098 - val_loss: 0.6496 - val_acc: 0.6145\n",
      "Epoch 32/32\n",
      "2200/2200 [==============================] - 497s 226ms/step - loss: 0.6524 - acc: 0.6117 - val_loss: 0.6470 - val_acc: 0.6141\n",
      "Generate Model - Ini: 2020-03-12 10:09:56\tEnd: 2020-03-12 14:36:14\tTotal: 4:26:17.949636\n",
      "Total experiment - Ini: 2020-03-12 10:09:44\tEnd: 2020-03-12 14:36:14\tTotal: 4:26:30.194739\n",
      "Loading data... /home/vanessa/PycharmProjects/RecurrentNetworks/tokenizers/anx_dep_multilabel/SMHD_TR_2640_VS_5000_TF_0_DF_0_RSW_F_IT_1_RP_F\n",
      "Preprocess data... /home/vanessa/PycharmProjects/RecurrentNetworks/dataset/anx_dep_multilabel/SMHD_test_2640.df\n",
      "Load data - Ini: 2020-03-12 14:36:14\tEnd: 2020-03-12 14:36:27\tTotal: 0:00:13.176678\n",
      "2200/2200 [==============================] - 72s 33ms/step\n",
      "Loading data... /home/vanessa/PycharmProjects/RecurrentNetworks/tokenizers/anx_dep_multilabel/SMHD_TR_2640_VS_5000_TF_0_DF_0_RSW_F_IT_1_RP_F\n",
      "Preprocess data... /home/vanessa/PycharmProjects/RecurrentNetworks/dataset/anx_dep_multilabel/SMHD_train_2640.df\n",
      "Load data - Ini: 2020-03-12 14:38:05\tEnd: 2020-03-12 14:38:30\tTotal: 0:00:25.123471\n",
      "Training using single GPU or CPU..\n",
      "Train on 2200 samples, validate on 2200 samples\n",
      "Epoch 1/32\n",
      "2200/2200 [==============================] - 552s 251ms/step - loss: 0.6802 - acc: 0.5941 - val_loss: 0.6730 - val_acc: 0.6000\n",
      "Epoch 2/32\n",
      "2200/2200 [==============================] - 528s 240ms/step - loss: 0.6744 - acc: 0.6000 - val_loss: 0.6729 - val_acc: 0.6000\n",
      "Epoch 3/32\n",
      "2200/2200 [==============================] - 520s 236ms/step - loss: 0.6742 - acc: 0.6000 - val_loss: 0.6726 - val_acc: 0.6000\n",
      "Epoch 4/32\n",
      "2200/2200 [==============================] - 514s 234ms/step - loss: 0.6697 - acc: 0.6005 - val_loss: 0.6633 - val_acc: 0.6018\n",
      "Epoch 5/32\n",
      "2200/2200 [==============================] - 516s 234ms/step - loss: 0.6655 - acc: 0.6005 - val_loss: 0.6576 - val_acc: 0.6062\n",
      "Epoch 6/32\n",
      "2200/2200 [==============================] - 516s 235ms/step - loss: 0.6573 - acc: 0.6059 - val_loss: 0.6442 - val_acc: 0.6180\n",
      "Epoch 7/32\n",
      "2200/2200 [==============================] - 515s 234ms/step - loss: 0.6482 - acc: 0.6220 - val_loss: 0.6462 - val_acc: 0.6277\n",
      "Epoch 8/32\n",
      "2200/2200 [==============================] - 515s 234ms/step - loss: 0.6612 - acc: 0.5989 - val_loss: 0.6425 - val_acc: 0.6162\n",
      "Epoch 9/32\n",
      "2200/2200 [==============================] - 513s 233ms/step - loss: 0.6502 - acc: 0.6164 - val_loss: 0.6530 - val_acc: 0.6144\n",
      "Epoch 10/32\n",
      "2200/2200 [==============================] - 512s 233ms/step - loss: 0.6353 - acc: 0.6380 - val_loss: 0.6511 - val_acc: 0.6136\n",
      "Epoch 11/32\n",
      "2200/2200 [==============================] - 514s 234ms/step - loss: 0.6329 - acc: 0.6364 - val_loss: 0.6467 - val_acc: 0.6156\n",
      "Epoch 12/32\n",
      "2200/2200 [==============================] - 514s 234ms/step - loss: 0.6337 - acc: 0.6355 - val_loss: 0.6480 - val_acc: 0.6205\n",
      "Epoch 13/32\n",
      "2200/2200 [==============================] - 513s 233ms/step - loss: 0.6681 - acc: 0.6011 - val_loss: 0.6755 - val_acc: 0.6002\n",
      "Epoch 14/32\n",
      "2200/2200 [==============================] - 513s 233ms/step - loss: 0.6730 - acc: 0.6018 - val_loss: 0.6733 - val_acc: 0.6002\n",
      "Epoch 15/32\n",
      "2200/2200 [==============================] - 512s 233ms/step - loss: 0.6741 - acc: 0.6006 - val_loss: 0.6730 - val_acc: 0.6002\n",
      "Epoch 16/32\n",
      "2200/2200 [==============================] - 512s 233ms/step - loss: 0.6725 - acc: 0.6006 - val_loss: 0.6731 - val_acc: 0.6002\n",
      "Epoch 17/32\n",
      "2200/2200 [==============================] - 513s 233ms/step - loss: 0.6713 - acc: 0.6014 - val_loss: 0.6732 - val_acc: 0.6002\n",
      "Epoch 18/32\n",
      "2200/2200 [==============================] - 516s 235ms/step - loss: 0.6714 - acc: 0.6024 - val_loss: 0.6730 - val_acc: 0.6002\n",
      "Epoch 19/32\n",
      "2200/2200 [==============================] - 514s 234ms/step - loss: 0.6721 - acc: 0.6012 - val_loss: 0.6730 - val_acc: 0.6002\n",
      "Epoch 20/32\n",
      "2200/2200 [==============================] - 514s 233ms/step - loss: 0.6713 - acc: 0.6021 - val_loss: 0.6730 - val_acc: 0.6002\n",
      "Epoch 21/32\n",
      "2200/2200 [==============================] - 513s 233ms/step - loss: 0.6722 - acc: 0.6012 - val_loss: 0.6729 - val_acc: 0.6002\n",
      "Epoch 22/32\n",
      "2200/2200 [==============================] - 514s 234ms/step - loss: 0.6714 - acc: 0.6017 - val_loss: 0.6730 - val_acc: 0.6002\n",
      "Epoch 23/32\n",
      "2200/2200 [==============================] - 514s 234ms/step - loss: 0.6709 - acc: 0.6027 - val_loss: 0.6732 - val_acc: 0.6002\n",
      "Epoch 24/32\n",
      "2200/2200 [==============================] - 512s 233ms/step - loss: 0.6740 - acc: 0.5980 - val_loss: 0.6729 - val_acc: 0.6002\n",
      "Epoch 00024: early stopping\n",
      "Generate Model - Ini: 2020-03-12 14:38:30\tEnd: 2020-03-12 18:05:23\tTotal: 3:26:52.463679\n",
      "Total experiment - Ini: 2020-03-12 14:38:30\tEnd: 2020-03-12 18:05:23\tTotal: 3:26:52.464258\n",
      "Loading data... /home/vanessa/PycharmProjects/RecurrentNetworks/tokenizers/anx_dep_multilabel/SMHD_TR_2640_VS_5000_TF_0_DF_0_RSW_F_IT_1_RP_F\n",
      "Preprocess data... /home/vanessa/PycharmProjects/RecurrentNetworks/dataset/anx_dep_multilabel/SMHD_test_2640.df\n",
      "Load data - Ini: 2020-03-12 18:05:23\tEnd: 2020-03-12 18:05:35\tTotal: 0:00:11.801163\n",
      "2200/2200 [==============================] - 71s 32ms/step\n"
     ]
    }
   ],
   "source": [
    "print('Initializer experiment 2.3 (model SMHD_ml_gl_2640)\\n'+\\\n",
    "      'Set: kernel_initializer=glorot_uniform=xavier_uniform, dataset=SMHD_2640 multi-label')\n",
    "\n",
    "exp = ExperimentProcesses('lstm_exp9_var_L3')\n",
    "exp.pp_data.set_dataset_source(dataset_name='SMHD', label_set=['control', 'anxiety', 'depression'],\n",
    "                               total_registers=2640, subdirectory=\"anx_dep_multilabel\")\n",
    "\n",
    "embedding_types = [dn.EmbeddingType.WORD2VEC_CUSTOM, dn.EmbeddingType.GLOVE_CUSTOM]\n",
    "use_embeddings = [dn.UseEmbedding.STATIC, dn.UseEmbedding.NON_STATIC]\n",
    "\n",
    "for embedding_type in embedding_types:\n",
    "    if embedding_type == dn.EmbeddingType.WORD2VEC_CUSTOM:\n",
    "        word_embedding_custom_files = ['SMHD-Skipgram-A-D-ADUsers-300.bin', 'SMHD-CBOW-A-D-ADUsers-300.bin']\n",
    "    else:\n",
    "        word_embedding_custom_files = ['SMHD-glove-A-D-ADUsers-300.pkl']\n",
    "\n",
    "    for word_embedding_custom_file in word_embedding_custom_files:\n",
    "        for use_embedding in use_embeddings:\n",
    "            exp.pp_data.embedding_type = embedding_type\n",
    "            exp.pp_data.use_embedding = use_embedding\n",
    "            exp.pp_data.word_embedding_custom_file = word_embedding_custom_file\n",
    "            exp.pp_data.load_dataset_type = dn.LoadDataset.TRAIN_DATA_MODEL\n",
    "\n",
    "            we_file_name = 'ET_' + str(embedding_type.value) + '_UE_' + str(use_embedding.value) +\\\n",
    "                           '_EF_' + word_embedding_custom_file.split('.')[0] + '_glove6B300d_glorot'\n",
    "\n",
    "            generate_model(exp, 'exp9_' + we_file_name[0:13] + we_file_name[18:30] + '_2640', we_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 Test lstm_exp9_var_L3_N16_B40_E32_D0.2 +  Glove6B, glorot x lecun kernel initializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializer experiment 2.3 (model SMHD_ml_gl_2640)\n",
      "Set: kernel_initializer=glorot_uniform=xavier_uniform, dataset=SMHD_2640 multi-label\n",
      "Loading data... /home/vanessa/PycharmProjects/RecurrentNetworks/tokenizers/anx_dep_multilabel/SMHD_TR_2640_VS_5000_TF_0_DF_0_RSW_F_IT_1_RP_F\n",
      "Preprocess data... /home/vanessa/PycharmProjects/RecurrentNetworks/dataset/anx_dep_multilabel/SMHD_train_2640.df\n",
      "Load Pre-train embeddings\n",
      "Load data - Ini: 2020-03-11 16:37:52\tEnd: 2020-03-11 16:38:16\tTotal: 0:00:24.774836\n",
      "Training using single GPU or CPU..\n",
      "Train on 2200 samples, validate on 2200 samples\n",
      "Epoch 1/32\n",
      "2200/2200 [==============================] - 567s 258ms/step - loss: 0.6773 - acc: 0.5908 - val_loss: 0.6719 - val_acc: 0.6000\n",
      "Epoch 2/32\n",
      "2200/2200 [==============================] - 536s 244ms/step - loss: 0.6701 - acc: 0.6003 - val_loss: 0.6612 - val_acc: 0.6029\n",
      "Epoch 3/32\n",
      "2200/2200 [==============================] - 531s 241ms/step - loss: 0.6590 - acc: 0.6032 - val_loss: 0.6446 - val_acc: 0.6289\n",
      "Epoch 4/32\n",
      "2200/2200 [==============================] - 534s 243ms/step - loss: 0.6481 - acc: 0.6248 - val_loss: 0.6413 - val_acc: 0.6276\n",
      "Epoch 5/32\n",
      "2200/2200 [==============================] - 530s 241ms/step - loss: 0.6401 - acc: 0.6288 - val_loss: 0.6117 - val_acc: 0.6677\n",
      "Epoch 6/32\n",
      "2200/2200 [==============================] - 529s 241ms/step - loss: 0.6381 - acc: 0.6273 - val_loss: 0.6181 - val_acc: 0.6571\n",
      "Epoch 7/32\n",
      "2200/2200 [==============================] - 531s 241ms/step - loss: 0.6342 - acc: 0.6405 - val_loss: 0.6632 - val_acc: 0.6074\n",
      "Epoch 8/32\n",
      "2200/2200 [==============================] - 528s 240ms/step - loss: 0.6421 - acc: 0.6235 - val_loss: 0.6499 - val_acc: 0.6267\n",
      "Epoch 9/32\n",
      "2200/2200 [==============================] - 532s 242ms/step - loss: 0.6345 - acc: 0.6311 - val_loss: 0.6320 - val_acc: 0.6305\n",
      "Epoch 10/32\n",
      "2200/2200 [==============================] - 529s 241ms/step - loss: 0.6385 - acc: 0.6242 - val_loss: 0.6324 - val_acc: 0.6236\n",
      "Epoch 11/32\n",
      "2200/2200 [==============================] - 532s 242ms/step - loss: 0.6306 - acc: 0.6406 - val_loss: 0.6225 - val_acc: 0.6464\n",
      "Epoch 12/32\n",
      "2200/2200 [==============================] - 539s 245ms/step - loss: 0.6256 - acc: 0.6420 - val_loss: 0.6209 - val_acc: 0.6527\n",
      "Epoch 13/32\n",
      "2200/2200 [==============================] - 515s 234ms/step - loss: 0.6160 - acc: 0.6564 - val_loss: 0.6135 - val_acc: 0.6580\n",
      "Epoch 14/32\n",
      "2200/2200 [==============================] - 519s 236ms/step - loss: 0.6184 - acc: 0.6571 - val_loss: 0.6182 - val_acc: 0.6456\n",
      "Epoch 15/32\n",
      "2200/2200 [==============================] - 504s 229ms/step - loss: 0.6411 - acc: 0.6288 - val_loss: 0.6627 - val_acc: 0.6094\n",
      "Epoch 16/32\n",
      "2200/2200 [==============================] - 485s 221ms/step - loss: 0.6412 - acc: 0.6259 - val_loss: 0.6295 - val_acc: 0.6418\n",
      "Epoch 17/32\n",
      "2200/2200 [==============================] - 484s 220ms/step - loss: 0.6188 - acc: 0.6541 - val_loss: 0.6193 - val_acc: 0.6552\n",
      "Epoch 18/32\n",
      "2200/2200 [==============================] - 484s 220ms/step - loss: 0.6182 - acc: 0.6489 - val_loss: 0.6450 - val_acc: 0.6280\n",
      "Epoch 19/32\n",
      "2200/2200 [==============================] - 494s 224ms/step - loss: 0.6111 - acc: 0.6664 - val_loss: 0.6000 - val_acc: 0.6755\n",
      "Epoch 20/32\n",
      "2200/2200 [==============================] - 500s 227ms/step - loss: 0.6049 - acc: 0.6691 - val_loss: 0.5969 - val_acc: 0.6861\n",
      "Epoch 21/32\n",
      "2200/2200 [==============================] - 501s 228ms/step - loss: 0.6025 - acc: 0.6732 - val_loss: 0.6260 - val_acc: 0.6353\n",
      "Epoch 22/32\n",
      "2200/2200 [==============================] - 500s 227ms/step - loss: 0.6069 - acc: 0.6561 - val_loss: 0.6283 - val_acc: 0.6267\n",
      "Epoch 23/32\n",
      "2200/2200 [==============================] - 499s 227ms/step - loss: 0.6183 - acc: 0.6397 - val_loss: 0.6067 - val_acc: 0.6727\n",
      "Epoch 24/32\n",
      "2200/2200 [==============================] - 500s 227ms/step - loss: 0.6135 - acc: 0.6517 - val_loss: 0.5975 - val_acc: 0.6800\n",
      "Epoch 25/32\n",
      "2200/2200 [==============================] - 499s 227ms/step - loss: 0.5947 - acc: 0.6847 - val_loss: 0.5891 - val_acc: 0.6859\n",
      "Epoch 26/32\n",
      "2200/2200 [==============================] - 501s 228ms/step - loss: 0.6144 - acc: 0.6523 - val_loss: 0.6258 - val_acc: 0.6350\n",
      "Epoch 27/32\n",
      "2200/2200 [==============================] - 501s 228ms/step - loss: 0.6152 - acc: 0.6436 - val_loss: 0.5993 - val_acc: 0.6758\n",
      "Epoch 28/32\n",
      "2200/2200 [==============================] - 501s 228ms/step - loss: 0.6117 - acc: 0.6503 - val_loss: 0.6108 - val_acc: 0.6611\n",
      "Epoch 29/32\n",
      "2200/2200 [==============================] - 501s 228ms/step - loss: 0.6120 - acc: 0.6676 - val_loss: 0.6103 - val_acc: 0.6583\n",
      "Epoch 30/32\n",
      "2200/2200 [==============================] - 499s 227ms/step - loss: 0.6206 - acc: 0.6479 - val_loss: 0.5916 - val_acc: 0.6908\n",
      "Epoch 31/32\n",
      "2200/2200 [==============================] - 500s 227ms/step - loss: 0.6137 - acc: 0.6521 - val_loss: 0.6013 - val_acc: 0.6748\n",
      "Epoch 32/32\n",
      "2200/2200 [==============================] - 499s 227ms/step - loss: 0.5844 - acc: 0.6892 - val_loss: 0.5939 - val_acc: 0.6852\n",
      "Generate Model - Ini: 2020-03-11 16:38:25\tEnd: 2020-03-11 21:12:00\tTotal: 4:33:35.652936\n",
      "Total experiment - Ini: 2020-03-11 16:38:18\tEnd: 2020-03-11 21:12:00\tTotal: 4:33:42.916639\n",
      "Loading data... /home/vanessa/PycharmProjects/RecurrentNetworks/tokenizers/anx_dep_multilabel/SMHD_TR_2640_VS_5000_TF_0_DF_0_RSW_F_IT_1_RP_F\n",
      "Preprocess data... /home/vanessa/PycharmProjects/RecurrentNetworks/dataset/anx_dep_multilabel/SMHD_test_2640.df\n",
      "Load data - Ini: 2020-03-11 21:12:00\tEnd: 2020-03-11 21:12:12\tTotal: 0:00:11.378052\n",
      "2200/2200 [==============================] - 75s 34ms/step\n",
      "Loading data... /home/vanessa/PycharmProjects/RecurrentNetworks/tokenizers/anx_dep_multilabel/SMHD_TR_2640_VS_5000_TF_0_DF_0_RSW_F_IT_1_RP_F\n",
      "Preprocess data... /home/vanessa/PycharmProjects/RecurrentNetworks/dataset/anx_dep_multilabel/SMHD_train_2640.df\n",
      "Load data - Ini: 2020-03-11 21:13:44\tEnd: 2020-03-11 21:14:07\tTotal: 0:00:22.828332\n",
      "Training using single GPU or CPU..\n",
      "Train on 2200 samples, validate on 2200 samples\n",
      "Epoch 1/32\n",
      "2200/2200 [==============================] - 556s 253ms/step - loss: 0.6802 - acc: 0.5941 - val_loss: 0.6730 - val_acc: 0.6000\n",
      "Epoch 2/32\n",
      "2200/2200 [==============================] - 522s 237ms/step - loss: 0.6744 - acc: 0.6000 - val_loss: 0.6729 - val_acc: 0.6000\n",
      "Epoch 3/32\n",
      "2200/2200 [==============================] - 522s 237ms/step - loss: 0.6742 - acc: 0.6000 - val_loss: 0.6726 - val_acc: 0.6000\n",
      "Epoch 4/32\n",
      "2200/2200 [==============================] - 523s 238ms/step - loss: 0.6698 - acc: 0.6008 - val_loss: 0.6645 - val_acc: 0.5997\n",
      "Epoch 5/32\n",
      "2200/2200 [==============================] - 517s 235ms/step - loss: 0.6579 - acc: 0.6038 - val_loss: 0.6537 - val_acc: 0.6086\n",
      "Epoch 6/32\n",
      "2200/2200 [==============================] - 523s 238ms/step - loss: 0.6484 - acc: 0.6226 - val_loss: 0.6462 - val_acc: 0.6235\n",
      "Epoch 7/32\n",
      "2200/2200 [==============================] - 524s 238ms/step - loss: 0.6469 - acc: 0.6229 - val_loss: 0.6518 - val_acc: 0.6153\n",
      "Epoch 8/32\n",
      "2200/2200 [==============================] - 526s 239ms/step - loss: 0.6398 - acc: 0.6303 - val_loss: 0.6527 - val_acc: 0.6215\n",
      "Epoch 9/32\n",
      "2200/2200 [==============================] - 525s 239ms/step - loss: 0.6422 - acc: 0.6273 - val_loss: 0.6486 - val_acc: 0.6179\n",
      "Epoch 10/32\n",
      "2200/2200 [==============================] - 529s 240ms/step - loss: 0.6290 - acc: 0.6447 - val_loss: 0.6480 - val_acc: 0.6230\n",
      "Epoch 11/32\n",
      "2200/2200 [==============================] - 523s 238ms/step - loss: 0.6266 - acc: 0.6503 - val_loss: 0.6468 - val_acc: 0.6218\n",
      "Epoch 12/32\n",
      "2200/2200 [==============================] - 523s 238ms/step - loss: 0.6432 - acc: 0.6202 - val_loss: 0.6547 - val_acc: 0.6091\n",
      "Epoch 13/32\n",
      "2200/2200 [==============================] - 525s 239ms/step - loss: 0.6317 - acc: 0.6400 - val_loss: 0.6700 - val_acc: 0.6080\n",
      "Epoch 14/32\n",
      "2200/2200 [==============================] - 524s 238ms/step - loss: 0.6462 - acc: 0.6218 - val_loss: 0.6516 - val_acc: 0.6124\n",
      "Epoch 15/32\n",
      "2200/2200 [==============================] - 527s 239ms/step - loss: 0.6290 - acc: 0.6423 - val_loss: 0.6472 - val_acc: 0.6217\n",
      "Epoch 16/32\n",
      "2200/2200 [==============================] - 530s 241ms/step - loss: 0.6251 - acc: 0.6470 - val_loss: 0.6509 - val_acc: 0.6224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/32\n",
      "2200/2200 [==============================] - 529s 240ms/step - loss: 0.6272 - acc: 0.6453 - val_loss: 0.6499 - val_acc: 0.6156\n",
      "Epoch 18/32\n",
      "2200/2200 [==============================] - 519s 236ms/step - loss: 0.6240 - acc: 0.6456 - val_loss: 0.6518 - val_acc: 0.6191\n",
      "Epoch 19/32\n",
      "2200/2200 [==============================] - 525s 239ms/step - loss: 0.6237 - acc: 0.6491 - val_loss: 0.6562 - val_acc: 0.6124\n",
      "Epoch 20/32\n",
      "2200/2200 [==============================] - 524s 238ms/step - loss: 0.6262 - acc: 0.6338 - val_loss: 0.6568 - val_acc: 0.6153\n",
      "Epoch 21/32\n",
      "2200/2200 [==============================] - 521s 237ms/step - loss: 0.6261 - acc: 0.6432 - val_loss: 0.6504 - val_acc: 0.6233\n",
      "Epoch 22/32\n",
      "2200/2200 [==============================] - 521s 237ms/step - loss: 0.6234 - acc: 0.6514 - val_loss: 0.6456 - val_acc: 0.6232\n",
      "Epoch 23/32\n",
      "2200/2200 [==============================] - 518s 235ms/step - loss: 0.6297 - acc: 0.6444 - val_loss: 0.6461 - val_acc: 0.6208\n",
      "Epoch 24/32\n",
      "2200/2200 [==============================] - 550s 250ms/step - loss: 0.6223 - acc: 0.6459 - val_loss: 0.6534 - val_acc: 0.6088\n",
      "Epoch 25/32\n",
      "2200/2200 [==============================] - 521s 237ms/step - loss: 0.6144 - acc: 0.6555 - val_loss: 0.6512 - val_acc: 0.6198\n",
      "Epoch 26/32\n",
      "2200/2200 [==============================] - 520s 237ms/step - loss: 0.6126 - acc: 0.6580 - val_loss: 0.6580 - val_acc: 0.6161\n",
      "Epoch 27/32\n",
      "2200/2200 [==============================] - 521s 237ms/step - loss: 0.6183 - acc: 0.6495 - val_loss: 0.6521 - val_acc: 0.6194\n",
      "Epoch 28/32\n",
      "2200/2200 [==============================] - 509s 231ms/step - loss: 0.6181 - acc: 0.6465 - val_loss: 0.6504 - val_acc: 0.6208\n",
      "Epoch 29/32\n",
      "2200/2200 [==============================] - 495s 225ms/step - loss: 0.6148 - acc: 0.6488 - val_loss: 0.6483 - val_acc: 0.6227\n",
      "Epoch 30/32\n",
      "2200/2200 [==============================] - 494s 225ms/step - loss: 0.6516 - acc: 0.6167 - val_loss: 0.6654 - val_acc: 0.5997\n",
      "Epoch 31/32\n",
      "2200/2200 [==============================] - 496s 225ms/step - loss: 0.6453 - acc: 0.6236 - val_loss: 0.6541 - val_acc: 0.6156\n",
      "Epoch 32/32\n",
      "2200/2200 [==============================] - 491s 223ms/step - loss: 0.6300 - acc: 0.6402 - val_loss: 0.6567 - val_acc: 0.6120\n",
      "Generate Model - Ini: 2020-03-11 21:14:07\tEnd: 2020-03-12 01:52:17\tTotal: 4:38:09.648901\n",
      "Total experiment - Ini: 2020-03-11 21:14:07\tEnd: 2020-03-12 01:52:17\tTotal: 4:38:09.649495\n",
      "Loading data... /home/vanessa/PycharmProjects/RecurrentNetworks/tokenizers/anx_dep_multilabel/SMHD_TR_2640_VS_5000_TF_0_DF_0_RSW_F_IT_1_RP_F\n",
      "Preprocess data... /home/vanessa/PycharmProjects/RecurrentNetworks/dataset/anx_dep_multilabel/SMHD_test_2640.df\n",
      "Load data - Ini: 2020-03-12 01:52:17\tEnd: 2020-03-12 01:52:28\tTotal: 0:00:11.575014\n",
      "2200/2200 [==============================] - 74s 34ms/step\n",
      "Loading data... /home/vanessa/PycharmProjects/RecurrentNetworks/tokenizers/anx_dep_multilabel/SMHD_TR_2640_VS_5000_TF_0_DF_0_RSW_F_IT_1_RP_F\n",
      "Preprocess data... /home/vanessa/PycharmProjects/RecurrentNetworks/dataset/anx_dep_multilabel/SMHD_train_2640.df\n",
      "Load Pre-train embeddings\n",
      "Load data - Ini: 2020-03-12 01:54:01\tEnd: 2020-03-12 01:54:26\tTotal: 0:00:24.071185\n",
      "Training using single GPU or CPU..\n",
      "Train on 2200 samples, validate on 2200 samples\n",
      "Epoch 1/32\n",
      "2200/2200 [==============================] - 555s 252ms/step - loss: 0.6817 - acc: 0.5812 - val_loss: 0.6745 - val_acc: 0.6000\n",
      "Epoch 2/32\n",
      "2200/2200 [==============================] - 536s 244ms/step - loss: 0.6746 - acc: 0.6000 - val_loss: 0.6725 - val_acc: 0.6000\n",
      "Epoch 3/32\n",
      "2200/2200 [==============================] - 524s 238ms/step - loss: 0.6716 - acc: 0.6000 - val_loss: 0.6706 - val_acc: 0.6000\n",
      "Epoch 4/32\n",
      "2200/2200 [==============================] - 519s 236ms/step - loss: 0.6658 - acc: 0.6036 - val_loss: 0.6668 - val_acc: 0.6017\n",
      "Epoch 5/32\n",
      "2200/2200 [==============================] - 516s 235ms/step - loss: 0.6662 - acc: 0.6012 - val_loss: 0.6674 - val_acc: 0.6000\n",
      "Epoch 6/32\n",
      "2200/2200 [==============================] - 517s 235ms/step - loss: 0.6566 - acc: 0.6064 - val_loss: 0.6647 - val_acc: 0.5995\n",
      "Epoch 7/32\n",
      "2200/2200 [==============================] - 512s 233ms/step - loss: 0.6444 - acc: 0.6256 - val_loss: 0.6674 - val_acc: 0.5912\n",
      "Epoch 8/32\n",
      "2200/2200 [==============================] - 515s 234ms/step - loss: 0.6482 - acc: 0.6230 - val_loss: 0.6648 - val_acc: 0.6023\n",
      "Epoch 9/32\n",
      "2200/2200 [==============================] - 508s 231ms/step - loss: 0.6512 - acc: 0.6168 - val_loss: 0.6588 - val_acc: 0.5947\n",
      "Epoch 10/32\n",
      "2200/2200 [==============================] - 508s 231ms/step - loss: 0.6612 - acc: 0.6026 - val_loss: 0.6612 - val_acc: 0.6042\n",
      "Epoch 11/32\n",
      "2200/2200 [==============================] - 508s 231ms/step - loss: 0.6433 - acc: 0.6236 - val_loss: 0.6611 - val_acc: 0.6020\n",
      "Epoch 12/32\n",
      "2200/2200 [==============================] - 508s 231ms/step - loss: 0.6358 - acc: 0.6345 - val_loss: 0.6653 - val_acc: 0.5998\n",
      "Epoch 13/32\n",
      "2200/2200 [==============================] - 512s 233ms/step - loss: 0.6392 - acc: 0.6276 - val_loss: 0.6653 - val_acc: 0.5938\n",
      "Epoch 14/32\n",
      "2200/2200 [==============================] - 521s 237ms/step - loss: 0.6345 - acc: 0.6348 - val_loss: 0.6641 - val_acc: 0.6015\n",
      "Epoch 15/32\n",
      "2200/2200 [==============================] - 521s 237ms/step - loss: 0.6338 - acc: 0.6359 - val_loss: 0.6685 - val_acc: 0.5968\n",
      "Epoch 16/32\n",
      "2200/2200 [==============================] - 522s 237ms/step - loss: 0.6323 - acc: 0.6338 - val_loss: 0.6638 - val_acc: 0.5980\n",
      "Epoch 17/32\n",
      "2200/2200 [==============================] - 522s 237ms/step - loss: 0.6281 - acc: 0.6439 - val_loss: 0.6637 - val_acc: 0.5967\n",
      "Epoch 18/32\n",
      "2200/2200 [==============================] - 522s 237ms/step - loss: 0.6275 - acc: 0.6477 - val_loss: 0.6645 - val_acc: 0.5986\n",
      "Epoch 19/32\n",
      "2200/2200 [==============================] - 522s 237ms/step - loss: 0.6240 - acc: 0.6445 - val_loss: 0.6658 - val_acc: 0.5985\n",
      "Epoch 20/32\n",
      "2200/2200 [==============================] - 521s 237ms/step - loss: 0.6253 - acc: 0.6465 - val_loss: 0.6613 - val_acc: 0.6030\n",
      "Epoch 21/32\n",
      "2200/2200 [==============================] - 519s 236ms/step - loss: 0.6192 - acc: 0.6582 - val_loss: 0.6640 - val_acc: 0.6023\n",
      "Epoch 22/32\n",
      "2200/2200 [==============================] - 521s 237ms/step - loss: 0.6187 - acc: 0.6570 - val_loss: 0.6679 - val_acc: 0.5983\n",
      "Epoch 23/32\n",
      "2200/2200 [==============================] - 521s 237ms/step - loss: 0.6203 - acc: 0.6553 - val_loss: 0.6659 - val_acc: 0.5994\n",
      "Epoch 24/32\n",
      "2200/2200 [==============================] - 521s 237ms/step - loss: 0.6170 - acc: 0.6576 - val_loss: 0.6636 - val_acc: 0.6008\n",
      "Epoch 25/32\n",
      "2200/2200 [==============================] - 520s 236ms/step - loss: 0.6196 - acc: 0.6542 - val_loss: 0.6628 - val_acc: 0.6136\n",
      "Epoch 00025: early stopping\n",
      "Generate Model - Ini: 2020-03-12 01:54:37\tEnd: 2020-03-12 05:31:19\tTotal: 3:36:42.225885\n",
      "Total experiment - Ini: 2020-03-12 01:54:26\tEnd: 2020-03-12 05:31:19\tTotal: 3:36:52.664750\n",
      "Loading data... /home/vanessa/PycharmProjects/RecurrentNetworks/tokenizers/anx_dep_multilabel/SMHD_TR_2640_VS_5000_TF_0_DF_0_RSW_F_IT_1_RP_F\n",
      "Preprocess data... /home/vanessa/PycharmProjects/RecurrentNetworks/dataset/anx_dep_multilabel/SMHD_test_2640.df\n",
      "Load data - Ini: 2020-03-12 05:31:19\tEnd: 2020-03-12 05:31:30\tTotal: 0:00:11.270742\n",
      "2200/2200 [==============================] - 66s 30ms/step\n",
      "Loading data... /home/vanessa/PycharmProjects/RecurrentNetworks/tokenizers/anx_dep_multilabel/SMHD_TR_2640_VS_5000_TF_0_DF_0_RSW_F_IT_1_RP_F\n",
      "Preprocess data... /home/vanessa/PycharmProjects/RecurrentNetworks/dataset/anx_dep_multilabel/SMHD_train_2640.df\n",
      "Load data - Ini: 2020-03-12 05:32:57\tEnd: 2020-03-12 05:33:21\tTotal: 0:00:23.775851\n",
      "Training using single GPU or CPU..\n",
      "Train on 2200 samples, validate on 2200 samples\n",
      "Epoch 1/32\n",
      "2200/2200 [==============================] - 555s 252ms/step - loss: 0.6803 - acc: 0.5941 - val_loss: 0.6730 - val_acc: 0.6000\n",
      "Epoch 2/32\n",
      "2200/2200 [==============================] - 522s 237ms/step - loss: 0.6744 - acc: 0.6000 - val_loss: 0.6729 - val_acc: 0.6000\n",
      "Epoch 3/32\n",
      "2200/2200 [==============================] - 521s 237ms/step - loss: 0.6742 - acc: 0.6000 - val_loss: 0.6726 - val_acc: 0.6000\n",
      "Epoch 4/32\n",
      "2200/2200 [==============================] - 519s 236ms/step - loss: 0.6704 - acc: 0.6002 - val_loss: 0.6680 - val_acc: 0.6000\n",
      "Epoch 5/32\n",
      "2200/2200 [==============================] - 515s 234ms/step - loss: 0.6695 - acc: 0.5880 - val_loss: 0.6864 - val_acc: 0.6000\n",
      "Epoch 6/32\n",
      "2200/2200 [==============================] - 512s 233ms/step - loss: 0.6733 - acc: 0.5988 - val_loss: 0.6684 - val_acc: 0.6000\n",
      "Epoch 7/32\n",
      "2200/2200 [==============================] - 513s 233ms/step - loss: 0.6703 - acc: 0.5980 - val_loss: 0.6600 - val_acc: 0.6000\n",
      "Epoch 8/32\n",
      "2200/2200 [==============================] - 512s 233ms/step - loss: 0.6669 - acc: 0.5992 - val_loss: 0.6544 - val_acc: 0.6248\n",
      "Epoch 9/32\n",
      "2200/2200 [==============================] - 513s 233ms/step - loss: 0.6635 - acc: 0.6038 - val_loss: 0.6635 - val_acc: 0.6165\n",
      "Epoch 10/32\n",
      "2200/2200 [==============================] - 513s 233ms/step - loss: 0.6735 - acc: 0.5912 - val_loss: 0.6724 - val_acc: 0.6000\n",
      "Epoch 11/32\n",
      "2200/2200 [==============================] - 513s 233ms/step - loss: 0.6715 - acc: 0.5995 - val_loss: 0.6715 - val_acc: 0.6000\n",
      "Epoch 12/32\n",
      "2200/2200 [==============================] - 514s 233ms/step - loss: 0.6724 - acc: 0.6006 - val_loss: 0.6666 - val_acc: 0.6000\n",
      "Epoch 13/32\n",
      "2200/2200 [==============================] - 512s 233ms/step - loss: 0.6708 - acc: 0.5995 - val_loss: 0.6656 - val_acc: 0.6000\n",
      "Epoch 14/32\n",
      "2200/2200 [==============================] - 513s 233ms/step - loss: 0.6677 - acc: 0.5995 - val_loss: 0.6534 - val_acc: 0.6000\n",
      "Epoch 15/32\n",
      "2200/2200 [==============================] - 514s 233ms/step - loss: 0.6686 - acc: 0.5986 - val_loss: 0.6533 - val_acc: 0.6000\n",
      "Epoch 16/32\n",
      "2200/2200 [==============================] - 514s 234ms/step - loss: 0.6665 - acc: 0.5995 - val_loss: 0.6511 - val_acc: 0.6000\n",
      "Epoch 17/32\n",
      "2200/2200 [==============================] - 513s 233ms/step - loss: 0.6669 - acc: 0.5980 - val_loss: 0.6511 - val_acc: 0.6000\n",
      "Epoch 18/32\n",
      "2200/2200 [==============================] - 512s 233ms/step - loss: 0.6644 - acc: 0.5980 - val_loss: 0.6550 - val_acc: 0.6000\n",
      "Epoch 19/32\n",
      "2200/2200 [==============================] - 513s 233ms/step - loss: 0.6642 - acc: 0.6012 - val_loss: 0.6612 - val_acc: 0.6194\n",
      "Epoch 20/32\n",
      "2200/2200 [==============================] - 511s 232ms/step - loss: 0.6660 - acc: 0.5959 - val_loss: 0.6489 - val_acc: 0.6000\n",
      "Epoch 21/32\n",
      "2200/2200 [==============================] - 509s 231ms/step - loss: 0.6636 - acc: 0.6017 - val_loss: 0.6774 - val_acc: 0.6012\n",
      "Epoch 22/32\n",
      "2200/2200 [==============================] - 510s 232ms/step - loss: 0.6688 - acc: 0.6055 - val_loss: 0.6727 - val_acc: 0.6006\n",
      "Epoch 23/32\n",
      "2200/2200 [==============================] - 510s 232ms/step - loss: 0.6618 - acc: 0.6129 - val_loss: 0.6633 - val_acc: 0.6138\n",
      "Epoch 24/32\n",
      "2200/2200 [==============================] - 511s 232ms/step - loss: 0.6695 - acc: 0.5933 - val_loss: 0.6649 - val_acc: 0.5992\n",
      "Epoch 25/32\n",
      "2200/2200 [==============================] - 511s 232ms/step - loss: 0.6652 - acc: 0.5970 - val_loss: 0.6603 - val_acc: 0.6000\n",
      "Epoch 26/32\n",
      "2200/2200 [==============================] - 510s 232ms/step - loss: 0.6709 - acc: 0.5989 - val_loss: 0.6755 - val_acc: 0.6000\n",
      "Epoch 27/32\n",
      "2200/2200 [==============================] - 510s 232ms/step - loss: 0.6667 - acc: 0.6048 - val_loss: 0.6720 - val_acc: 0.6029\n",
      "Epoch 28/32\n",
      "2200/2200 [==============================] - 511s 232ms/step - loss: 0.6631 - acc: 0.6112 - val_loss: 0.6737 - val_acc: 0.6021\n",
      "Epoch 29/32\n",
      "2200/2200 [==============================] - 509s 231ms/step - loss: 0.6634 - acc: 0.6089 - val_loss: 0.6694 - val_acc: 0.6067\n",
      "Epoch 30/32\n",
      "2200/2200 [==============================] - 504s 229ms/step - loss: 0.6607 - acc: 0.6118 - val_loss: 0.6507 - val_acc: 0.6109\n",
      "Epoch 31/32\n",
      "2200/2200 [==============================] - 503s 229ms/step - loss: 0.6557 - acc: 0.6132 - val_loss: 0.6495 - val_acc: 0.6156\n",
      "Epoch 32/32\n",
      "2200/2200 [==============================] - 505s 229ms/step - loss: 0.6494 - acc: 0.6173 - val_loss: 0.6407 - val_acc: 0.6279\n",
      "Generate Model - Ini: 2020-03-12 05:33:22\tEnd: 2020-03-12 10:07:28\tTotal: 4:34:06.033418\n",
      "Total experiment - Ini: 2020-03-12 05:33:22\tEnd: 2020-03-12 10:07:28\tTotal: 4:34:06.034086\n",
      "Loading data... /home/vanessa/PycharmProjects/RecurrentNetworks/tokenizers/anx_dep_multilabel/SMHD_TR_2640_VS_5000_TF_0_DF_0_RSW_F_IT_1_RP_F\n",
      "Preprocess data... /home/vanessa/PycharmProjects/RecurrentNetworks/dataset/anx_dep_multilabel/SMHD_test_2640.df\n",
      "Load data - Ini: 2020-03-12 10:07:28\tEnd: 2020-03-12 10:07:40\tTotal: 0:00:11.684782\n",
      "2200/2200 [==============================] - 69s 31ms/step\n",
      "Loading data... /home/vanessa/PycharmProjects/RecurrentNetworks/tokenizers/anx_dep_multilabel/SMHD_TR_2640_VS_5000_TF_0_DF_0_RSW_F_IT_1_RP_F\n",
      "Preprocess data... /home/vanessa/PycharmProjects/RecurrentNetworks/dataset/anx_dep_multilabel/SMHD_train_2640.df\n",
      "Load Pre-train embeddings\n",
      "Found 688071 word vectors.\n",
      "Load data - Ini: 2020-03-12 10:09:12\tEnd: 2020-03-12 10:09:43\tTotal: 0:00:31.219673\n",
      "Training using single GPU or CPU..\n",
      "Train on 2200 samples, validate on 2200 samples\n",
      "Epoch 1/32\n",
      "2200/2200 [==============================] - 547s 249ms/step - loss: 0.6790 - acc: 0.5973 - val_loss: 0.6732 - val_acc: 0.6000\n",
      "Epoch 2/32\n",
      "2200/2200 [==============================] - 519s 236ms/step - loss: 0.6740 - acc: 0.5998 - val_loss: 0.6726 - val_acc: 0.6000\n",
      "Epoch 3/32\n",
      "2200/2200 [==============================] - 505s 230ms/step - loss: 0.6725 - acc: 0.6000 - val_loss: 0.6713 - val_acc: 0.6000\n",
      "Epoch 4/32\n",
      "2200/2200 [==============================] - 494s 225ms/step - loss: 0.6686 - acc: 0.6002 - val_loss: 0.6664 - val_acc: 0.5961\n",
      "Epoch 5/32\n",
      "2200/2200 [==============================] - 496s 225ms/step - loss: 0.6637 - acc: 0.5965 - val_loss: 0.6638 - val_acc: 0.6017\n",
      "Epoch 6/32\n",
      "2200/2200 [==============================] - 495s 225ms/step - loss: 0.6593 - acc: 0.6088 - val_loss: 0.6591 - val_acc: 0.6059\n",
      "Epoch 7/32\n",
      "2200/2200 [==============================] - 495s 225ms/step - loss: 0.6506 - acc: 0.6132 - val_loss: 0.6727 - val_acc: 0.5885\n",
      "Epoch 8/32\n",
      "2200/2200 [==============================] - 496s 225ms/step - loss: 0.6592 - acc: 0.6006 - val_loss: 0.6591 - val_acc: 0.6048\n",
      "Epoch 9/32\n",
      "2200/2200 [==============================] - 495s 225ms/step - loss: 0.6511 - acc: 0.6088 - val_loss: 0.6592 - val_acc: 0.5986\n",
      "Epoch 10/32\n",
      "2200/2200 [==============================] - 495s 225ms/step - loss: 0.6769 - acc: 0.5950 - val_loss: 0.6731 - val_acc: 0.6000\n",
      "Epoch 11/32\n",
      "2200/2200 [==============================] - 496s 225ms/step - loss: 0.6734 - acc: 0.6000 - val_loss: 0.6730 - val_acc: 0.6000\n",
      "Epoch 12/32\n",
      "2200/2200 [==============================] - 497s 226ms/step - loss: 0.6730 - acc: 0.6002 - val_loss: 0.6730 - val_acc: 0.6000\n",
      "Epoch 13/32\n",
      "2200/2200 [==============================] - 496s 225ms/step - loss: 0.6723 - acc: 0.6000 - val_loss: 0.6728 - val_acc: 0.6000\n",
      "Epoch 14/32\n",
      "2200/2200 [==============================] - 496s 225ms/step - loss: 0.6727 - acc: 0.5997 - val_loss: 0.6727 - val_acc: 0.6000\n",
      "Epoch 15/32\n",
      "2200/2200 [==============================] - 496s 226ms/step - loss: 0.6719 - acc: 0.6002 - val_loss: 0.6726 - val_acc: 0.5998\n",
      "Epoch 16/32\n",
      "2200/2200 [==============================] - 497s 226ms/step - loss: 0.6705 - acc: 0.6005 - val_loss: 0.6719 - val_acc: 0.5998\n",
      "Epoch 17/32\n",
      "2200/2200 [==============================] - 499s 227ms/step - loss: 0.6688 - acc: 0.6021 - val_loss: 0.6660 - val_acc: 0.6026\n",
      "Epoch 18/32\n",
      "2200/2200 [==============================] - 497s 226ms/step - loss: 0.6625 - acc: 0.6076 - val_loss: 0.6611 - val_acc: 0.6083\n",
      "Epoch 19/32\n",
      "2200/2200 [==============================] - 496s 225ms/step - loss: 0.6598 - acc: 0.6056 - val_loss: 0.6629 - val_acc: 0.5891\n",
      "Epoch 20/32\n",
      "2200/2200 [==============================] - 497s 226ms/step - loss: 0.6597 - acc: 0.5992 - val_loss: 0.6633 - val_acc: 0.6038\n",
      "Epoch 21/32\n",
      "2200/2200 [==============================] - 496s 225ms/step - loss: 0.6685 - acc: 0.6018 - val_loss: 0.6651 - val_acc: 0.6058\n",
      "Epoch 22/32\n",
      "2200/2200 [==============================] - 498s 226ms/step - loss: 0.6604 - acc: 0.6073 - val_loss: 0.6556 - val_acc: 0.6123\n",
      "Epoch 23/32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2200/2200 [==============================] - 497s 226ms/step - loss: 0.6631 - acc: 0.6027 - val_loss: 0.6685 - val_acc: 0.6008\n",
      "Epoch 24/32\n",
      "2200/2200 [==============================] - 498s 226ms/step - loss: 0.6636 - acc: 0.6011 - val_loss: 0.6691 - val_acc: 0.6033\n",
      "Epoch 25/32\n",
      "2200/2200 [==============================] - 497s 226ms/step - loss: 0.6619 - acc: 0.6073 - val_loss: 0.6574 - val_acc: 0.6111\n",
      "Epoch 26/32\n",
      "2200/2200 [==============================] - 496s 225ms/step - loss: 0.6565 - acc: 0.6083 - val_loss: 0.6422 - val_acc: 0.6289\n",
      "Epoch 27/32\n",
      "2200/2200 [==============================] - 497s 226ms/step - loss: 0.6575 - acc: 0.6068 - val_loss: 0.6380 - val_acc: 0.6344\n",
      "Epoch 28/32\n",
      "2200/2200 [==============================] - 497s 226ms/step - loss: 0.6539 - acc: 0.6114 - val_loss: 0.6533 - val_acc: 0.6155\n",
      "Epoch 29/32\n",
      "2200/2200 [==============================] - 498s 226ms/step - loss: 0.6541 - acc: 0.6092 - val_loss: 0.6624 - val_acc: 0.6098\n",
      "Epoch 30/32\n",
      "2200/2200 [==============================] - 497s 226ms/step - loss: 0.6508 - acc: 0.6129 - val_loss: 0.6429 - val_acc: 0.6202\n",
      "Epoch 31/32\n",
      "2200/2200 [==============================] - 496s 225ms/step - loss: 0.6521 - acc: 0.6098 - val_loss: 0.6496 - val_acc: 0.6145\n",
      "Epoch 32/32\n",
      "2200/2200 [==============================] - 497s 226ms/step - loss: 0.6524 - acc: 0.6117 - val_loss: 0.6470 - val_acc: 0.6141\n",
      "Generate Model - Ini: 2020-03-12 10:09:56\tEnd: 2020-03-12 14:36:14\tTotal: 4:26:17.949636\n",
      "Total experiment - Ini: 2020-03-12 10:09:44\tEnd: 2020-03-12 14:36:14\tTotal: 4:26:30.194739\n",
      "Loading data... /home/vanessa/PycharmProjects/RecurrentNetworks/tokenizers/anx_dep_multilabel/SMHD_TR_2640_VS_5000_TF_0_DF_0_RSW_F_IT_1_RP_F\n",
      "Preprocess data... /home/vanessa/PycharmProjects/RecurrentNetworks/dataset/anx_dep_multilabel/SMHD_test_2640.df\n",
      "Load data - Ini: 2020-03-12 14:36:14\tEnd: 2020-03-12 14:36:27\tTotal: 0:00:13.176678\n",
      "2200/2200 [==============================] - 72s 33ms/step\n",
      "Loading data... /home/vanessa/PycharmProjects/RecurrentNetworks/tokenizers/anx_dep_multilabel/SMHD_TR_2640_VS_5000_TF_0_DF_0_RSW_F_IT_1_RP_F\n",
      "Preprocess data... /home/vanessa/PycharmProjects/RecurrentNetworks/dataset/anx_dep_multilabel/SMHD_train_2640.df\n",
      "Load data - Ini: 2020-03-12 14:38:05\tEnd: 2020-03-12 14:38:30\tTotal: 0:00:25.123471\n",
      "Training using single GPU or CPU..\n",
      "Train on 2200 samples, validate on 2200 samples\n",
      "Epoch 1/32\n",
      "2200/2200 [==============================] - 552s 251ms/step - loss: 0.6802 - acc: 0.5941 - val_loss: 0.6730 - val_acc: 0.6000\n",
      "Epoch 2/32\n",
      "2200/2200 [==============================] - 528s 240ms/step - loss: 0.6744 - acc: 0.6000 - val_loss: 0.6729 - val_acc: 0.6000\n",
      "Epoch 3/32\n",
      "2200/2200 [==============================] - 520s 236ms/step - loss: 0.6742 - acc: 0.6000 - val_loss: 0.6726 - val_acc: 0.6000\n",
      "Epoch 4/32\n",
      "2200/2200 [==============================] - 514s 234ms/step - loss: 0.6697 - acc: 0.6005 - val_loss: 0.6633 - val_acc: 0.6018\n",
      "Epoch 5/32\n",
      "2200/2200 [==============================] - 516s 234ms/step - loss: 0.6655 - acc: 0.6005 - val_loss: 0.6576 - val_acc: 0.6062\n",
      "Epoch 6/32\n",
      "2200/2200 [==============================] - 516s 235ms/step - loss: 0.6573 - acc: 0.6059 - val_loss: 0.6442 - val_acc: 0.6180\n",
      "Epoch 7/32\n",
      "2200/2200 [==============================] - 515s 234ms/step - loss: 0.6482 - acc: 0.6220 - val_loss: 0.6462 - val_acc: 0.6277\n",
      "Epoch 8/32\n",
      "2200/2200 [==============================] - 515s 234ms/step - loss: 0.6612 - acc: 0.5989 - val_loss: 0.6425 - val_acc: 0.6162\n",
      "Epoch 9/32\n",
      "2200/2200 [==============================] - 513s 233ms/step - loss: 0.6502 - acc: 0.6164 - val_loss: 0.6530 - val_acc: 0.6144\n",
      "Epoch 10/32\n",
      "2200/2200 [==============================] - 512s 233ms/step - loss: 0.6353 - acc: 0.6380 - val_loss: 0.6511 - val_acc: 0.6136\n",
      "Epoch 11/32\n",
      "2200/2200 [==============================] - 514s 234ms/step - loss: 0.6329 - acc: 0.6364 - val_loss: 0.6467 - val_acc: 0.6156\n",
      "Epoch 12/32\n",
      "2200/2200 [==============================] - 514s 234ms/step - loss: 0.6337 - acc: 0.6355 - val_loss: 0.6480 - val_acc: 0.6205\n",
      "Epoch 13/32\n",
      "2200/2200 [==============================] - 513s 233ms/step - loss: 0.6681 - acc: 0.6011 - val_loss: 0.6755 - val_acc: 0.6002\n",
      "Epoch 14/32\n",
      "2200/2200 [==============================] - 513s 233ms/step - loss: 0.6730 - acc: 0.6018 - val_loss: 0.6733 - val_acc: 0.6002\n",
      "Epoch 15/32\n",
      "2200/2200 [==============================] - 512s 233ms/step - loss: 0.6741 - acc: 0.6006 - val_loss: 0.6730 - val_acc: 0.6002\n",
      "Epoch 16/32\n",
      "2200/2200 [==============================] - 512s 233ms/step - loss: 0.6725 - acc: 0.6006 - val_loss: 0.6731 - val_acc: 0.6002\n",
      "Epoch 17/32\n",
      "2200/2200 [==============================] - 513s 233ms/step - loss: 0.6713 - acc: 0.6014 - val_loss: 0.6732 - val_acc: 0.6002\n",
      "Epoch 18/32\n",
      "2200/2200 [==============================] - 516s 235ms/step - loss: 0.6714 - acc: 0.6024 - val_loss: 0.6730 - val_acc: 0.6002\n",
      "Epoch 19/32\n",
      "2200/2200 [==============================] - 514s 234ms/step - loss: 0.6721 - acc: 0.6012 - val_loss: 0.6730 - val_acc: 0.6002\n",
      "Epoch 20/32\n",
      "2200/2200 [==============================] - 514s 233ms/step - loss: 0.6713 - acc: 0.6021 - val_loss: 0.6730 - val_acc: 0.6002\n",
      "Epoch 21/32\n",
      "2200/2200 [==============================] - 513s 233ms/step - loss: 0.6722 - acc: 0.6012 - val_loss: 0.6729 - val_acc: 0.6002\n",
      "Epoch 22/32\n",
      "2200/2200 [==============================] - 514s 234ms/step - loss: 0.6714 - acc: 0.6017 - val_loss: 0.6730 - val_acc: 0.6002\n",
      "Epoch 23/32\n",
      "2200/2200 [==============================] - 514s 234ms/step - loss: 0.6709 - acc: 0.6027 - val_loss: 0.6732 - val_acc: 0.6002\n",
      "Epoch 24/32\n",
      "2200/2200 [==============================] - 512s 233ms/step - loss: 0.6740 - acc: 0.5980 - val_loss: 0.6729 - val_acc: 0.6002\n",
      "Epoch 00024: early stopping\n",
      "Generate Model - Ini: 2020-03-12 14:38:30\tEnd: 2020-03-12 18:05:23\tTotal: 3:26:52.463679\n",
      "Total experiment - Ini: 2020-03-12 14:38:30\tEnd: 2020-03-12 18:05:23\tTotal: 3:26:52.464258\n",
      "Loading data... /home/vanessa/PycharmProjects/RecurrentNetworks/tokenizers/anx_dep_multilabel/SMHD_TR_2640_VS_5000_TF_0_DF_0_RSW_F_IT_1_RP_F\n",
      "Preprocess data... /home/vanessa/PycharmProjects/RecurrentNetworks/dataset/anx_dep_multilabel/SMHD_test_2640.df\n",
      "Load data - Ini: 2020-03-12 18:05:23\tEnd: 2020-03-12 18:05:35\tTotal: 0:00:11.801163\n",
      "2200/2200 [==============================] - 71s 32ms/step\n"
     ]
    }
   ],
   "source": [
    "print('Initializer experiment 2.3 (model SMHD_ml_gl_2640)\\n'+\\\n",
    "      'Set: kernel_initializer=glorot_uniform=xavier_uniform, dataset=SMHD_2640 multi-label')\n",
    "\n",
    "exp = ExperimentProcesses('lstm_exp9_var_L3')\n",
    "exp.pp_data.set_dataset_source(dataset_name='SMHD', label_set=['control', 'anxiety', 'depression'],\n",
    "                               total_registers=2640, subdirectory=\"anx_dep_multilabel\")\n",
    "\n",
    "use_embeddings = [dn.UseEmbedding.STATIC, dn.UseEmbedding.NON_STATIC]\n",
    "kernel_functions = ['lecun_uniform', 'glorot_uniform']\n",
    "for kernel_function in kernel_functions:\n",
    "    for use_embedding in use_embeddings:\n",
    "        exp.pp_data.embedding_type = dn.EmbeddingType.GLOVE_6B\n",
    "        exp.pp_data.use_embedding = use_embedding\n",
    "        exp.pp_data.word_embedding_custom_file = ''\n",
    "        exp.pp_data.load_dataset_type = dn.LoadDataset.TRAIN_DATA_MODEL\n",
    "\n",
    "        we_file_name = 'ET_' + str(embedding_type.value) + '_UE_' + str(use_embedding.value) +\\\n",
    "                       '_EF_' + word_embedding_custom_file.split('.')[0] + '_glove6B300d_' + kernel_function\n",
    "\n",
    "        generate_model(exp, 'exp9_' + we_file_name[0:13] + we_file_name[18:30] + '_2640', we_file_name, \n",
    "                       kernel_function)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
