{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include project path to available custom class at jupyter\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath('/home/stacked_ensemble/'))\n",
    "\n",
    "# Disable warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras.backend.tensorflow_backend as K\n",
    "import shap\n",
    "import datetime\n",
    "import re\n",
    "import copy\n",
    "K.set_session\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import utils.definition_network as dn\n",
    "import pickle\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from network_model.custom_ensemble import CustomEnsemble\n",
    "from network_model.stacked_ensemble import StackedEnsemble\n",
    "from utils.preprocess_data import PreprocessData\n",
    "from utils.shap_analyse_plots import ShapAnalysePlots\n",
    "from utils.con_postgres import ConPostgres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Helper functions and objects available for all tests in this experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_period_time_end(time_ini, task_desc):\n",
    "    time_end = datetime.datetime.now()\n",
    "    period = time_end - time_ini\n",
    "    print('%s\\t Ini: %s  End: %s  Total: %s' % (task_desc, time_ini.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                                                time_end.strftime(\"%Y-%m-%d %H:%M:%S\"), period))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_samples_by_total_class(data_df, labels_set, total_sample_by_label):\n",
    "    samples = []\n",
    "\n",
    "    for label in labels_set:\n",
    "        samples.append(data_df[data_df.label == label][0:total_sample_by_label])\n",
    "\n",
    "    return pd.concat(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. SHAP stacked ensemble analysis (level 1: deep learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Auxiliary Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_params_stacked_ensemble(dataset_train_path, dataset_test_path, load_cad_submodels):\n",
    "    epoch = 16\n",
    "    batch_size = 8\n",
    "    use_submodel = dict({SUBMODEL: [1,2,3]})\n",
    "    neurons_by_submodel = 12\n",
    "    hidden_layer = 3\n",
    "\n",
    "    metric = 'accuracy'\n",
    "    loss_fn = 'binary_crossentropy'\n",
    "    activation_output_fn = 'sigmoid'\n",
    "    optimizer_fn = 'adam'\n",
    "    activation_hidden_fn = 'tanh'\n",
    "    kernel_initializer = 'glorot_uniform'\n",
    "    use_bias = True\n",
    "    bias_initializer = 'zeros'\n",
    "    kernel_regularizer = None\n",
    "    bias_regularizer = None\n",
    "    activity_regularizer = None\n",
    "    kernel_constraint = None\n",
    "    bias_constraint = None\n",
    "    path_submodels = dn.PATH_PROJECT + \"weak_classifiers/\"\n",
    "    type_submodels = dn.TypePredictionLabel.MULTI_LABEL_CATEGORICAL\n",
    "\n",
    "    hidden_layers_set = []\n",
    "    for idx in range(hidden_layer):\n",
    "            hidden_layers_set.append(\n",
    "                    dict({'units': neurons_by_submodel,\n",
    "                                'activation': activation_hidden_fn,\n",
    "                                'use_bias': use_bias,\n",
    "                                'kernel_initializer': kernel_initializer,\n",
    "                                'bias_initializer': bias_initializer,\n",
    "                                'kernel_regularizer': kernel_regularizer,\n",
    "                                'bias_regularizer': bias_regularizer,\n",
    "                                'activity_regularizer': activity_regularizer,\n",
    "                                'kernel_constraint': kernel_constraint,\n",
    "                                'bias_constraint': bias_constraint}))\n",
    "\n",
    "    set_network = dict({'epochs': epoch,\n",
    "                                            'batch_size': batch_size,\n",
    "                                            'patient_train': int(\n",
    "                                                    epoch / 2),\n",
    "                                            'activation_output_fn': activation_output_fn,\n",
    "                                            'loss_function': loss_fn,\n",
    "                                            'optmizer_function': optimizer_fn,\n",
    "                                            'main_metric': metric,\n",
    "                                            'load_cad_submodels': load_cad_submodels,\n",
    "                                            'dataset_train_path': dataset_train_path,\n",
    "                                            'dataset_test_path': dataset_test_path,\n",
    "                                            'path_submodels': path_submodels,\n",
    "                                            'type_submodels': type_submodels,\n",
    "                                            'submodels': use_submodel,\n",
    "                                            'hidden_layers': hidden_layers_set\n",
    "                                            })\n",
    "\n",
    "    name_test = 'E_' + str(epoch) + '_BS_' + str(batch_size) + \\\n",
    "                            '_US_' + str(len(use_submodel)) + '_N_' + str(neurons_by_submodel) + \\\n",
    "                            '_HL_' + str(hidden_layer) + '_M_' + str(metric)[0:2] + \\\n",
    "                            '_AO_' + str(bias_constraint)[0:2] + \\\n",
    "                            '_LF_' + str(loss_fn)[0:2] + '_OP_' + str(optimizer_fn) + \\\n",
    "                            '_AH_' + str(activation_hidden_fn)[0:2] + '_KI_' + str(kernel_initializer)[0:2] + \\\n",
    "                            '_UB_' + str(use_bias)[0] + '_BI_' + str(bias_initializer)[0:2] + \\\n",
    "                            '_KR_' + str(kernel_regularizer) + '_BR_' + str(bias_regularizer) + \\\n",
    "                            '_AR_' + str(activity_regularizer) + '_KC_' + str(kernel_constraint)[0:2] + \\\n",
    "                            '_BC_' + str(bias_constraint)[0:2]\n",
    "\n",
    "    return name_test, set_network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_stacked_ensemble():\n",
    "    dataset_train_path = 'dataset/anx_dep_multilabel/SMHD_multi_label_test_train_2112.df'\n",
    "    dataset_test_path = 'dataset/anx_dep_multilabel/SMHD_multi_label_test_test_528.df'\n",
    "\n",
    "    name_test, set_network = set_params_stacked_ensemble(dataset_train_path, dataset_test_path, False)\n",
    "    print(\"Experiment: \" + name_test)\n",
    "    ensemble_stk = StackedEnsemble('stacked_submodels_'+SUBMODEL, 1, '')\n",
    "\n",
    "    ensemble_stk.list_report_metrics = []\n",
    "    ensemble_stk.ensemble_stacked_conf = set_network\n",
    "    ensemble_stk.k_fold = 5\n",
    "    ensemble_stk.labels_set = ['control', 'anxiety', 'depression']\n",
    "    ensemble_stk.labels_ensemble = ['control', 'anxiety', 'depression']\n",
    "\n",
    "    ensemble_stk.type_predict_label = dn.TypePredictionLabel.MULTI_LABEL_CATEGORICAL\n",
    "    ensemble_stk.metrics_based_sample = False\n",
    "\n",
    "    ensemble_stk.set_network_params_ensemble_stack()\n",
    "    ensemble_stk.load_submodels()\n",
    "    ensemble_stk.load_pre_trained_model(dn.PATH_PROJECT + \"experiments/stacked_ensemble/\"\\\n",
    "                                        \"t2_E_16_BS_8_US_2_N_12_HL_3_M_ac_AO_No_LF_bi_OP_adam_AH_ta_KI_gl_UB_T_BI_ze_KR_None_BR_None_AR_None_KC_No_BC_No_train_valid_kf_0_ens_stk_model.h5\")\n",
    "    return ensemble_stk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenizer_by_submodel(ens_model):\n",
    "    time_ini = datetime.datetime.now()    \n",
    "\n",
    "    word_lookup_dict = dict()\n",
    "    tokenizers_dict = dict()\n",
    "    for key_model, value in ens_model.all_submodels.items():\n",
    "        tokenizer = ens_model.all_submodels[key_model]['exp'].pp_data.load_tokenizer()\n",
    "        tokenizers_dict.update({key_model: tokenizer})\n",
    "\n",
    "    set_period_time_end(time_ini, \"get_word_lookup_by_submodel...\")\n",
    "\n",
    "    return tokenizers_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_posts(x_data):\n",
    "    find_pad = np.where(x_data == 0)[1]\n",
    "    total_pads = len(find_pad)\n",
    "    total_valid_terms = len(x_data[0][total_pads:])\n",
    "    #print(total_pads, total_valid_terms)\n",
    "    if total_valid_terms >= total_pads:\n",
    "        #print('just complete ', total_pads)\n",
    "        t1 = np.array([x_data[0][total_pads:(2*total_pads)]])\n",
    "        t2 = np.array([x_data[0][total_pads:]])\n",
    "        tf = np.concatenate((t1, t2), axis=1)\n",
    "    else:\n",
    "        mult_valid_terms = int(total_pads/total_valid_terms)\n",
    "        #print('multivalid', mult_valid_terms)\n",
    "        \n",
    "        valid_posts_list = []\n",
    "        for i in range(mult_valid_terms):\n",
    "            valid_posts_list.append([x_data[0][total_pads:]])\n",
    "        \n",
    "        if mult_valid_terms == 1:\n",
    "            tf = np.concatenate((valid_posts_list[i], valid_posts_list[i]), axis=1)\n",
    "        else:\n",
    "            tf = valid_posts_list[0]\n",
    "            for i in range(1,len(valid_posts_list)):\n",
    "                tf = np.concatenate((tf, valid_posts_list[i]), axis=1)\n",
    "    \n",
    "        total_terms = int(tf.shape[1])\n",
    "        #print('multivalid ', total_terms, tf.shape)\n",
    "        \n",
    "        if total_terms < 5000:\n",
    "            total_terms = 5000-total_terms\n",
    "            #print('complementa com ', total_terms)\n",
    "            t1 = np.array([tf[0][0:total_terms]])\n",
    "            #print(t1.shape)\n",
    "            tf = np.concatenate((t1, tf), axis=1)\n",
    "\n",
    "    #print(tf.shape)\n",
    "    \n",
    "    return tf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_posts_samples(x_data):\n",
    "    for idx, x_sample in enumerate(x_data):\n",
    "        # print(x_data[idx])\n",
    "        x_data[idx] = fill_posts(np.array([x_sample]))\n",
    "        # print(x_data[idx])\n",
    "    return x_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_explainer_by_submodel(ens_model, data_df):\n",
    "    explainers_dict = dict()\n",
    "    for key_model, value in ens_model.all_submodels.items():\n",
    "        time_ini = datetime.datetime.now()    \n",
    "        exp = ens_model.all_submodels[key_model]['exp']\n",
    "        exp.pp_data.type_prediction_label = ens_model.type_predict_label\n",
    "        x_data, y_data = exp.pp_data.load_subdataset_generic(data_df, ens_model.labels_set)\n",
    "\n",
    "        # Replace pad with itself text\n",
    "        x_data = fill_posts_samples(x_data)\n",
    "        \n",
    "        explainer = shap.KernelExplainer(ens_model.all_submodels[key_model]['model_class'].model.predict, x_data)\n",
    "        explainers_dict.update({key_model: explainer})\n",
    "        set_period_time_end(time_ini, \"generate_explainer \"+str(key_model)+\"...\")\n",
    "\n",
    "    return explainers_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_shap_values_all_submodels(ens_model, data_df, explainers_dict, nsamples):\n",
    "    predicts_by_model = dict()\n",
    "    shap_values_dict = dict()\n",
    "    for key_model, value in ens_model.all_submodels.items():\n",
    "        time_ini = datetime.datetime.now()    \n",
    "        exp = ens_model.all_submodels[key_model]['exp']\n",
    "        \n",
    "        exp.pp_data.type_prediction_label = ens_model.type_predict_label\n",
    "        x_data, y_data = exp.pp_data.load_subdataset_generic(data_df, ens_model.labels_set)\n",
    "        \n",
    "        # Replace pad with itself text\n",
    "        x_data = fill_posts_samples(x_data)\n",
    "        \n",
    "        y_hat = ens_model.all_submodels[key_model]['model_class'].model.predict(x_data)\n",
    "        predicts_by_model.update({key_model: y_hat})\n",
    "        print('Predict submodel ', key_model, ': ', y_hat)      \n",
    "        \n",
    "        shap_values = explainers_dict[key_model].shap_values(x_data, y=y_data, nsamples=nsamples)\n",
    "        shap_values_dict.update({key_model: shap_values})\n",
    "        set_period_time_end(time_ini, \"generate_explainer \"+str(key_model)+\"...\")\n",
    "\n",
    "    return predicts_by_model, shap_values_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_shap_values_words(shap_plot, ens_model, total_words):\n",
    "    for key_model, value in ens_model.all_submodels.items():\n",
    "        shap_plot.explainer_values_print(key_model, total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enconding_data_by_submodel(key_model, ens_model, data_df):\n",
    "    exp = ens_model.all_submodels[key_model]['exp']\n",
    "    exp.pp_data.type_prediction_label = ens_model.type_predict_label\n",
    "    x_test, y_test = exp.pp_data.load_subdataset_generic(data_df, ens_model.labels_set)\n",
    "    \n",
    "    # Replace pad with itself text\n",
    "    x_test = fill_posts_samples(x_test)\n",
    "    \n",
    "    return x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_model(ens_model, data_df):\n",
    "    print('\\nModel \\t UserId \\t y      \\t y_pred \\t\\t y_pred(%)')\n",
    "\n",
    "    for key_model, value in ens_model.all_submodels.items():\n",
    "        exp = ens_model.all_submodels[key_model]['exp']\n",
    "        \n",
    "        exp.pp_data.type_prediction_label = ens_model.type_predict_label\n",
    "        x_data, y_data = exp.pp_data.load_subdataset_generic(data_df, ens_model.labels_set)\n",
    "\n",
    "        # Replace pad with itself text\n",
    "        x_data = fill_posts_samples(x_data)\n",
    "\n",
    "        y_hat = ens_model.all_submodels[key_model]['model_class'].model.predict(x_data)\n",
    "        \n",
    "        for index in range(len(y_data)):\n",
    "            print('%s \\t %s \\t %s \\t %s \\t %s' % (key_model, data_df.iloc[index].user_id, y_data[index], \n",
    "                                                  np.round(y_hat[index]), y_hat[index]))    \n",
    "\n",
    "    y, y_hat = ens_model.test_final_model(data_df)           \n",
    "    print('Predict ensemble model: ', y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_model_by_year(ens_model, data_df):\n",
    "    years = data_df.year.unique()\n",
    "    \n",
    "    for year in years:\n",
    "        new_data_df = data_df[data_df.year == year]\n",
    "        print('Predict sample for year = ', str(year))\n",
    "        predict_model(ens_model, new_data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_shap_values_by_submodel(key_model, shap_plot, ens_model, data_df, nsamples):\n",
    "    predicts_by_model = dict()\n",
    "    shap_values_dict = dict()\n",
    "\n",
    "    time_ini = datetime.datetime.now()    \n",
    "    exp = ens_model.all_submodels[key_model]['exp']\n",
    "\n",
    "    exp.pp_data.type_prediction_label = ens_model.type_predict_label\n",
    "    x_data, y_data = exp.pp_data.load_subdataset_generic(data_df, ens_model.labels_set)\n",
    "    \n",
    "    # Replace pad with itself text\n",
    "    x_data = fill_posts_samples(x_data)  \n",
    "\n",
    "    y_hat = ens_model.all_submodels[key_model]['model_class'].model.predict(x_data)\n",
    "    predicts_by_model.update({key_model: y_hat})\n",
    "    print('Predict submodel ', key_model, ': ', y_hat)      \n",
    "\n",
    "    shap_values = shap_plot.explainers_dict[key_model].shap_values(x_data, y=y_data, nsamples=nsamples)\n",
    "    shap_values_dict.update({key_model: shap_values})\n",
    "    set_period_time_end(time_ini, \"generate_explainer \"+str(key_model)+\"...\")\n",
    "\n",
    "    return predicts_by_model, shap_values_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataframe_top_words(shap_plot, total_words, ens_model, samples_df, nsamples):\n",
    "    analise_samples = []\n",
    "    \n",
    "    for key_model, value in ens_model.all_submodels.items():\n",
    "        # calcula valor shapley e predição\n",
    "        predicts_by_model, shap_plot.shap_values_dict = generate_shap_values_by_submodel(key_model, \n",
    "                                                                                         shap_plot, ens_model, \n",
    "                                                                                         samples_df, nsamples)\n",
    "        # resgata palavras para a mostra\n",
    "        x_test, y_test = enconding_data_by_submodel(key_model, ens_model, samples_df)\n",
    "\n",
    "        # Replace pad with itself text\n",
    "        x_test = fill_posts_samples(x_test)\n",
    "\n",
    "        word_lookup = shap_plot.generate_word_lookup(key_model, x_test)[0]\n",
    "\n",
    "        word_for_class = dict()\n",
    "        for index_class, label_name in enumerate(shap_plot.labels_classifier):\n",
    "            ps_wrds, ns_wrds, p_wrds, n_wrds = shap_plot.get_signal_pos_neg_words(key_model, word_lookup,\n",
    "                                                                                  total_words, index_class, 0)\n",
    "            word_for_class.update({label_name: {'sig_pos_words': ps_wrds, \n",
    "                                                'sig_neg_words': ns_wrds,\n",
    "                                                'pos_words': p_wrds, \n",
    "                                                'neg_words': n_wrds}})\n",
    "\n",
    "        analise_samples.append([samples_df[0:1].user_id.values[0], \n",
    "                                samples_df[0:1].label.values[0], \n",
    "                                samples_df[0:1].texts.values[0], key_model, \n",
    "                                predicts_by_model[key_model], word_for_class, x_test,\n",
    "                                shap_plot.explainers_dict[key_model].expected_value,\n",
    "                                dict({key_model: shap_plot.shap_values_dict[key_model]}), \n",
    "                                str(word_lookup)])\n",
    "\n",
    "    data_df = pd.DataFrame(analise_samples, \n",
    "                           columns=['user_id', 'label', 'texts', 'key_model', \n",
    "                                    'prediction', 'word_for_class', 'x_test',\n",
    "                                    'explainers_expected_value', 'shap_values', 'word_lookup'])\n",
    "    return data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_plot(shap_plot, analise_model_df, samples_selec, max_features, submodel):\n",
    "    key_models = [submodel+str(i) for i in range(1,6)]\n",
    "\n",
    "    for i in range(len(samples_selec)):\n",
    "        user_id = samples_selec[i:i+1].user_id.values[0]\n",
    "\n",
    "        for key_model in key_models:\n",
    "            wl = eval(analise_model_df[(analise_model_df.user_id == user_id) & \n",
    "                                       (analise_model_df.key_model == key_model)].word_lookup.values[0])\n",
    "            xt = analise_model_df[(analise_model_df.user_id == user_id) & \n",
    "                                  (analise_model_df.key_model == key_model)].x_test.values[0]\n",
    "            sv = analise_model_df[(analise_model_df.user_id == user_id) & \n",
    "                                  (analise_model_df.key_model == key_model)].shap_values.values[0][key_model]\n",
    "\n",
    "            print('AMOSTRA USER_ID %s, CLASS %s, MODEL %s' % (str(user_id), samples_selec[i:i+1].label.values[0],\n",
    "                                                              key_model))\n",
    "            shap_plot.shap_values_dict[key_model] = sv\n",
    "\n",
    "            shap_plot.feature_importance_plot(key_model, wl, xt, max_features=max_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_samples(ens_model, shap_plot, samples_selec, submodel):\n",
    "    nsample = dict({'ca': 30, 'cd':30, 'cad': 25})\n",
    "    \n",
    "    predict_model(ens_model, samples_selec)\n",
    "\n",
    "    for i in range(len(samples_selec)):\n",
    "        df = generate_dataframe_top_words(shap_plot, 100, ens_model, samples_selec[i:i+1], nsample[submodel])\n",
    "        \n",
    "        df.to_pickle(dn.PATH_PROJECT + 'analise_'+submodel.lower()+'_user_'+\n",
    "                     str(samples_selec[i:i+1].user_id.values[0])+'.df')\n",
    "\n",
    "        generate_plot(shap_plot, df, samples_selec[i:i+1], 10, submodel.upper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_shap_ca():\n",
    "    print('SETS ----> ', DATASET_TRAIN, LABEL_CLASS)\n",
    "\n",
    "    train_df = pd.read_pickle(dn.PATH_PROJECT + DATASET_TRAIN)\n",
    "    train_df = select_samples_by_total_class(train_df, LABEL_CLASS, TOTAL_SAMPLES_TRAIN_BY_CLASS)\n",
    "    print(train_df.groupby('label').size())    \n",
    "    \n",
    "    ens_stk_ca = load_stacked_ensemble()\n",
    "    \n",
    "    shap_plot_ca = ShapAnalysePlots()\n",
    "    shap_plot_ca.labels_classifier = ens_stk_ca.labels_set\n",
    "\n",
    "    shap_plot_ca.tokenizers_dict = get_tokenizer_by_submodel(ens_stk_ca)\n",
    "    \n",
    "    shap_plot_ca.explainers_dict = generate_explainer_by_submodel(ens_stk_ca, train_df)\n",
    "    \n",
    "    return ens_stk_ca, shap_plot_ca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_shap_cd():\n",
    "    print('SETS ----> ', DATASET_TRAIN, LABEL_CLASS)\n",
    "\n",
    "    train_df = pd.read_pickle(dn.PATH_PROJECT + DATASET_TRAIN)\n",
    "    train_df = select_samples_by_total_class(train_df, LABEL_CLASS, TOTAL_SAMPLES_TRAIN_BY_CLASS)\n",
    "    print(train_df.groupby('label').size())    \n",
    "    \n",
    "    ens_stk_cd = load_stacked_ensemble()\n",
    "    \n",
    "    shap_plot_cd = ShapAnalysePlots()\n",
    "    shap_plot_cd.labels_classifier = ens_stk_cd.labels_set\n",
    "\n",
    "    shap_plot_cd.tokenizers_dict = get_tokenizer_by_submodel(ens_stk_cd)\n",
    "    \n",
    "    shap_plot_cd.explainers_dict = generate_explainer_by_submodel(ens_stk_cd, train_df)\n",
    "    \n",
    "    return ens_stk_cd, shap_plot_cd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_shap_cad():\n",
    "    print('SETS ----> ', DATASET_TRAIN, LABEL_CLASS)\n",
    "    \n",
    "    train_df = pd.read_pickle(dn.PATH_PROJECT + DATASET_TRAIN)\n",
    "    train_df = select_samples_by_total_class(train_df, LABEL_CLASS, TOTAL_SAMPLES_TRAIN_BY_CLASS)\n",
    "    print(train_df.groupby('label').size())    \n",
    "    \n",
    "    ens_stk_cad = load_stacked_ensemble()\n",
    "    \n",
    "    shap_plot_cad = ShapAnalysePlots()\n",
    "    shap_plot_cad.labels_classifier = ens_stk_cad.labels_set\n",
    "\n",
    "    shap_plot_cad.tokenizers_dict = get_tokenizer_by_submodel(ens_stk_cad)\n",
    "    \n",
    "    shap_plot_cad.explainers_dict = generate_explainer_by_submodel(ens_stk_cad, train_df)\n",
    "    \n",
    "    return ens_stk_cad, shap_plot_cad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Analysis of the ensemble model via submodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBMODEL = 'CA'\n",
    "DATASET_TRAIN = 'dataset/anxiety/SMHD_train_1040.df'\n",
    "LABEL_CLASS = ['control', 'anxiety', 'depression']\n",
    "TOTAL_SAMPLES_TRAIN_BY_CLASS = 300\n",
    "\n",
    "ens_stk_ca, shap_plot_ca = generate_shap_ca()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBMODEL = 'CD'\n",
    "DATASET_TRAIN = 'dataset/depression/SMHD_train_2160.df'\n",
    "LABEL_CLASS = ['control', 'anxiety', 'depression']\n",
    "TOTAL_SAMPLES_TRAIN_BY_CLASS = 300\n",
    "\n",
    "ens_stk_cd, shap_plot_cd = generate_shap_cd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBMODEL = 'CAD'\n",
    "DATASET_TRAIN = 'dataset/anxiety,depression/SMHD_train_880.df'\n",
    "LABEL_CLASS = ['control', 'anxiety,depression']\n",
    "TOTAL_SAMPLES_TRAIN_BY_CLASS = 200\n",
    "\n",
    "ens_stk_cad, shap_plot_cad = generate_shap_cad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_pickle(dn.PATH_PROJECT + 'dataset/samples_for_interpretation/SMHD_multi_label_test_test_352_user_id.df')\n",
    "test_df.groupby('label').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "process_samples(ens_stk_ca, shap_plot_ca, test_df, 'ca')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "process_samples(ens_stk_cd, shap_plot_cd, test_df, 'cd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "process_samples(ens_stk_cad, shap_plot_cad, test_df, 'cad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
